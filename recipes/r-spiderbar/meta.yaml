{% set version = '0.2.5' %}
{% set posix = 'm2-' if win else '' %}
{% set native = 'm2w64-' if win else '' %}

package:
  name: r-spiderbar
  version: {{ version|replace("-", "_") }}

source:
  url:
    - {{ cran_mirror }}/src/contrib/spiderbar_{{ version }}.tar.gz
    - {{ cran_mirror }}/src/contrib/Archive/spiderbar/spiderbar_{{ version }}.tar.gz
  sha256: 45b7f1c28d7764319d5222f170dd7c0733a8400ff5d2a9aba9d5f7a4eda81d7a

build:
  merge_build_host: True  # [win]
  number: 0
  skip: True  # [win]
  rpaths:
    - lib/R/lib/
    - lib/

requirements:
  build:
    - {{ compiler('c') }}              # [not win]
    - {{ compiler('m2w64_c') }}        # [win]
    - {{ compiler('cxx') }}            # [not win]
    - {{ compiler('m2w64_cxx') }}      # [win]
    - {{ posix }}filesystem        # [win]
    - {{ posix }}make
    - {{ posix }}sed               # [win]
    - {{ posix }}coreutils         # [win]
    - {{ posix }}zip               # [win]
    - cross-r-base {{ r_base }}    # [build_platform != target_platform]
  host:
    - r-base
    - r-rcpp
  run:
    - r-base
    - {{ native }}gcc-libs         # [win]
    - r-rcpp

test:
  commands:
    - $R -e "library('spiderbar')"           # [not win]
    - "\"%R%\" -e \"library('spiderbar')\""  # [win]

about:
  home: https://github.com/hrbrmstr/spiderbar
  license: MIT
  summary: The 'Robots Exclusion Protocol' <https://www.robotstxt.org/orig.html> documents a
    set of standards for allowing or excluding robot/spider crawling of different areas
    of site content. Tools are provided which wrap The 'rep-cpp' <https://github.com/seomoz/rep-cpp>
    C++ library for processing these 'robots.txt' files.
  license_family: MIT
  license_file:
    - '{{ environ["PREFIX"] }}/lib/R/share/licenses/MIT'
    - LICENSE

extra:
  recipe-maintainers:
    - conda-forge/r
