diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/__init__.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/__init__.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/__init__.py
deleted file mode 100644
index 0f32c82..0000000
--- a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/__init__.py
+++ /dev/null
@@ -1,75 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-FADM: Fairness-aware Data Mining
-"""
-
-#==============================================================================
-# Module metadata variables
-#==============================================================================
-
-__author__ = "Toshihiro Kamishima ( http://www.kamishima.net/ )"
-__date__ = "2012/08/25"
-__version__ = "1.0.0"
-__copyright__ = "Copyright (c) 2012 Toshihiro Kamishima all rights reserved."
-__license__ = "MIT License: http://www.opensource.org/licenses/mit-license.php"
-__docformat__ = "restructuredtext en"
-
-#==============================================================================
-# Imports
-#==============================================================================
-
-import logging
-
-#==============================================================================
-# Public symbols
-#==============================================================================
-
-__all__ = ['eval', 'nb', 'lr', 'util']
-
-#==============================================================================
-# Constants
-#==============================================================================
-
-#==============================================================================
-# Module variables
-#==============================================================================
-
-#==============================================================================
-# Classes
-#==============================================================================
-
-#==============================================================================
-# Functions 
-#==============================================================================
-
-#==============================================================================
-# Module initialization 
-#==============================================================================
-
-# init logging system ---------------------------------------------------------
-
-logger = logging.getLogger('fadm')
-if not logger.handlers:
-    logger.addHandler(logging.NullHandler)
-
-#==============================================================================
-# Test routine
-#==============================================================================
-
-def _test():
-    """ test function for this module
-    """
-
-    # perform doctest
-    import sys
-    import doctest
-
-    doctest.testmod()
-
-    sys.exit(0)
-
-# Check if this is call as command script -------------------------------------
-
-if __name__ == '__main__':
-    _test()
diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/lr/__init__.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/lr/__init__.py
deleted file mode 100644
index 8eaa807..0000000
--- a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/lr/__init__.py
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-Logistic Regression
-"""
-
-#==============================================================================
-# Imports
-#==============================================================================
-
-#==============================================================================
-# Public symbols
-#==============================================================================
-
-__all__ = ['pr']
diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/lr/pr.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/lr/pr.py
deleted file mode 100644
index 4a46472..0000000
--- a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/lr/pr.py
+++ /dev/null
@@ -1,503 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-Two Class logistic regression module with Prejudice Remover
-
-the number of sensitive features is restricted to one, and the feature must
-be binary.
-
-Attributes
-----------
-EPSILON : floast
-    small positive constant
-N_S : int
-    the number of sensitive features
-N_CLASSES : int
-    the number of classes
-"""
-
-from __future__ import print_function
-from __future__ import division
-from __future__ import unicode_literals
-
-#==============================================================================
-# Module metadata variables
-#==============================================================================
-
-#==============================================================================
-# Imports
-#==============================================================================
-
-import logging
-import numpy as np
-from scipy.optimize import fmin_cg
-from sklearn.linear_model import LogisticRegression
-from sklearn.base import BaseEstimator, ClassifierMixin
-
-#==============================================================================
-# Public symbols
-#==============================================================================
-
-__all__ = ['LRwPRType4']
-
-#==============================================================================
-# Constants
-#==============================================================================
-
-EPSILON = 1.0e-10
-SIGMOID_RANGE = np.log((1.0 - EPSILON) / EPSILON)
-N_S = 1
-N_CLASSES = 2
-
-#==============================================================================
-# Module variables
-#==============================================================================
-
-#==============================================================================
-# Functions
-#==============================================================================
-
-def sigmoid(x, w):
-    """ sigmoid(w^T x)
-    To suppress the warnings at np.exp, do "np.seterr(all='ignore')"
-
-    Parameters
-    ----------
-    x : array, shape=(d)
-        input vector
-    w : array, shape=(d)
-        weight
-
-    -------
-    sigmoid : float
-        sigmoid(w^T x)
-    """
-
-    s = np.clip(np.dot(w, x), -SIGMOID_RANGE, SIGMOID_RANGE)
-
-    return 1.0 / (1.0 + np.exp(-s))
-
-
-#==============================================================================
-# Classes
-#==============================================================================
-
-class LRwPR(BaseEstimator, ClassifierMixin):
-    """ Two class LogisticRegression with Prejudice Remover
-
-    Parameters
-    ----------
-    C : float
-        regularization parameter
-    eta : float
-        penalty parameter
-    fit_intercept : bool
-        use a constant term
-    penalty : str
-        fixed to 'l2'
-
-    Attributes
-    ----------
-    minor_type : int
-        type of likelihood fitting
-    `coef_` : array, shape=(n_features)
-        parameters for logistic regression model
-    `mx_` : array-like, shape(n_sfv, n_nsf)
-        mx_[si, :] is a mean rows of X whose corresponding sensitive
-        feature is exactly si.
-    `n_s_` : int
-        the number of sensitive features
-    `n_sfv_` : int
-        the number of sensitive feature values.
-    `c_s_` : ary, shape=(`n_sfv_`)
-        the counts of each senstive values in training samples
-    `n_features_` : int
-        the number of non-sensitive features including a bias constant
-    `n_samples_` : int
-        the number of samples
-    `f_loss_` : float
-        the value of loss function after training
-    """
-
-    def __init__(self, C=1.0, eta=1.0, fit_intercept=True, penalty='l2'):
-
-        if C < 0.0:
-            raise TypeError
-        self.fit_intercept = fit_intercept
-        self.penalty = penalty
-        self.C = C
-        self.eta = eta
-        self.minor_type = 0
-        self.f_loss_ = np.inf
-
-    def predict(self, X):
-        """ predict classes
-
-        Parameters
-        ----------
-        X : array, shape=(n_samples, n_features)
-            feature vectors of samples
-
-        Returns
-        -------
-        y : array, shape=(n_samples), dtype=int
-            array of predicted class
-        """
-
-        return np.argmax(self.predict_proba(X), 1)
-
-class LRwPRPredictProbaType2Mixin(LRwPR):
-    """ mixin for singe type 2 likelihood
-    """
-
-    def predict_proba(self, X):
-        """ predict probabilities
-
-        a set of weight vectors, whose size if the same as the number of the
-        sensitive features, are available and these weights are selected
-        according to the value of a sensitive feature
-
-        Parameters
-        ----------
-        X : array, shape=(n_samples, n_features)
-            feature vectors of samples
-
-        Returns
-        -------
-        y_proba : array, shape=(n_samples, n_classes), dtype=float
-            array of predicted class
-        """
-
-        # add a constanet term
-        s = np.atleast_1d(np.squeeze(np.array(X)[:, -self.n_s_]).astype(int))
-        if self.fit_intercept:
-            X = np.c_[np.atleast_2d(X)[:, :-self.n_s_], np.ones(X.shape[0])]
-        else:
-            X = np.atleast_2d(X)[:, :-self.n_s_]
-        coef = self.coef_.reshape(self.n_sfv_, self.n_features_)
-
-        proba = np.empty((X.shape[0], N_CLASSES))
-        proba[:, 1] = [sigmoid(X[i, :], coef[s[i], :])
-                       for i in range(X.shape[0])]
-        proba[:, 0] = 1.0 - proba[:, 1]
-
-        return proba
-
-class LRwPRFittingType1Mixin(LRwPR):
-    """ Fitting Method Mixin
-    """
-
-    def init_coef(self, itype, X, y, s):
-        """ set initial weight
-
-        initialization methods are specified by `itype`
-
-        * 0: cleared by 0
-        * 1: follows standard normal distribution
-        * 2: learned by standard logistic regression
-        * 3: learned by standard logistic regression separately according to
-          the value of sensitve feature
-
-        Parameters
-        ----------
-        itype : int
-            type of initialization method
-        X : array, shape=(n_samples, n_features)
-            feature vectors of samples
-        y : array, shape=(n_samples)
-            target class of samples
-        s : array, shape=(n_samples)
-            values of sensitive features
-        """
-
-        if itype == 0:
-            # clear by zeros
-            self.coef_ = np.zeros(self.n_sfv_ * self.n_features_,
-                                  dtype=np.float)
-        elif itype == 1:
-            # at random
-            self.coef_ = np.random.randn(self.n_sfv_ * self.n_features_)
-
-        elif itype == 2:
-            # learned by standard LR
-            self.coef_ = np.empty(self.n_sfv_ * self.n_features_,
-                                  dtype=np.float)
-            coef = self.coef_.reshape(self.n_sfv_, self.n_features_)
-
-            clr = LogisticRegression(C=self.C, penalty='l2',
-                                     fit_intercept=False)
-            clr.fit(X, y)
-
-            coef[:, :] = clr.coef_
-        elif itype == 3:
-            # learned by standard LR
-            self.coef_ = np.empty(self.n_sfv_ * self.n_features_,
-                                  dtype=np.float)
-            coef = self.coef_.reshape(self.n_sfv_, self.n_features_)
-
-            for i in range(self.n_sfv_):
-                clr = LogisticRegression(C=self.C, penalty='l2',
-                                         fit_intercept=False)
-                clr.fit(X[s == i, :], y[s == i])
-                coef[i, :] = clr.coef_
-        else:
-            raise TypeError
-
-    def fit(self, X, y, ns=N_S, itype=0, **kwargs):
-        """ train this model
-
-        Parameters
-        ----------
-        X : array, shape = (n_samples, n_features)
-            feature vectors of samples
-        y : array, shape = (n_samples)
-            target class of samples
-        ns : int
-            number of sensitive features. currently fixed to N_S
-        itype : int
-            type of initialization method
-        kwargs : any
-            arguments to optmizer
-        """
-
-        # rearrange input arguments
-        s = np.atleast_1d(np.squeeze(np.array(X)[:, -ns]).astype(int))
-        if self.fit_intercept:
-            X = np.c_[np.atleast_2d(X)[:, :-ns], np.ones(X.shape[0])]
-        else:
-            X = np.atleast_2d(X)[:, :-ns]
-
-        # check optimization parameters
-        if not 'disp' in kwargs:
-            kwargs['disp'] = False
-        if not 'maxiter' in kwargs:
-            kwargs['maxiter'] = 100
-
-        # set instance variables
-        self.n_s_ = ns
-        self.n_sfv_ = np.max(s) + 1
-        self.c_s_ = np.array([np.sum(s == si).astype(np.float)
-                              for si in range(self.n_sfv_)])
-        self.n_features_ = X.shape[1]
-        self.n_samples_ = X.shape[0]
-
-        # optimization
-        self.init_coef(itype, X, y, s)
-        self.coef_ = fmin_cg(self.loss,
-                             self.coef_,
-                             fprime=self.grad_loss,
-                             args=(X, y, s),
-                             **kwargs)
-
-        # get final loss
-        self.f_loss_ = self.loss(self.coef_, X, y, s)
-
-class LRwPRObjetiveType4Mixin(LRwPR):
-    """ objective function of logistic regression with prejudice remover
-
-    Loss Function type 4: Weights for logistic regression are prepared for each
-    value of S. Penalty for enhancing is defined as mutual information between
-    Y and S.
-    """
-
-    def loss(self, coef_, X, y, s):
-        """ loss function: negative log - likelihood with l2 regularizer
-        To suppress the warnings at np.log, do "np.seterr(all='ignore')"
-
-        Parameters
-        ----------
-        `coef_` : array, shape=(`n_sfv_` * n_features)
-            coefficients of model
-        X : array, shape=(n_samples, n_features)
-            feature vectors of samples
-        y : array, shape=(n_samples)
-            target class of samples
-        s : array, shape=(n_samples)
-            values of sensitive features
-
-        Returns
-        -------
-        loss : float
-            loss function value
-        """
-
-        coef = coef_.reshape(self.n_sfv_, self.n_features_)
-
-#        print >> sys.stderr, "loss:", coef[0, :], coef[1, :]
-
-        ### constants
-
-        # sigma = Pr[y=0|x,s] = sigmoid(w(s)^T x)
-        p = np.array([sigmoid(X[i, :], coef[s[i], :])
-                      for i in range(self.n_samples_)])
-
-        # rho(s) = Pr[y=0|s] = \sum_{(xi,si)in D st si=s} sigma(xi,si) / #D[s]
-        q = np.array([np.sum(p[s == si])
-                      for si in range(self.n_sfv_)]) / self.c_s_
-
-        # pi = Pr[y=0] = \sum_{(xi,si)in D} sigma(xi,si)
-        r = np.sum(p) / self.n_samples_
-
-        ### loss function
-
-        # likelihood
-        # \sum_{x,s,y in D} y log(sigma) + (1 - y) log(1 - sigma)
-        l = np.sum(y * np.log(p) + (1.0 - y) * np.log(1.0 - p))
-
-        # fairness-aware regularizer
-        # \sum_{x,s in D} \
-        #    sigma(x,x)       [log(rho(s))     - log(pi)    ] + \
-        #    (1 - sigma(x,s)) [log(1 - rho(s)) - log(1 - pi)]
-        f = np.sum(p * (np.log(q[s]) - np.log(r))
-             + (1.0 - p) * (np.log(1.0 - q[s]) - np.log(1.0 - r)))
-
-        # l2 regularizer
-        reg = np.sum(coef * coef)
-
-        l = -l + self.eta * f + 0.5 * self.C * reg
-#        print >> sys.stderr, l
-        return l
-
-    def grad_loss(self, coef_, X, y, s):
-        """ first derivative of loss function
-
-        Parameters
-        ----------
-        `coef_` : array, shape=(`n_sfv_` * n_features)
-            coefficients of model
-        X : array, shape=(n_samples, n_features)
-            feature vectors of samples
-        y : array, shape=(n_samples)
-            target class of samples
-        s : array, shape=(n_samples)
-            values of sensitive features
-
-        Returns
-        grad_loss : float
-            first derivative of loss function
-        """
-
-        coef = coef_.reshape(self.n_sfv_, self.n_features_)
-        l_ = np.empty(self.n_sfv_ * self.n_features_)
-        l = l_.reshape(self.n_sfv_, self.n_features_)
-#        print >> sys.stderr, "grad_loss:", coef[0, :], coef[1, :]
-
-        ### constants
-        # prefix "d_": derivertive by w(s)
-
-        # sigma = Pr[y=0|x,s] = sigmoid(w(s)^T x)
-        # d_sigma(x,s) = d sigma / d w(s) = sigma (1 - sigma) x
-        p = np.array([sigmoid(X[i, :], coef[s[i], :])
-                      for i in range(self.n_samples_)])
-        dp = (p * (1.0 - p))[:, np.newaxis] * X
-
-        # rho(s) = Pr[y=0|s] = \sum_{(xi,si)in D st si=s} sigma(xi,si) / #D[s]
-        # d_rho(s) = \sum_{(xi,si)in D st si=s} d_sigma(xi,si) / #D[s]
-        q = np.array([np.sum(p[s == si])
-                      for si in range(self.n_sfv_)]) / self.c_s_
-        dq = np.array([np.sum(dp[s == si, :], axis=0)
-                       for si in range(self.n_sfv_)]) \
-                       / self.c_s_[:, np.newaxis]
-
-        # pi = Pr[y=0] = \sum_{(xi,si)in D} sigma(xi,si) / #D
-        # d_pi = \sum_{(xi,si)in D} d_sigma(xi,si) / #D
-        r = np.sum(p) / self.n_samples_
-        dr = np.sum(dp, axis=0) / self.n_samples_
-
-        # likelihood
-        # l(si) = \sum_{x,y in D st s=si} (y - sigma(x, si)) x
-        for si in range(self.n_sfv_):
-            l[si, :] = np.sum((y - p)[s == si][:, np.newaxis] * X[s == si, :],
-                              axis=0)
-
-        # fairness-aware regularizer
-        # differentialy by w(s)
-        # \sum_{x,s in {D st s=si} \
-        #     [(log(rho(si)) - log(pi)) - (log(1 - rho(si)) - log(1 - pi))] \
-        #     * d_sigma
-        # + \sum_{x,s in {D st s=si} \
-        #     [ {sigma(xi, si) - rho(si)} / {rho(si) (1 - rho(si))} ] \
-        #     * d_rho
-        # - \sum_{x,s in {D st s=si} \
-        #     [ {sigma(xi, si) - pi} / {pi (1 - pi)} ] \
-        #     * d_pi
-
-        f1 = (np.log(q[s]) - np.log(r)) \
-             - (np.log(1.0 - q[s]) - np.log(1.0 - r))
-        f2 = (p - q[s]) / (q[s] * (1.0 - q[s]))
-        f3 = (p - r) / (r * (1.0 - r))
-        f4 = f1[:, np.newaxis] * dp \
-            + f2[:, np.newaxis] * dq[s, :] \
-            - np.outer(f3, dr)
-        f = np.array([np.sum(f4[s == si, :], axis=0)
-                      for si in range(self.n_sfv_)])
-
-        # l2 regularizer
-        reg = coef
-
-        # sum
-        l[:, :] = -l + self.eta * f + self.C * reg
-#        print >> sys.stderr, "l =", l
-
-        return l_
-
-class LRwPRType4\
-    (LRwPRObjetiveType4Mixin,
-     LRwPRFittingType1Mixin,
-     LRwPRPredictProbaType2Mixin):
-    """ Two class LogisticRegression with Prejudice Remover
-
-    Parameters
-    ----------
-    C : float
-        regularization parameter
-    eta : float
-        penalty parameter
-    fit_intercept : bool
-        use a constant term
-    penalty : str
-        fixed to 'l2'
-    """
-
-    def __init__(self, C=1.0, eta=1.0, fit_intercept=True, penalty='l2'):
-
-        super(LRwPRType4, self).\
-            __init__(C=C, eta=eta,
-                     fit_intercept=fit_intercept, penalty=penalty)
-
-        self.coef_ = None
-        self.mx_ = None
-        self.n_s_ = 0
-        self.n_sfv_ = 0
-        self.minor_type = 4
-
-#==============================================================================
-# Module initialization
-#==============================================================================
-
-# init logging system
-
-logger = logging.getLogger('fadm')
-if not logger.handlers:
-    logger.addHandler(logging.NullHandler)
-
-#==============================================================================
-# Test routine
-#==============================================================================
-
-def _test():
-    """ test function for this module
-    """
-
-    # perform doctest
-    import sys
-    import doctest
-
-    doctest.testmod()
-
-    sys.exit(0)
-
-# Check if this is call as command script
-
-if __name__ == '__main__':
-    _test()
diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/util/__init__.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/util/__init__.py
deleted file mode 100644
index 74da56e..0000000
--- a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/util/__init__.py
+++ /dev/null
@@ -1,11 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-Utilities
-"""
-
-#==============================================================================
-# Imports
-#==============================================================================
-
-from ._base import *
diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/util/_base.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/util/_base.py
deleted file mode 100644
index 5dd2adc..0000000
--- a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/fadm/util/_base.py
+++ /dev/null
@@ -1,150 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-import from 50b745c1d18d5c4b01d9d00e406b5fdaab3515ea @ KamLearn
-
-Utility routines
-"""
-
-#==============================================================================
-# Module metadata variables
-#==============================================================================
-
-#==============================================================================
-# Imports
-#==============================================================================
-
-import sys
-import logging
-import numpy as np
-
-#==============================================================================
-# Public symbols
-#==============================================================================
-
-__all__ = ['add_constant_feature',
-           'fill_missing_with_mean',
-           'decode_nfv']
-
-#==============================================================================
-# Constants
-#==============================================================================
-
-#==============================================================================
-# Module variables
-#==============================================================================
-
-#==============================================================================
-# Classes
-#==============================================================================
-
-#==============================================================================
-# Functions
-#==============================================================================
-
-def add_constant_feature(D):
-    """ add ones at the first column of the matrix
-
-    Parameters
-    __________
-    D : array, shape(n, m)
-        raw data matrix
-
-    Returns
-    -------
-    D : array, shape((n + 1, m)
-        data matrix with constant terms
-    """
-
-    return np.c_[np.ones(D.shape[0]), D]
-
-def fill_missing_with_mean(D, default=0.0):
-    """ fill missing value with the means of non-missing values in the column
-
-    Parameters
-    ----------
-    D : array, shape(n, m)
-        raw data matrix
-    default : float
-        default value if all values are NaN
-
-    Returns
-    -------
-    D : array, shape(n, m)
-        a data matrix whose missing values are filled
-    """
-
-    for i in range(D.shape[1]):
-        if np.any(np.isnan(D[:, i])):
-            v = np.mean(D[np.isfinite(D[:, i]), i])
-            if np.isnan(v):
-                v = default
-            D[np.isnan(D[:, i]), i] = v
-
-    return D
-
-def decode_nfv(nfvstr, nf):
-    """ parse the string for a list of feature domain sizes
-    
-    Parameters
-    ----------
-    nfvstr : str
-        string specified in a command-line option
-    nf : int
-        the number of features
-
-    Returns
-    -------
-     nfv : array, dtype=int, shape=(n_features)
-        array of sizes of feature domain
-
-    Raises
-    ------
-    ValueError
-        uninterpretable inputs
-    """
-
-    try:
-        nfv = np.fromstring(nfvstr, dtype=int, sep=':')
-
-        if len(nfv) == 1:
-            nfv = nfv * nf
-        elif len(nfv) != nf:
-            raise ValueError
-        if np.any(nfv < 0) or np.any(nfv == 1):
-            raise ValueError
-    except ValueError:
-        sys.exit("Illegal specfication of the numbers of feature values")
-
-    return nfv
-
-#==============================================================================
-# Module initialization
-#==============================================================================
-
-# init logging system ---------------------------------------------------------
-
-logger = logging.getLogger('fadm')
-if not logger.handlers:
-    logger.addHandler(logging.NullHandler)
-
-#==============================================================================
-# Test routine
-#==============================================================================
-
-def _test():
-    """ test function for this module
-    """
-
-    # perform doctest
-    import sys
-    import doctest
-
-    doctest.testmod()
-
-    sys.exit(0)
-
-# Check if this is call as command script -------------------------------------
-
-if __name__ == '__main__':
-    _test()
diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/predict_lr.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/predict_lr.py
deleted file mode 100644
index 5527e31..0000000
--- a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/predict_lr.py
+++ /dev/null
@@ -1,278 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-Predict classes for logistic regression model
-
-SYNOPSIS::
-
-    SCRIPT [options]
-
-Description
-===========
-
-Columns of Outputs:
-
-1. true sample class number
-2. predicted class number
-3. sensitive feature
-4. class 0 probability
-5. class 1 probability
-
-Delimiters of columns are a single space.
-
-Options
-=======
-
--i <INPUT>, --in <INPUT>
-    specify <INPUT> file name
--o <OUTPUT>, --out <OUTPUT>
-    specify <OUTPUT> file name
--m <MODEL>, --model <MODEL>
-    trained classifier (default "classification.model")
---ns
-    ignore sensitive features
---hideinfo
-    suppress output meta information
--q, --quiet
-    set logging level to ERROR, no messages unless errors
---rseed <RSEED>
-    random number seed. if None, use /dev/urandom (default None)
-
-Attributes
-==========
-N_NS : int
-    the number of non sensitive features
-"""
-
-#==============================================================================
-# Module metadata variables
-#==============================================================================
-
-__author__ = "Toshihiro Kamishima ( http://www.kamishima.net/ )"
-__date__ = "2012/08/26"
-__version__ = "3.0.0"
-__copyright__ = "Copyright (c) 2012 Toshihiro Kamishima all rights reserved."
-__license__ = "MIT License: http://www.opensource.org/licenses/mit-license.php"
-__docformat__ = "restructuredtext en"
-
-#==============================================================================
-# Imports
-#==============================================================================
-
-import sys
-import argparse
-import os
-import platform
-from subprocess import getoutput
-import logging
-import datetime
-import pickle
-import numpy as np
-
-# private modeules ------------------------------------------------------------
-from fadm.util import fill_missing_with_mean
-
-#==============================================================================
-# Public symbols
-#==============================================================================
-
-__all__ = []
-
-#==============================================================================
-# Constants
-#==============================================================================
-
-N_NS = 1
-
-#==============================================================================
-# Module variables
-#==============================================================================
-
-#==============================================================================
-# Classes
-#==============================================================================
-
-#==============================================================================
-# Functions
-#==============================================================================
-
-#==============================================================================
-# Main routine
-#==============================================================================
-
-def main(opt):
-    """ Main routine that exits with status code 0
-    """
-
-    ### pre process
-
-    # load model file
-    clr = pickle.load(opt.model)
-    clr_info = pickle.load(opt.model)
-
-    # read data
-    D = np.loadtxt(opt.infile)
-
-    # split data and process missing values
-    y = np.array(D[:, -1])
-    if opt.ns:
-        X = fill_missing_with_mean(D[:, :-(1 + N_NS)])
-    else:
-        X = fill_missing_with_mean(D[:, :-1])
-    S = np.atleast_2d(D[:, -(1 + N_NS):-1])
-
-    ### main process
-
-    # set starting time
-    start_time = datetime.datetime.now()
-    start_utime = os.times()[0]
-    opt.start_time = start_time.isoformat()
-    logger.info("start time = " + start_time.isoformat())
-
-    # prediction and write results
-    p = clr.predict_proba(X)
-
-    # output prediction
-    n = 0
-    m = 0
-    for i in range(p.shape[0]):
-        c = np.argmax(p[i, :])
-        opt.outfile.write("%d %d " % (y[i], c))
-        opt.outfile.write(" ".join(S[i, :].astype(str)) + " ")
-        opt.outfile.write(str(p[i, 0]) + " " + str(p[i, 1]) + "\n")
-        n += 1
-        m += 1 if c == y[i] else 0
-
-    # set end and elapsed time
-    end_time = datetime.datetime.now()
-    end_utime = os.times()[0]
-    logger.info("end time = " + end_time.isoformat())
-    opt.end_time = end_time.isoformat()
-    logger.info("elapsed_time = " + str((end_time - start_time)))
-    opt.elapsed_time = str((end_time - start_time))
-    logger.info("elapsed_utime = " + str((end_utime - start_utime)))
-    opt.elapsed_utime = str((end_utime - start_utime))
-
-    ### output
-
-    # add meta info
-    opt.nos_samples = n
-    logger.info('nos_samples = ' + str(opt.nos_samples))
-    opt.nos_correct_samples = m
-    logger.info('nos_correct_samples = ' + str(opt.nos_correct_samples))
-    opt.accuracy = m / float(n)
-    logger.info('accuracy = ' + str(opt.accuracy))
-    opt.negative_mean_prob = np.mean(p[:, 0])
-    logger.info('negative_mean_prob = ' + str(opt.negative_mean_prob))
-    opt.positive_mean_prob = np.mean(p[:, 1])
-    logger.info('positive_mean_prob = ' + str(opt.positive_mean_prob))
-
-    # output meta information
-    if opt.info:
-        for key in clr_info.keys():
-            opt.outfile.write("#classifier_%s=%s\n" %
-                              (key, str(clr_info[key])))
-
-        for key, key_val in vars(opt).items():
-            opt.outfile.write("#%s=%s\n" % (key, str(key_val)))
-
-    ### post process
-
-    # close file
-    if opt.infile != sys.stdin:
-        opt.infile.close()
-
-    if opt.outfile != sys.stdout:
-        opt.outfile.close()
-
-    if opt.model != sys.stdout:
-        opt.model.close()
-
-    sys.exit(0)
-
-### Preliminary processes before executing a main routine
-if __name__ == '__main__':
-    ### set script name
-    script_name = sys.argv[0].split('/')[-1]
-
-    ### init logging system
-    logger = logging.getLogger(script_name)
-    logging.basicConfig(level=logging.INFO,
-                        format='[%(name)s: %(levelname)s'
-                               ' @ %(asctime)s] %(message)s')
-
-    ### command-line option parsing
-
-    ap = argparse.ArgumentParser(
-        description='pydoc is useful for learning the details.')
-
-    # common options
-    ap.add_argument('--version', action='version',
-                    version='%(prog)s ' + __version__)
-
-    apg = ap.add_mutually_exclusive_group()
-    apg.set_defaults(verbose=True)
-    apg.add_argument('--verbose', action='store_true')
-    apg.add_argument('-q', '--quiet', action='store_false', dest='verbose')
-
-    ap.add_argument("--rseed", type=int, default=None)
-
-    # basic file i/o
-    ap.add_argument('-i', '--in', dest='infile',
-                    default=None, type=argparse.FileType('r'))
-    ap.add_argument('infilep', nargs='?', metavar='INFILE',
-                    default=sys.stdin, type=argparse.FileType('r'))
-    ap.add_argument('-o', '--out', dest='outfile',
-                    default=None, type=argparse.FileType('w'))
-    ap.add_argument('outfilep', nargs='?', metavar='OUTFILE',
-                    default=sys.stdout, type=argparse.FileType('w'))
-
-    # script specific options
-    ap.add_argument('-m', '--model', type=argparse.FileType('rb'),
-                    required=True)
-    ap.set_defaults(ns=False)
-    ap.add_argument("--ns", dest="ns", action="store_true")
-    ap.set_defaults(info=True)
-    ap.add_argument('--hideinfo', dest='info', action='store_false')
-
-    # parsing
-    opt = ap.parse_args()
-
-    # post-processing for command-line options
-    # disable logging messages by changing logging level
-    if not opt.verbose:
-        logger.setLevel(logging.ERROR)
-
-    # set random seed
-    np.random.seed(opt.rseed)
-
-    # basic file i/o
-    if opt.infile is None:
-        opt.infile = opt.infilep
-    del vars(opt)['infilep']
-    logger.info("input_file = " + opt.infile.name)
-    if opt.outfile is None:
-        opt.outfile = opt.outfilep
-    del vars(opt)['outfilep']
-    logger.info("output_file = " + opt.outfile.name)
-
-    ### set meta-data of script and machine
-    opt.script_name = script_name
-    opt.script_version = __version__
-    opt.python_version = platform.python_version()
-    opt.sys_uname = platform.uname()
-    if platform.system() == 'Darwin':
-        opt.sys_info =\
-        getoutput('system_profiler'
-                  ' -detailLevel mini SPHardwareDataType')\
-        .split('\n')[4:-1]
-    elif platform.system() == 'FreeBSD':
-        opt.sys_info = getoutput('sysctl hw').split('\n')
-    elif platform.system() == 'Linux':
-        opt.sys_info = getoutput('cat /proc/cpuinfo').split('\n')
-
-    ### suppress warnings in numerical computation
-    np.seterr(all='ignore')
-
-    ### call main routine
-    main(opt)
diff --git a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/train_pr.py b/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/train_pr.py
deleted file mode 100644
index 9574666..0000000
--- a/aif360/algorithms/inprocessing/kamfadm-2012ecmlpkdd/train_pr.py
+++ /dev/null
@@ -1,319 +0,0 @@
-#!/usr/bin/env python
-# -*- coding: utf-8 -*-
-"""
-training logistic regression
-
-SYNOPSIS::
-
-    SCRIPT [options]
-
-Description
-===========
-
-The last column indicates binary class.
-
-Options
-=======
-
--i <INPUT>, --in <INPUT>
-    specify <INPUT> file name
--o <OUTPUT>, --out <OUTPUT>
-    specify <OUTPUT> file name
--C <REG>, --reg <REG>
-    regularization parameter (default 1.0)
--e <eta>, --eta <eta>
-    fairness penalty parameter (default 1.0)
--l <LTYPE>, --ltype <LTYPE>
-    likehood fitting type (default 4)
--t <NTRY>, --try <NTRY>
-    the number of trials with random restart. if 0, all coefficients are
-    initialized by zeros, and a model is trained only once. (default 0)
--n <ITYPE>, --itype <ITYPE>
-    method to initialize coefficients. 0: by zero, 1: at random following
-    normal distribution, 2: learned by standard LR, 3: separately learned by
-    standard LR (default 3)
--q, --quiet
-    set logging level to ERROR, no messages unless errors
---rseed <RSEED>
-    random number seed. if None, use /dev/urandom (default None)
---version
-    show version
-
-Attributes
-==========
-N_NS : int
-    the number of non sensitive features
-"""
-
-#==============================================================================
-# Module metadata variables
-#==============================================================================
-
-__author__ = "Toshihiro Kamishima ( http://www.kamishima.net/ )"
-__date__ = "2012/08/26"
-__version__ = "3.0.0"
-__copyright__ = "Copyright (c) 2011 Toshihiro Kamishima all rights reserved."
-__license__ = "MIT License: http://www.opensource.org/licenses/mit-license.php"
-__docformat__ = "restructuredtext en"
-
-#==============================================================================
-# Imports
-#==============================================================================
-
-
-import sys
-import argparse
-import os
-import platform
-from subprocess import getoutput
-import logging
-import datetime
-import pickle
-import numpy as np
-
-# private modeules ------------------------------------------------------------
-import site
-site.addsitedir('.')
-
-from fadm import __version__ as fadm_version
-from sklearn import __version__ as sklearn_version
-from fadm.util import fill_missing_with_mean
-from fadm.lr.pr import *
-
-#==============================================================================
-# Public symbols
-#==============================================================================
-
-__all__ = []
-
-#==============================================================================
-# Constants
-#==============================================================================
-
-N_NS = 1
-
-#==============================================================================
-# Module variables
-#==============================================================================
-
-#==============================================================================
-# Classes
-#==============================================================================
-
-#==============================================================================
-# Functions
-#==============================================================================
-
-def train(X, y, ns, opt):
-    """ train model
-
-    Parameters
-    ----------
-    X : ary, shape=(n_samples, n_features)
-        features
-    y : ary, shape=(n_samples)
-        classes
-    ns : int
-        the number of sensitive features
-    opt : object
-        options
-
-    Returns
-    -------
-    clr : classifier object
-        trained classifier
-    """
-    if opt.ltype == 4:
-        clr = LRwPRType4(eta=opt.eta, C=opt.C)
-        clr.fit(X, y, ns, itype=opt.itype)
-    else:
-        sys.exit("Illegal likelihood fitting type")
-
-    return clr
-
-#==============================================================================
-# Main routine
-#==============================================================================
-
-def main(opt):
-    """ Main routine that exits with status code 0
-    """
-
-    ### pre process
-
-    # read data
-    D = np.loadtxt(opt.infile)
-
-    # split data and process missing values
-    y = np.array(D[:, -1])
-    X = fill_missing_with_mean(D[:, :-1])
-    del D
-
-    ### main process
-
-    # set starting time
-    start_time = datetime.datetime.now()
-    start_utime = os.times()[0]
-    opt.start_time = start_time.isoformat()
-    logger.info("start time = " + start_time.isoformat())
-
-    # init constants
-    ns = 1
-
-    # train
-    if opt.ntry <= 0:
-        # train only once with zero coefficients
-        clr = train(X, y, ns, opt)
-        opt.final_loss = clr.f_loss_
-        logger.info('final_loss = ' + str(opt.final_loss))
-    else:
-        # train multiple times with random restarts
-        clr = None
-        best_loss = np.inf
-        best_trial = 0
-        for trial in range(opt.ntry):
-            logger.info("Trial No. " + str(trial + 1))
-            tmp_clr = train(X, y, ns, opt)
-            logger.info("loss = " + str(tmp_clr.f_loss_))
-            if tmp_clr.f_loss_ < best_loss:
-                clr = tmp_clr
-                best_loss = clr.f_loss_
-                best_trial = trial + 1
-        opt.final_loss = best_loss
-        logger.info('final_loss = ' + str(opt.final_loss))
-        opt.best_trial = best_trial
-        logger.info('best_trial = ' + str(opt.best_trial))
-
-    # set end and elapsed time
-    end_time = datetime.datetime.now()
-    end_utime = os.times()[0]
-    logger.info("end time = " + end_time.isoformat())
-    opt.end_time = end_time.isoformat()
-    logger.info("elapsed_time = " + str((end_time - start_time)))
-    opt.elapsed_time = str((end_time - start_time))
-    logger.info("elapsed_utime = " + str((end_utime - start_utime)))
-    opt.elapsed_utime = str((end_utime - start_utime))
-
-    ### output
-
-    # add info
-    opt.nos_samples = X.shape[0]
-    logger.info('nos_samples = ' + str(opt.nos_samples))
-    opt.nos_features = X.shape[1]
-    logger.info('nos_features = ' + str(X.shape[1]))
-    opt.classifier = clr.__class__.__name__
-    logger.info('classifier = ' + opt.classifier)
-    opt.fadm_version = fadm_version
-    logger.info('fadm_version = ' + opt.fadm_version)
-    opt.sklearn_version = sklearn_version
-    logger.info('sklearn_version = ' + opt.sklearn_version)
-#    opt.training_score = clr.score(X, y)
-#    logger.info('training_score = ' + str(opt.training_score))
-
-    # write file
-    pickle.dump(clr, opt.outfile)
-    info = {}
-    for key, key_val in vars(opt).items():
-        info[key] = str(key_val)
-    pickle.dump(info, opt.outfile)
-
-    ### post process
-
-    # close file
-    if opt.infile is not sys.stdin:
-        opt.infile.close()
-
-    if opt.outfile is not sys.stdout:
-        opt.outfile.close()
-
-    sys.exit(0)
-
-
-### Preliminary processes before executing a main routine
-if __name__ == '__main__':
-    ### set script name
-    script_name = sys.argv[0].split('/')[-1]
-
-    ### init logging system
-    logger = logging.getLogger(script_name)
-    logging.basicConfig(level=logging.INFO,
-                        format='[%(name)s: %(levelname)s'
-                               ' @ %(asctime)s] %(message)s')
-
-    ### command-line option parsing
-
-    ap = argparse.ArgumentParser(
-        description='pydoc is useful for learning the details.')
-
-    # common options
-    ap.add_argument('--version', action='version',
-                    version='%(prog)s ' + __version__)
-
-    apg = ap.add_mutually_exclusive_group()
-    apg.set_defaults(verbose=True)
-    apg.add_argument('--verbose', action='store_true')
-    apg.add_argument('-q', '--quiet', action='store_false', dest='verbose')
-
-    ap.add_argument("--rseed", type=int, default=None)
-
-    # basic file i/o
-    ap.add_argument('-i', '--in', dest='infile',
-                    default=None, type=argparse.FileType('r'))
-    ap.add_argument('infilep', nargs='?', metavar='INFILE',
-                    default=sys.stdin, type=argparse.FileType('r'))
-    ap.add_argument('-o', '--out', dest='outfile',
-                    default=None, type=argparse.FileType('wb'))
-    ap.add_argument('outfilep', nargs='?', metavar='OUTFILE',
-                    default=sys.stdout, type=argparse.FileType('wb'))
-
-    # script specific options
-    ap.add_argument('-C', '--reg', dest='C', type=float, default=1.0)
-    ap.set_defaults(ns=False)
-    ap.add_argument('-e', '--eta', type=float, default=1.0)
-    ap.add_argument('-l', '--ltype', type=int, default=4)
-    ap.add_argument('-n', '--itype', type=int, default=3)
-    ap.set_defaults(ns=False)
-    ap.add_argument('--ns', dest='ns', action='store_true')
-    ap.add_argument('-t', '--try', dest='ntry', type=int, default=0)
-
-    # parsing
-    opt = ap.parse_args()
-
-    # post-processing for command-line options
-    # disable logging messages by changing logging level
-    if not opt.verbose:
-        logger.setLevel(logging.ERROR)
-
-    # set random seed
-    np.random.seed(opt.rseed)
-
-    # basic file i/o
-    if opt.infile is None:
-        opt.infile = opt.infilep
-    del vars(opt)['infilep']
-    logger.info("input_file = " + opt.infile.name)
-    if opt.outfile is None:
-        opt.outfile = opt.outfilep
-    del vars(opt)['outfilep']
-    logger.info("output_file = " + opt.outfile.name)
-
-    ### set meta-data of script and machine
-    opt.script_name = script_name
-    opt.script_version = __version__
-    opt.python_version = platform.python_version()
-    opt.sys_uname = platform.uname()
-    if platform.system() == 'Darwin':
-        opt.sys_info =\
-        getoutput('system_profiler'
-                  ' -detailLevel mini SPHardwareDataType')\
-        .split('\n')[4:-1]
-    elif platform.system() == 'FreeBSD':
-        opt.sys_info = getoutput('sysctl hw').split('\n')
-    elif platform.system() == 'Linux':
-        opt.sys_info = getoutput('cat /proc/cpuinfo').split('\n')
-
-    ### suppress warnings in numerical computation
-    np.seterr(all='ignore')
-
-    ### call main routine
-    main(opt)
diff --git a/aif360/algorithms/inprocessing/prejudice_remover.py b/aif360/algorithms/inprocessing/prejudice_remover.py
index e70c9ae..59290e2 100644
--- a/aif360/algorithms/inprocessing/prejudice_remover.py
+++ b/aif360/algorithms/inprocessing/prejudice_remover.py
@@ -17,15 +17,15 @@ See: https://github.com/algofairness/fairness-comparison/tree/master/fairness/al
 
 Changes made to fairness-comparison code:
     * removed all files not used by PrejudiceRemover algorithm:
-        - kamfadm-2012ecmlpkdd/data/*
-        - kamfadm-2012ecmlpkdd/fadm/eval/*
-        - kamfadm-2012ecmlpkdd/fadm/nb/*
-        - kamfadm-2012ecmlpkdd/fai_bin_bin.py
-        - kamfadm-2012ecmlpkdd/predict_nb.py
-        - kamfadm-2012ecmlpkdd/train_cv2nb.py
-        - kamfadm-2012ecmlpkdd/train_lr.py
-        - kamfadm-2012ecmlpkdd/train_nb.py
-    * fixed typo in kamfadm-2012ecmlpkdd/fadm/lr/pr.py:244 (typeError -> TypeError)
+        - kamfadm_2012ecmlpkdd/data/*
+        - kamfadm_2012ecmlpkdd/fadm/eval/*
+        - kamfadm_2012ecmlpkdd/fadm/nb/*
+        - kamfadm_2012ecmlpkdd/fai_bin_bin.py
+        - kamfadm_2012ecmlpkdd/predict_nb.py
+        - kamfadm_2012ecmlpkdd/train_cv2nb.py
+        - kamfadm_2012ecmlpkdd/train_lr.py
+        - kamfadm_2012ecmlpkdd/train_nb.py
+    * fixed typo in kamfadm_2012ecmlpkdd/fadm/lr/pr.py:244 (typeError -> TypeError)
     * removed commands.py and instead use subprocess.getoutput
 
 Notes from fairness-comparison's KamishimaAlgorithm.py on changes made to
@@ -61,7 +61,7 @@ original Kamishima code.
     second-to-last column as the sensitive attribute, and pr.py:265-268
     will take the remaining columns as non-sensitive.
 
-The code in kamfadm-2012ecmlpkdd/ is the (fairness-comparison) modified version
+The code in kamfadm_2012ecmlpkdd/ is the (fairness-comparison) modified version
 of the original ECML paper code.
 
 See: changes-to-downloaded-code.diff and KamishimaAlgorithm.py for more details.
@@ -152,7 +152,7 @@ class PrejudiceRemover(Transformer):
         # ADDED FOLLOWING LINE to get absolute path of this file, i.e.
         # prejudice_remover.py
         k_path = os.path.dirname(os.path.abspath(__file__))
-        train_pr = os.path.join(k_path, 'kamfadm-2012ecmlpkdd', 'train_pr.py')
+        train_pr = os.path.join(k_path, 'kamfadm_2012ecmlpkdd', 'train_pr.py')
         # changed paths in the calls below to (a) specify path of train_pr,
         # predict_lr RELATIVE to this file, and (b) compute & use absolute path
         #  and (c) replace python3 with python
@@ -194,7 +194,7 @@ class PrejudiceRemover(Transformer):
         # ADDED FOLLOWING LINE to get absolute path of this file, i.e.
         # prejudice_remover.py
         k_path = os.path.dirname(os.path.abspath(__file__))
-        predict_lr = os.path.join(k_path, 'kamfadm-2012ecmlpkdd', 'predict_lr.py')
+        predict_lr = os.path.join(k_path, 'kamfadm_2012ecmlpkdd', 'predict_lr.py')
         # changed paths in the calls below to (a) specify path of train_pr,
         # predict_lr RELATIVE to this file, and (b) compute & use absolute path,
         # and (c) replace python3 with python
