{% set name = "llm-guard" %}
{% set version = "0.3.7" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/llm-guard-{{ version }}.tar.gz
  sha256: 95aff27c8081cd9157b66911b76f8b4e63a17981feeaa6128f9a3265da31cb8e

build:
  number: 0
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation

requirements:
  host:
    - python >=3.9
    - setuptools
    - wheel
    - pip
  run:
    - python >=3.9
    - detect-secrets ==1.4.0
    - faker >=22,<23
    - fuzzysearch ==0.7.3
    - json-repair ==0.4.5
    - nltk >=3.8,<4
    - presidio-analyzer >=2.2,<3
    - presidio-anonymizer >=2.2,<3
    - protobuf >=3.20,<4
    - regex ==2023.12.25
    - sentencepiece ==0.1.99
    - tiktoken >=0.5,<0.6
    - pytorch ==2.0.1
    - transformers ==4.36.2
    - xformers ==0.0.22
    - span-marker ==1.5.0

test:
  imports:
    - llm_guard
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://llm-guard.com
  summary: LLM Guard by Laiyer.ai is a comprehensive tool designed to fortify the security of Large Language Models (LLMs).
  license: MIT
  license_file: LICENSE
  description: |
    LLM-Guard is a comprehensive tool designed to fortify the security of Large Language Models (LLMs). 
    By offering sanitization, detection of harmful language, prevention of data leakage, and resistance 
    against prompt injection attacks, LLM-Guard ensures that your interactions with LLMs remain safe and 
    secure.

    ![banner-image](https://llm-guard.com/assets/flow.png)

    PyPI: [https://pypi.org/project/{{ name | lower }}/](https://pypi.org/project/{{ name | lower }}/)

  doc_url: https://laiyer-ai.github.io/llm-guard
  dev_url: https://github.com/laiyer-ai/llm-guard

extra:
  recipe-maintainers:
    - sugatoray
