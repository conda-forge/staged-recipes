{% set name = "tokenizers" %}
{% set version = "0.8.0" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 703101ffc1cce87e39a8fa9754126a5c29590b03817a73727e3268474dc716e6

build:
  number: 0
  noarch: python
  script: "{{ PYTHON }} setup.py install"

requirements:
  build:
    - python
    - rust
    - setuptools
    - setuptools-rust
  host:
    - python
    - setuptools
    - setuptools-rust
  run:
    - python

test:
  imports:
    - tokenizers
    - tokenizers.models
    - tokenizers.decoders
    - tokenizers.normalizers
    - tokenizers.pre_tokenizers
    - tokenizers.processors
    - tokenizers.trainers
    - tokenizers.implementations

about:
  home: https://github.com/huggingface/tokenizers
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE
  summary: Provides an implementation of today's most used tokenizers, with a focus on performance and versatility.

  doc_url: https://github.com/huggingface/tokenizers
  dev_url: https://github.com/huggingface/tokenizers

extra:
  recipe-maintainers:
    - setu4993
