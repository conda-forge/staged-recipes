context:
  version: 0.1.1

package:
  name: parquet-stream-writer
  version: ${{ version }}

source:
- url: https://pypi.org/packages/source/p/parquet-stream-writer/parquet_stream_writer-${{ version }}.tar.gz
  sha256: f9c306ea5ded59ca809741824a3a0fb4b6ab173af046f135dd770d54a9097d43

build:
  script: ${{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  noarch: python
  number: 0

requirements:
  host:
  - python ${{ python_min }}.*
  - uv-build >=0.9,<0.10
  - pip
  run:
  - python >=${{ python_min }}
  - pyarrow >=21.0

tests:
- python:
    imports:
    - parquet_stream_writer
    pip_check: true
    python_version: ${{ python_min }}.*

about:
  license: MIT
  license_file: LICENSE
  homepage: https://github.com/apcamargo/parquet-stream-writer
  summary: Write streaming data to Parquet files with automatic sharding.
  description: |
    `parquet-stream-writer` provides a memory-efficient way to write streaming
    data to Parquet. It buffers incoming records and writes them incrementally
    to disk. When a configurable size threshold is reached, it starts a new
    Parquet shard, avoiding the need to load the entire dataset into memory.
    This makes this library suitable for datasets that are too large to fit in
    the available memory or for continuously generated data.

extra:
  recipe-maintainers:
    - apcamargo
