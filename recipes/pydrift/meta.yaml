{% set name = "pydrift" %}
{% set version = "0.2.1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 5b8df1d8780b8bf15a4756564bb01860244664f30e22da8cb92bf21f763c3818

build:
  noarch: python
  number: 0
  script: "{{ PYTHON }} -m pip install . -vv"

requirements:
  host:
    - python
    - pip
    - poetry
  run:
    - python
    - pip
    - catboost >=0.23
    - pandas >=1.0.3
    - scikit-learn >=0.23.1
    - shap >=0.35.0
    - typing-extensions >=3.7.4

test:
  imports:
    - pydrift

about:
  home: https://github.com/sergiocalde94/Data-And-Model-Drift-Checker
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: 'Measuring the degradation of a machine learning model'

  description: |
    How do we measure the degradation of a machine learning process?
    Why does the performance of our predictive models decrease? Maybe it is
    that a data source has changed (one or more variables) or maybe what
    changes is the relationship of these variables with the target we want
    to predict. `pydrift` tries to facilitate this task to the data scientist,
    performing this kind of checks and somehow measuring that degradation.
  doc_url: https://sergiocalde94.github.io/Data-And-Model-Drift-Checker/
  dev_url: https://github.com/sergiocalde94/Data-And-Model-Drift-Checker/

extra:
  recipe-maintainers:
    - sergiocalde94
