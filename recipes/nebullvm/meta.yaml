{% set name = "nebullvm" %}
{% set version = "0.1.1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/nebullvm-{{ version }}.tar.gz
  sha256: 17097a88f17ddce5be172bd50752216c3f41b3b3ce32ed4dc20cf2cd485a4f19

build:
  number: 0
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv --no-deps

requirements:
  host:
    - pip
    - openvino
    - python >=3.7
  run:
    - python >=3.7
    - joblib >=1.1.0
    - numpy >=1.19.0
    - onnx >=1.10.0
    - py-cpuinfo >=8.0.0
    - tensorflow >=2.7.0
    - tf2onnx >=1.8.4
    - pytorch >=1.10.0
    - openvino

test:
  imports:
    - nebullvm
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://pypi.org/project/nebullvm/
  summary: |
    All-in-one library that allows you to test multiple DL compilers in one line 
    of code and speed up the inference of your DL models by 5-20 times.
  license: Apache-2.0
  license_file: LICENSE
  description: |
    <img src="https://user-images.githubusercontent.com/83510798/155317935-523dcf79-9adb-4131-9511-8e269a1f97dd.png">

    ## Nebullvm

    > _**All-in-one library that allows you to test multiple DL compilers in one line of code and speed up the inference of your DL models by 5-20 times.**_

    This repository contains the opensource `nebullvm` package, an opensource project
    aiming to reunite all the opensource AI compilers under the same easy-to-use interface.

    You will love this library if:<br />
    ðŸš€ you want to speed up the response time of your AI models;\
    ðŸ‘Ÿ you don't want to test all DL compilers on the market, 
    but you just want to know the best one for your specific application;\
    ðŸ¥‡ you enjoy simplifying complex problems: in fact with one 
    line of code you can know which DL compiler is best suited for your 
    application;\
    ðŸ’™ you are passionate about AI performance optimization.

    We designed something that is super easy to use: you just need to 
    input your DL model and you will automatically get back an 
    optimized version of the model for the hardware where you performed 
    the optimization.   

    PyPI: [https://pypi.org/project/nebullvm/](https://pypi.org/project/nebullvm/)

  doc_url: 
  dev_url: https://github.com/nebuly-ai/nebullvm

extra:
  recipe-maintainers:
    - sugatoray
