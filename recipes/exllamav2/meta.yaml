{% set version = "0.1.8" %}
{% set torch_proc_type = "cuda" ~ cuda_compiler_version | replace('.', '') if cuda_compiler_version != "None" else "cpu" %}

package:
  name: exllamav2
  version: {{ version }}

source:
  url: https://github.com/turboderp/exllamav2/archive/refs/tags/v{{ version }}.tar.gz
  sha256: 69a82bd8b4904b19b12bde8228d49216bf4fb4de0ae3d75922a6b3057ea448e3

build:
  # noarch: python
  script_env:
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX      # [cuda_compiler_version == "11.2"]
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9+PTX  # [cuda_compiler_version == "11.8"]
    - TORCH_CUDA_ARCH_LIST=5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX  # [(cuda_compiler_version or "").startswith("12")]
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0
  skip: true  # [win]
  skip: true  # [cuda_compiler_version == "None"]
  skip: true  # [py < 39]
  #skip: true  # [py > 311]
  rpaths:
    - lib/
    # PyTorch libs are in site-packages intead of with other shared objects
    - {{ SP_DIR }}/torch/lib/
  missing_dso_whitelist:
    - "*/libtorch_python.so"

requirements:
  build:
    - cmake
    - make
    - {{ stdlib("c") }}
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
    
  host:
    - cuda-version {{ cuda_compiler_version }}
    - python
    - pip
    - setuptools
    - pytorch
    - torchvision
    - ninja
    
  run:
    - python
    - pandas
    - ninja
    - fastparquet
    - pytorch >=2.2.0
    - safetensors >=0.3.2
    - sentencepiece >=0.1.97
    - pygments
    - websockets
    - regex
    - numpy~=1.26.4
    - tokenizers
    - rich

test:
  imports:
    - exllamav2
  requires:
    - pip

about:
  home: https://github.com/turboderp/exllamav2
  license: MIT
  license_file: LICENSE
  summary: ExLlamaV2 is an inference library for running local LLMs on modern consumer GPUs.

extra:
  recipe-maintainers:
    - mediocretech
