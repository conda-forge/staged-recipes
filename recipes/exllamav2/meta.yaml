{% set version = "0.1.5" %}
{% set torch_proc_type = "cuda" ~ cuda_compiler_version | replace('.', '') if cuda_compiler_version != "None" else "cpu" %}

package:
  name: exllamav2
  version: {{ version }}

source:
  url: https://github.com/turboderp/exllamav2/archive/refs/tags/v{{ version }}.tar.gz
  sha256: e9326b2bfef15d983cb8d03a86976c4ca3197dd31d2964e9522d16643543eea9

build:
  # noarch: python
  script_env:
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX      # [cuda_compiler_version == "11.2"]
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9+PTX  # [cuda_compiler_version == "11.8"]
    - TORCH_CUDA_ARCH_LIST=5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX  # [(cuda_compiler_version or "").startswith("12")]
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0
  skip: true  # [win]
  skip: true  # [cuda_compiler_version == "None"]
  skip: true  # [py < 38]
  skip: true  # [py > 311]
  rpaths:
    - lib/
    # PyTorch libs are in site-packages intead of with other shared objects
    - {{ SP_DIR }}/torch/lib/
  missing_dso_whitelist:
    - "*/libtorch_python.so"

requirements:
  build:
    - cmake
    - make
    - {{ stdlib("c") }}
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('gxx') }}
    - {{ compiler('cuda') }}
    
  host:
    - cuda-version {{ cuda_compiler_version }}
    - python
    - pip
    - pytorch
    - torchvision
    - ninja
    
  run:
    - python
    - pandas
    - ninja
    - fastparquet
    - pytorch >=2.2.0
    - safetensors >=0.3.2
    - sentencepiece >=0.1.97
    - pygments
    - websockets
    - regex
    - numpy
    - cudatoolkit
    - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]

    # - python
    # - pandas
    # - ninja
    # - fastparquet
    # - pytorch >=2.1.0
    # - safetensors >=0.3.2
    # - sentencepiece >=0.1.97
    # - pygments
    # - websockets
    # - regex
    # - pytorch ~=2.1.0
    # - libtorch
    # - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
    - cuda-cupti-dev    # [(cuda_compiler_version or "").startswith("12")]
    - cuda-cudart-dev   # [(cuda_compiler_version or "").startswith("12")]
    - cuda-nvml-dev     # [(cuda_compiler_version or "").startswith("12")]
    - cuda-nvtx-dev     # [(cuda_compiler_version or "").startswith("12")]

test:
  imports:
    - exllamav2
  requires:
    - pip

about:
  home: https://github.com/turboderp/exllamav2
  license: MIT
  license_file: LICENSE
  summary: ExLlamaV2 is an inference library for running local LLMs on modern consumer GPUs.

extra:
  recipe-maintainers:
    - mediocretech
