{% set name = "win_xgboost" %}
{% set version = "0.7.post3" %}
{% set sha256 = "4224193159763ab50352b3c53087510a9ebef4c9dcbafb5c5f8c7e2a1d6d7b70" %}

package:
  name: win_xgboost
  version: 0.7.post3

source:
  fn: xgboost-0.7.post3.tar.gz
  url: https://pypi.python.org/packages/cb/1c/a2ec1798d444e2b86b27fdbf18d9ab08c25c3d374de77429b55a76f42404/xgboost-0.7.post3.tar.gz
  sha256: 4224193159763ab50352b3c53087510a9ebef4c9dcbafb5c5f8c7e2a1d6d7b70

build:
  number: 1
  script: python setup.py install --single-version-externally-managed --record record.txt
  skip: True  # [py<36]
  skip: True  # [not win]

requirements:
  build:
    - python
    - pip
    - numpy
    - scipy
    - scikit-learn
    - matplotlib
    - setuptools
    - openssl
    - xz
    - wheel
    - sqlite
    - zlib


  run:
    - python

test:
  imports:
    - xgboost
  commands:
    - python -c "import xgboost;"

about:
  home: https://github.com/dmlc/xgboost
  license: Apache-2.0
  license_family: Apache
  license_file: '{{ environ["RECIPE_DIR"] }}/LICENSE'
  summary: 'Scalable, Portable and Distributed Gradient Boosting Library'

  description: |
    XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.
  doc_url: https://xgboost.readthedocs.io/en/latest/
  dev_url: http://dmlc.ml/

extra:
  recipe-maintainers:
    - bricewalker
