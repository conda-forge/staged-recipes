schema_version: 1

context:
  name: aphrodite-flash-attn
  version: 2.6.1.post2

package:
  name: ${{ name|lower }}
  version: ${{ version }}

source:
  url: https://github.com/AlpinDale/flash-attention/archive/v${{ version }}.tar.gz
  sha256: 081ff239ec3008c2b9d298795fa76de57025beabbdf9dd092f9ffa7f463ae318

build:
  number: 0
  skip: (linux and cuda_compiler_version != "12.4") and not linux
  script: ${{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation

requirements:
  build:
    - ${{ compiler('c') }}
    - ${{ compiler('cxx') }}
    - ${{ compiler('cuda') }}
    - ${{ stdlib("c") }}
  host:
    - python
    - psutil
    - pip
    - pytorch ==2.4.0
    - setuptools
    - cuda-version ${{ cuda_compiler_version }}
    - cutlass
  run:
    - python
    - pytorch ==2.4.0
    - cuda-version ${{ cuda_compiler_version }}

tests:
  - python:
      imports:
        - aphrodite_flash_attn
  - requirements:
      run:
        - pip
    script:
      - pip check

about:
  summary: Cross-platform lib for process and system monitoring in Python.
  license: BSD-3-Clause
  license_file: LICENSE
  homepage: https://github.com/AlpinDale/flash-attention
  repository: https://github.com/AlpinDale/flash-attention

extra:
  recipe-maintainers:
    - HeavyTony2

