{% set torchserve_version = "0.12.0" %}
# see https://github.com/pytorch/serve/blob/master/model-archiver/model_archiver/version.txt
{% set model_archiver_version = "0.12.0" %}

package:
  name: torchserve-model-archiver
  version: {{ model_archiver_version }}

source:
  url: https://github.com/pytorch/serve/archive/v{{ torchserve_version }}.tar.gz
  sha256: d5eb31a540b6a1dea4ead5c8b122d8b7d40b06fe2fc257ba67ed5b071852be0b

build:
  noarch: python
  script: {{ PYTHON }} -m pip install model-archiver/ -vv
  number: 0
  entry_points:
    - torch-model-archiver = model_archiver.model_packaging:generate_model_archive

requirements:
  host:
    - python {{ python_min }}
    - pip
    - setuptools
  run:
    - python >={{ python_min }}
    - enum-compat

test:
  commands:
    - torch-model-archiver -v
  requires:
    - python {{ python_min }}

about:
  home: https://github.com/pytorch/serve/blob/master/model-archiver/README.md
  summary: Creates archives of trained neural net models that can be consumed by TorchServe for inference.
  description: |
    A key feature of TorchServe is the ability to package all model artifacts into a single model archive file. It is a separate command line interface (CLI), torch-model-archiver, that can take model checkpoints or model definition file with state_dict, and package them into a .mar file. This file can then be redistributed and served by anyone using TorchServe. It takes in the following model artifacts: a model checkpoint file in case of torchscript or a model definition file and a state_dict file in case of eager mode, and other optional assets that may be required to serve the model. The CLI creates a .mar file that TorchServe's server CLI uses to serve the models.
  license: Apache-2.0
  license_file: LICENSE
  doc_url: https://pytorch.org/serve/
  dev_url: https://github.com/pytorch/serve

extra:
  recipe-maintainers:
    - tdsmith
