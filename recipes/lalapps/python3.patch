--- ./src/calibration/lalapps_noise_comp_pipe.py	(original)
+++ ./src/calibration/lalapps_noise_comp_pipe.py	(refactored)
@@ -77,18 +77,18 @@
 df_pad=128
 
 if not opts.config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.segment_filename:
-  print >> sys.stderr, "No segment filename specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No segment filename specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.basename:
-  print >> sys.stderr, "No dag file base name specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No dag file base name specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 # create the config parser object and read in the ini file
@@ -142,11 +142,11 @@
 epoch_cnt = 0;
 
 # loop over the segments defined by the calibration epochs
-print "\n"
+print("\n")
 for epoch in epochs.epoch_segs():
   noise_output_files = []
   noise_output_files2 = []
-  print "setting up jobs for calibration epoch " + str(epoch[1])+" - "+str(epoch[2]) + "..."
+  print("setting up jobs for calibration epoch " + str(epoch[1])+" - "+str(epoch[2]) + "...")
   #output the epochs in their own directories
   epoch_dir = 'EPOCH'+'-'+str(epoch[1])+'-'+str(epoch[2])
   mkdir_node2 = strain.MkdirNode(mkdir_job,epoch_dir)
@@ -270,7 +270,7 @@
   if opts.write_dax: dag.write_pegasus_rls_cache(cp.get("ldgsubmitdax","gsiftp"),cp.get("ldgsubmitdax","pool"))
   if opts.write_script: dag.write_script()
 
-  print "\nDAG contains " + str(len(dag.get_nodes())) + " nodes.\n"
+  print("\nDAG contains " + str(len(dag.get_nodes())) + " nodes.\n")
 
   # write out a log file for this script
   log_fh = open(opts.basename + '.pipeline.log', 'w')
@@ -291,21 +291,21 @@
   for seg in data:
     for chunk in seg:
       total_data += len(chunk)
-  print >> log_fh, "total data =", total_data
-
-  print >> log_fh, "\n===========================================\n"
-  print >> log_fh, data
+  print("total data =", total_data, file=log_fh)
+
+  print("\n===========================================\n", file=log_fh)
+  print(data, file=log_fh)
   for seg in data:
-    print >> log_fh, seg
+    print(seg, file=log_fh)
     for chunk in seg:
-      print >> log_fh, chunk, 'length', int(chunk.end())-int(chunk.start())
+      print(chunk, 'length', int(chunk.end())-int(chunk.start()), file=log_fh)
       endgps=chunk.end()
 
 if not opts.cat_noise_jobs:
   # write a message telling the user that the DAG has been written
-  print "\nCreated a DAG file which can be submitted by executing"
-  print "\n   condor_submit_dag", dag.get_dag_file()
-  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
+  print("\nCreated a DAG file which can be submitted by executing")
+  print("\n   condor_submit_dag", dag.get_dag_file())
+  print("""\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
   If you are running LSCdataFind jobs, do not forget to initialize your grid
   proxy certificate on the condor submit machine by running the commands
 
@@ -328,7 +328,7 @@
 
   Contact the administrator of your cluster to find the hostname and port of the
   LSCdataFind server.
-  """
+  """)
 
 sys.exit(0)
 
--- ./src/calibration/lalapps_strain_pipe.py	(original)
+++ ./src/calibration/lalapps_strain_pipe.py	(refactored)
@@ -34,7 +34,7 @@
   -f, --dag-file           basename for .dag file (excluding the .dag)
   -t, --aux-path           path to auxiliary files
 """
-  print >> sys.stderr, msg
+  print(msg, file=sys.stderr)
 
 # pasrse the command line options to figure out what we should do
 shortop = "hv:s:e:S:f:t:a:b:"
@@ -66,7 +66,7 @@
 
 for o, a in opts:
   if o in ("-v", "--version"):
-    print "$Id$"
+    print("$Id$")
     sys.exit(0)
   elif o in ("-h", "--help"):
     usage()
@@ -86,7 +86,7 @@
   elif o in ("-b", "--trig-end"):
     pass
   else:
-    print >> sys.stderr, "Unknown option:", o
+    print("Unknown option:", o, file=sys.stderr)
     usage()
     sys.exit(1)
 
@@ -95,29 +95,29 @@
 df_pad=128
 
 if not config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if (not GPSStart or not GPSEnd) and not segment_filename:
-  print >> sys.stderr, "No GPS start time and end times or segment filename specified."
-  print >> sys.stderr, "Either GPS start time and end times, or a segment filename must be specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No GPS start time and end times or segment filename specified.", file=sys.stderr)
+  print("Either GPS start time and end times, or a segment filename must be specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not basename:
-  print >> sys.stderr, "No dag file base name specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No dag file base name specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not aux_path:
-  print >> sys.stderr, "No auxiliary file path specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No auxiliary file path specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 # try and make a directory to store the cache files and job logs
@@ -159,7 +159,7 @@
 # if runnign on-line make segments filename
 if running_online:
   segment_file=open('strain_segment.txt',mode='w')
-  print >> segment_file, '1',GPSStart,' ',GPSEnd,' ',int(GPSEnd)-int(GPSStart)
+  print('1',GPSStart,' ',GPSEnd,' ',int(GPSEnd)-int(GPSStart), file=segment_file)
   segment_file.close()
   segment_filename='strain_segment.txt'
 
@@ -206,7 +206,7 @@
 
   command = "/archive/home/xsiemens/lscsoft/glue/bin/LSCdataFindcheck --gps-start-time "+str(seg.start())+\
   " --gps-end-time "+str(seg.end())+" "+df.get_output()
-  print >> cachecheck_file, command
+  print(command, file=cachecheck_file)
 
   if prev_df:
     df.add_parent(prev_df)
@@ -222,11 +222,11 @@
     gps_str=str(chunk.start())
     gps_time_first_four=gps_str[0]+gps_str[1]+gps_str[2]+gps_str[3]
     try: os.mkdir(base_data_dirL1+'/'+ifo[0]+'-'+frametypeL1+'-'+gps_time_first_four)
-    except OSError, err:
+    except OSError as err:
       import errno
       #print "Warning:", err
     try: os.mkdir(base_data_dirL2+'/'+ifo[0]+'-'+frametypeL2+'-'+gps_time_first_four)
-    except OSError, err:
+    except OSError as err:
       import errno
       #print "Warning:", err
 
@@ -241,9 +241,9 @@
     strain1.add_parent(df)
     dag.add_node(strain1)
 
-    print >> framelist_file, 'ls '+directory+'/'+ifo[0] \
+    print('ls '+directory+'/'+ifo[0] \
     +'-'+frametype+'-'+str(int(chunk.start())+int(overlap))+'-' \
-    +str(int(chunk.end())-int(chunk.start())-2*int(overlap))+'.gwf > /dev/null'
+    +str(int(chunk.end())-int(chunk.start())-2*int(overlap))+'.gwf > /dev/null', file=framelist_file)
 
 cachecheck_file.close()
 framelist_file.close()
@@ -267,24 +267,24 @@
 for seg in data:
   for chunk in seg:
     total_data += len(chunk)
-print >> log_fh, "total data =", total_data
-
-print >> log_fh, "\n===========================================\n"
-print >> log_fh, data
+print("total data =", total_data, file=log_fh)
+
+print("\n===========================================\n", file=log_fh)
+print(data, file=log_fh)
 for seg in data:
-  print >> log_fh, seg
+  print(seg, file=log_fh)
   for chunk in seg:
-    print >> log_fh, chunk, 'length', int(chunk.end())-int(chunk.start())
+    print(chunk, 'length', int(chunk.end())-int(chunk.start()), file=log_fh)
     endgps=chunk.end()
 
 if running_online:
-  print >> sys.stdout, seg.start()+overlap,int(chunk.end())-overlap
+  print(seg.start()+overlap,int(chunk.end())-overlap, file=sys.stdout)
 
 if not running_online:
   # write a message telling the user that the DAG has been written
-  print "\nCreated a DAG file which can be submitted by executing"
-  print "\n   condor_submit_dag", dag.get_dag_file()
-  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
+  print("\nCreated a DAG file which can be submitted by executing")
+  print("\n   condor_submit_dag", dag.get_dag_file())
+  print("""\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
   If you are running LSCdataFind jobs, do not forget to initialize your grid
   proxy certificate on the condor submit machine by running the commands
 
@@ -307,7 +307,7 @@
 
   Contact the administrator of your cluster to find the hostname and port of the
   LSCdataFind server.
-  """
+  """)
 
 sys.exit(0)
 
--- ./src/calibration/strain.py	(original)
+++ ./src/calibration/strain.py	(refactored)
@@ -228,7 +228,7 @@
         # set the frame type based on the LFNs returned by datafind
         #self.add_var_opt('frame-type',b)
       else:
-        raise CondorDAGNodeError, "Unknown LFN cache format"
+        raise CondorDAGNodeError("Unknown LFN cache format")
 
 # Convenience functions to cat together noise output files.
 def open_noise_cat_file(dir):
@@ -300,11 +300,11 @@
     amphf[f] = 0.0
   freqcnt = 0;
 
-  print "\tfirst pass through systematics files..."
+  print("\tfirst pass through systematics files...")
   for file in flist:
     try: input = open(file,'r')
     except:
-      print "WARNING: file " + file + " doesn't exist"
+      print("WARNING: file " + file + " doesn't exist")
       continue
     for line in input.readlines():
       tmp = line.split()
@@ -394,12 +394,12 @@
 
   freqcnt = 0;
 
-  print "\tsecond pass through systematics files..."
+  print("\tsecond pass through systematics files...")
   #Compute the moments of the distribution
   for file in flist:
     try: input = open(file,'r')
     except:
-      print "WARNING: file " + file + " doesn't exist"
+      print("WARNING: file " + file + " doesn't exist")
       continue
     for line in input.readlines():
       tmp = line.split()
@@ -458,7 +458,7 @@
 
   fl.close()
   # Plot the results
-  print "\tplotting..."
+  print("\tplotting...")
   # Plot the systematic in magnitude
   magfigname = "sys_mag"+epoch[1]+"-"+epoch[2]+".png"
   figure(1)
@@ -592,7 +592,7 @@
   page.write('<h3>Raw distribution of residual noise</h3><hr><br>\n')
   for f in freq:
     #time.sleep(10)
-    print "plotting "+str(f)
+    print("plotting "+str(f))
     figname = "n_hist_"+str(f)+'_'+epoch[1]+"-"+epoch[2]+".png"
     #figure(1)
     plot(binVec,realHistVecs[f])
@@ -647,7 +647,7 @@
   for file in filelist:
     try: input = open(file,'r')
     except:
-      print "WARNING: file " + file + " doesn't exist"
+      print("WARNING: file " + file + " doesn't exist")
       continue
     #if STOP > 100: break
     #STOP+=1
@@ -769,7 +769,7 @@
   A = array(specList,typecode='f')
   figure(1)
   pcolor(X,Y,A.transpose(),shading='flat',vmin=0.95,vmax=1.05)
-  print "...plotting qscan for " + start
+  print("...plotting qscan for " + start)
   title('h(t) and h(f) power ratios per freq bin GPS '+start + '\n min = '+str(MIN) + ' max = '+str(MAX) )
   xlabel('Time')
   ylabel('Frequency')
@@ -897,7 +897,7 @@
       # just look at darm and h(t) for puny outliers.
       self.add_file_arg(qfile)
     else:
-      print ".....found 10% outlier running full qscan\n"
+      print(".....found 10% outlier running full qscan\n")
       # run the standard qscan on outliers greater than 10%
       self.add_file_arg(qfile+'FULL')
     self.add_var_arg('@default')
--- ./src/inspiral/inspiral.py	(original)
+++ ./src/inspiral/inspiral.py	(refactored)
@@ -59,7 +59,7 @@
         self.add_ini_opts(mycp, sec)
 
       else:
-        print >>sys.stderr, "warning: config file is missing section [" + sec + "]"
+        print("warning: config file is missing section [" + sec + "]", file=sys.stderr)
 
     self.set_stdout_file('logs/' + exec_name + \
         '-$(macrogpsstarttime)-$(macrogpsendtime)-$(cluster)-$(process).out')
@@ -269,8 +269,8 @@
       # make sure the vanilla universe is being used
       universe = cp.get('condor', 'universe')
       if universe != 'vanilla':
-        raise RuntimeError, 'Cannot run GPU inspiral jobs on Condor ' + \
-            universe + ' universe. Please use vanilla.'
+        raise RuntimeError('Cannot run GPU inspiral jobs on Condor ' + \
+            universe + ' universe. Please use vanilla.')
       # make sure the executable has CUDA dependencies
       executable = cp.get('condor', exec_name)
       objdump_re = re.compile(r'^\s*NEEDED\s*(libcufft\.|libcudart\.).*')
@@ -283,8 +283,8 @@
           cuda_deps = True
           break
       if not cuda_deps:
-        raise RuntimeError, 'Inspiral executable has no CUDA ' + \
-            'dependencies. Please use a CUDA-enabled build.'
+        raise RuntimeError('Inspiral executable has no CUDA ' + \
+            'dependencies. Please use a CUDA-enabled build.')
       self.add_opt('gpu-device-id', '0')
       self.add_condor_cmd('+WantGPU', 'true')
       self.add_condor_cmd('Requirements', '( GPU_PRESENT =?= true)')
@@ -420,7 +420,7 @@
             os.symlink(arg,os.path.split(arg)[-1])
             self.add_file_opt(opt,fname)
           except:
-            print >>sys.stderr, "sym link failed for " + arg + " grid workflows might be broken"
+            print("sym link failed for " + arg + " grid workflows might be broken", file=sys.stderr)
             self.add_file_opt(opt,arg)
         else:
           self.add_file_opt(opt,fname)
@@ -725,7 +725,7 @@
     IFO-EXECUTABLE_IFOTAG_USERTAG-GPS_START-DURATION
     """
     if not self.get_start() or not self.get_end() or not self.get_ifo():
-      raise InspiralError, "Start time, end time or ifo has not been set"
+      raise InspiralError("Start time, end time or ifo has not been set")
 
     filebase = self.get_ifo() + '-' + self.job().get_exec_name().upper()
 
@@ -905,7 +905,7 @@
     must be kept synchronized with the name of the output file in bbhinj.c.
     """
     if not self.get_start() or not self.get_end():
-      raise InspiralError, "Start time or end time has not been set"
+      raise InspiralError("Start time or end time has not been set")
     if self.get_user_tag():
       bbhinject = 'HL-INJECTIONS_' + self.get_user_tag() + '-'
       bbhinject = bbhinject + str(self.get_start()) + '-'
@@ -952,7 +952,7 @@
     be kept synchronized with the name of the output file in randombank.c.
     """
     if not self.get_start() or not self.get_end():
-      raise InspiralError, "Start time or end time has not been set"
+      raise InspiralError("Start time or end time has not been set")
     if self.get_user_tag():
       bank = 'P-TMPLTBANK_' + self.get_user_tag() + '-' 
       bank = bank + str(self.get_start())
@@ -999,7 +999,7 @@
     synchronized with the name of the output files in splitbank.c.
     """
     if not self.get_bank() or not self.get_num_banks():
-      raise InspiralError, "Bank file or number of banks has not been set"
+      raise InspiralError("Bank file or number of banks has not been set")
 
     banks = []
     x = self.__bankfile.split('-')
@@ -1233,7 +1233,7 @@
     synchronized with the name of the output file in inca.c.
     """
     if not self.get_start() or not self.get_end() or not self.get_ifo_a():
-      raise InspiralError, "Start time, end time or ifo a has not been set"
+      raise InspiralError("Start time, end time or ifo a has not been set")
 
     basename = self.get_ifo_a() + '-INCA'
 
@@ -1260,7 +1260,7 @@
     synchronized with the name of the output file in inca.c.
     """
     if not self.get_start() or not self.get_end() or not self.get_ifo_b():
-      raise InspiralError, "Start time, end time or ifo a has not been set"
+      raise InspiralError("Start time, end time or ifo a has not been set")
 
     basename = self.get_ifo_b() + '-INCA'
 
@@ -1405,7 +1405,7 @@
     synchronized with the name of the output file in thinca.c.
     """
     if not self.get_start() or not self.get_end() or not self.get_ifos():
-      raise InspiralError, "Start time, end time or ifos have not been set"
+      raise InspiralError("Start time, end time or ifos have not been set")
     
     if self.__num_slides:
       basename = self.get_ifos() + '-' + self.job().get_exec_name().upper() \
@@ -1468,7 +1468,7 @@
     thinca_to_coinc node is being run.
     """
     if not self.__input_cache:
-      raise ValueError, "no input-cache specified"
+      raise ValueError("no input-cache specified")
     # open the input cache file
     fp = open(self.__input_cache, 'r')
     input_cache = lal.Cache().fromfile(fp).sieve( description = coinc_file_tag )
@@ -1677,7 +1677,7 @@
     get the name of the output file
     """
     if not self.get_ifo():
-      raise InspiralError, "ifos have not been set"
+      raise InspiralError("ifos have not been set")
 
     fname = self.get_ifo() + "-SIRE"
     if self.get_inj_file():
@@ -1689,7 +1689,7 @@
 
     if (self.get_start() and not self.get_end()) or \
         (self.get_end() and not self.get_start()):
-      raise InspiralError, "If one of start and end is set, both must be"
+      raise InspiralError("If one of start and end is set, both must be")
 
     if (self.get_start()):
       duration=self.get_end()- self.get_start()
@@ -1832,15 +1832,15 @@
     get the name of the output file
     """
     if not self.get_ifos():
-      raise InspiralError, "ifos have not been set"
+      raise InspiralError("ifos have not been set")
 
     self.set_output_tag()
     fname = self.get_ifos() + '-' + self.get_output_tag()
 
     if (self.get_start() and not self.get_end()) or \
            (self.get_end() and not self.get_start()):
-      raise InspiralError, "If one of start and end is set, "\
-            "both must be"
+      raise InspiralError("If one of start and end is set, "\
+            "both must be")
 
     if (self.get_start()):
       duration=self.get_end() - self.get_start()
@@ -1963,7 +1963,7 @@
     """
 
     if not self.get_ifos():
-      raise InspiralError, "Ifos have not been set"
+      raise InspiralError("Ifos have not been set")
 
     basename = self.get_ifos() + '-COHBANK'
 
@@ -2021,7 +2021,7 @@
     """
 
     if not self.get_ifos():
-      raise InspiralError, "Ifos have not been set"
+      raise InspiralError("Ifos have not been set")
 
     basename = self.get_ifos() + '-COHINSPBANK'
 
@@ -2076,7 +2076,7 @@
     Returns the file name of output from coherent inspiral.
     """
     if not self.get_start() or not self.get_end() or not self.get_ifo_tag():
-      raise InspiralError, "Start time, end time or ifos have not been set"
+      raise InspiralError("Start time, end time or ifos have not been set")
       
     basename = self.get_ifo_tag() + '-CHIA'
 
@@ -2204,15 +2204,15 @@
     get the name of the output file
     """
     if not self.get_ifos():
-      raise InspiralError, "ifos have not been set"
+      raise InspiralError("ifos have not been set")
 
     self.set_output_tag()
     fname = self.get_ifos() + '-' + self.get_output_tag()
 
     if (self.get_start() and not self.get_end()) or \
            (self.get_end() and not self.get_start()):
-      raise InspiralError, "If one of start and end is set, "\
-            "both must be"
+      raise InspiralError("If one of start and end is set, "\
+            "both must be")
 
     if (self.get_start()):
       duration=self.get_end() - self.get_start()
--- ./src/inspiral/inspiralutils.py	(original)
+++ ./src/inspiral/inspiralutils.py	(refactored)
@@ -34,19 +34,19 @@
   """
   Run a program on the shell and print informative messages on failure.
   """
-  if show_command: print command
+  if show_command: print(command)
 
   p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
     shell=isinstance(command, str))
   out, err = p.communicate()
 
   if p.returncode != 0:
-      print >>sys.stderr, "External call failed."
-      print >>sys.stderr, "  stdout: %s" % out
-      print >>sys.stderr, "  stderr: %s" % err
+      print("External call failed.", file=sys.stderr)
+      print("  stdout: %s" % out, file=sys.stderr)
+      print("  stderr: %s" % err, file=sys.stderr)
       raise subprocess.CalledProcessError(p.returncode, command)
   if show_stdout:
-      print out
+      print(out)
 
 ##############################################################################
 def mkdir( newdir ):
@@ -174,7 +174,7 @@
   if not generate_segments: return segFindFile
 
   whichtoanalyze = ifo.lower() + "-analyze"
-  print "For " + ifo + ", analyze " +config.get("segments",whichtoanalyze)
+  print("For " + ifo + ", analyze " +config.get("segments",whichtoanalyze))
   
   # run segFind to determine science segments
   segFindCall = ' '.join([ config.get("condor", "segfind"),
@@ -205,8 +205,8 @@
   hwinjDefurl = config.get("hardware-injections", "hwinj-def-server-url")
   hwinjDefFile = config.get("hardware-injections", "hwinj-def-file")
 
-  print "Downloading hardware injection list " + hwinjDefFile + " from " \
-        + hwinjDefurl
+  print("Downloading hardware injection list " + hwinjDefFile + " from " \
+        + hwinjDefurl)
   hwinjDefFile, info = urllib.urlretrieve(hwinjDefurl + '/' + hwinjDefFile,
         hwinjDefFile)
   ifostr = ''
@@ -246,7 +246,7 @@
 	"--gps-end-time", end ])
   
   if generateVetoes:
-    print "Generating veto category xml files... this may take some time..."
+    print("Generating veto category xml files... this may take some time...")
     make_external_call(genVetoCall)
 
 ##############################################################################
@@ -264,11 +264,11 @@
   """
   
   if not (os.path.isfile(veto_cat_file) and os.access(veto_cat_file,os.R_OK) ):
-      print 'Veto file not found or unreadable, skipping %s'%(veto_cat_file)
+      print('Veto file not found or unreadable, skipping %s'%(veto_cat_file))
       return False
 
-  print "Converting veto-category xml file %s to txt file %s" \
-	%(veto_cat_file, output_file)
+  print("Converting veto-category xml file %s to txt file %s" \
+	%(veto_cat_file, output_file))
   sys.stdout.flush()
   # get xml-to-txt converter
   xml_to_txt_converter = config.get("condor", "ligolw_print")
@@ -329,8 +329,8 @@
     if generateVetoes: 
       return_val = convert_veto_cat_xml_to_txt(config, veto_cat_file, vetoFile)
       if not return_val:
-        print "No vetoes found for %s cat %i. %s will contain no segments." \
-          %(ifo, category, vetoFile) 
+        print("No vetoes found for %s cat %i. %s will contain no segments." \
+          %(ifo, category, vetoFile)) 
       
 
     # if there are previous vetoes, generate combined
@@ -406,8 +406,8 @@
   dataFindFile = ifo_type + "-" + str(start) + "-" + \
       str(end - start) + ".txt"
 
-  print "Running ligo_data_find to determine available data from " + type + \
-      " frames for " + ifo
+  print("Running ligo_data_find to determine available data from " + type + \
+      " frames for " + ifo)
   dataFindCall = executable 
   for opt,arg in config.items("datafind"):
     dataFindCall += " --" + opt + " " + arg
@@ -433,8 +433,8 @@
   rel_outfile = "../" + directory + "/" + os.path.basename(infile)
   if "veto-file" in vetoes:
     if infile == "":
-      print >>sys.stderr, "warning: " + vetoes + " left blank; proceeding "\
-        "without DQ vetoes"
+      print("warning: " + vetoes + " left blank; proceeding "\
+        "without DQ vetoes", file=sys.stderr)
       outfile += vetoes + "_BLANK.txt"
       rel_outfile += vetoes + "_BLANK.txt"
       open(outfile, "w").write("")  # touch
@@ -455,8 +455,8 @@
   vetoDefFile = config.get("segments", "veto-def-file")
 
   if generate_segments:
-    print "Downloading veto-definer file " + vetoDefFile + " from " \
-	+ vetoDefurl
+    print("Downloading veto-definer file " + vetoDefFile + " from " \
+	+ vetoDefurl)
     vetoDefFile, info = urllib.urlretrieve(vetoDefurl + '/' + vetoDefFile,
 	vetoDefFile)
 
@@ -479,15 +479,15 @@
     dq_url = config.get("segments","dq-server-url")
     dq_segdb_file = config.get("segments", ifo.lower() + '-dq-file')
     if dq_segdb_file == "":
-      print >>sys.stderr, "warning: no file provided to %s-dq-file; " \
-        "running without data quality" % ifo.lower()
+      print("warning: no file provided to %s-dq-file; " \
+        "running without data quality" % ifo.lower(), file=sys.stderr)
     else:
-      print "Downloading DQ segment file " + dq_segdb_file + " from " \
-            + dq_url + " to " + dqSegFile + " ...",
+      print("Downloading DQ segment file " + dq_segdb_file + " from " \
+            + dq_url + " to " + dqSegFile + " ...", end=' ')
       sys.stdout.flush()
       dqSegFile, info = urllib.urlretrieve(dq_url + '/' + dq_segdb_file,
             dqSegFile)
-      print "done"
+      print("done")
   return dqSegFile
 
 
@@ -521,18 +521,18 @@
       str(end - start) + ".xml"
 
   if generate_segments:
-    print "Generating science segments for " + ifo + " ...",
+    print("Generating science segments for " + ifo + " ...", end=' ')
     sys.stdout.flush()
   sciSegFile = science_segments(ifo, config, generate_segments)
 
   # generate vetoFiles
   if generate_segments: 
     sciSegs = segmentsUtils.fromsegwizard(file(sciSegFile)).coalesce()
-    print " done."
-    print "Generating cat 1 veto segments for " + ifo + " ...",
+    print(" done.")
+    print("Generating cat 1 veto segments for " + ifo + " ...", end=' ')
     sys.stdout.flush()
     vetoFiles = veto_segments(ifo, config, [1], generate_segments)
-    print "done"
+    print("done")
 
     # remove cat 1 veto times
     if os.path.exists(vetoFiles[1]):
@@ -545,13 +545,13 @@
       analyzedSegs = sciSegs.__and__(dfSegs)
       missedSegs = sciSegs.__and__(dfSegs.__invert__())
       segmentsUtils.tosegwizard(file(missedFile,"w"), missedSegs)
-      print "Writing " + ifo + " segments which cannot be analyzed to file " \
-          + missedFile
-      print "Writing " + ifo + " segments which cannot be analyzed to file " \
-          + missedFileXML
-      print "Not analyzing %d s, representing %.2f percent of time" %  \
+      print("Writing " + ifo + " segments which cannot be analyzed to file " \
+          + missedFile)
+      print("Writing " + ifo + " segments which cannot be analyzed to file " \
+          + missedFileXML)
+      print("Not analyzing %d s, representing %.2f percent of time" %  \
          (missedSegs.__abs__(),
-         100. * missedSegs.__abs__() / max(analyzedSegs.__abs__(), 0.1) )
+         100. * missedSegs.__abs__() / max(analyzedSegs.__abs__(), 0.1) ))
 
     else: analyzedSegs = sciSegs
 
@@ -584,17 +584,17 @@
       utils.write_fileobj(xmldoc, fp, gz=False)
     fp.close()
 
-    print "Writing " + ifo + " segments of total time " + \
-        str(analyzedSegs.__abs__()) + "s to file: " + segFile
-    print "Writing " + ifo + " segments of total time " + \
-        str(analyzedSegs.__abs__()) + "s to file: " + segFileXML
-    print "done"
+    print("Writing " + ifo + " segments of total time " + \
+        str(analyzedSegs.__abs__()) + "s to file: " + segFile)
+    print("Writing " + ifo + " segments of total time " + \
+        str(analyzedSegs.__abs__()) + "s to file: " + segFileXML)
+    print("done")
 
   if data_quality_vetoes: 
-    print "Generating cat " + str(veto_categories) + " veto segments for " + ifo + "..."
+    print("Generating cat " + str(veto_categories) + " veto segments for " + ifo + "...")
     sys.stdout.flush()
   dqVetoes = veto_segments(ifo, config, veto_categories, data_quality_vetoes )
-  if data_quality_vetoes: print "done"
+  if data_quality_vetoes: print("done")
 
   return tuple([segFile, dqVetoes])
 
@@ -623,8 +623,8 @@
         for ifo in ifos])
     if (maxLength/2/maxSlide - 1) < int(numSlides):
       numSlides = str(maxLength/2/maxSlide - 1)
-      print "Setting number of slides to " + numSlides + \
-          " to avoid double wrapping"
+      print("Setting number of slides to " + numSlides + \
+          " to avoid double wrapping")
 
   return numSlides
 
@@ -758,8 +758,8 @@
     # fail if the seed is not identical to its integer form
     # to prevent later problems with inspinj
     if not ( str(int(injSeed)) == injSeed ):
-      print >>sys.stderr, "Injection seed: " + injSeed + "\n"
-      print >>sys.stderr, "Error: the injection seed must be an integer without leading zeros! Exiting..."
+      print("Injection seed: " + injSeed + "\n", file=sys.stderr)
+      print("Error: the injection seed must be an integer without leading zeros! Exiting...", file=sys.stderr)
       sys.exit(1)
 
     # copy over the arguments from the relevant injection section
@@ -796,17 +796,17 @@
 
   hipecp.write(file(iniFile,"w"))
 
-  print "Running hipe in directory " + hipeDir
+  print("Running hipe in directory " + hipeDir)
   if dataFind or tmpltBank:
-    print "Running datafind / template bank generation"
+    print("Running datafind / template bank generation")
   elif injSeed:
-    print "Injection seed: " + injSeed
+    print("Injection seed: " + injSeed)
   else:
-    print "No injections, " + str(hipecp.get("input","num-slides")) + \
-          " time slides"
+    print("No injections, " + str(hipecp.get("input","num-slides")) + \
+          " time slides")
   if vetoCat:
-    print "Running the category " + str(vetoCat) + " vetoes"
-  print
+    print("Running the category " + str(vetoCat) + " vetoes")
+  print()
 
   # work out the hipe call:
   hipeCommand = config.get("condor","hipe")
@@ -1070,12 +1070,12 @@
 
   plotcp.write(file(iniFile,"w"))
 
-  print "Running plot hipe in directory " + plotDir
-  print "Using zero lag sieve: " + zerolagSuffix 
-  print "Using time slide sieve: " + slideSuffix  
-  print "Using injection sieve: " + injectionSuffix 
-  print "Using bank sieve: " + bankSuffix 
-  print
+  print("Running plot hipe in directory " + plotDir)
+  print("Using zero lag sieve: " + zerolagSuffix) 
+  print("Using time slide sieve: " + slideSuffix)  
+  print("Using injection sieve: " + injectionSuffix) 
+  print("Using bank sieve: " + bankSuffix) 
+  print()
 
   # work out the hipe call:
   plotCommand = config.get("condor","plot")
@@ -1290,8 +1290,8 @@
     cacheFile = hipe_cache( ifos, usertag, cp.getint("input", "gps-start-time"), cp.getint("input", "gps-end-time") )
     
     if not os.path.isfile(os.path.join("full_data", cacheFile)):
-      print>>sys.stderr, "WARNING: Cache file FULL_DATA/" + cacheFile
-      print>>sys.stderr, "does not exist! This might cause later failures."
+      print("WARNING: Cache file FULL_DATA/" + cacheFile, file=sys.stderr)
+      print("does not exist! This might cause later failures.", file=sys.stderr)
 
     outfilename = os.path.join(hw_inj_dir, ''.join(ifos) + '-HWINJ_SUMMARY')
     if veto:
@@ -1370,7 +1370,7 @@
   # link datafind output from original hipe
   try: os.symlink("../datafind/cache", "hipe_cache")
   except: pass
-  print "Running followup pipe in directory " + followupDir
+  print("Running followup pipe in directory " + followupDir)
 
   # work out the followup_pipe call:
   followupCommand = config.get("condor","follow")
@@ -1461,7 +1461,7 @@
 def omega_scan_setup(cp,ifos):
   cp.set('omega-scans','do-omega-scan','')
   cp.set('omega-scans','omega-executable',cp.get('condor','omegascan'))
-  print "Beginning set up of omega scan directory."
+  print("Beginning set up of omega scan directory.")
   start = cp.get("input","gps-start-time")
   end = cp.get("input","gps-end-time")
   # First we set up the configuration files
@@ -1521,7 +1521,7 @@
       channel_name = 'virgo-channel'
       type_name = 'virgo-type'
     else:
-      print "IFO " + ifo + " is not yet supported for omega scans in ihope"
+      print("IFO " + ifo + " is not yet supported for omega scans in ihope")
       continue
     if cp.has_option('omega-setup',channel_name):
       channels = (cp.get('omega-setup',channel_name)).split(',')
@@ -1547,7 +1547,7 @@
     outFile.close()
     cp.set('omega-scans',ifo.lower() + '-omega-config-file','../omega_setup/'+ifo + '_omega_config.txt')
 
-  print "Created omega scan configuration files"
+  print("Created omega scan configuration files")
 
   # And we need to create the necessary frame caches
   if not os.path.isdir('cache'):
@@ -1577,7 +1577,7 @@
     make_external_call(convertCall)
     cp.set('omega-scans',ifo.lower() + '-omega-frame-file','../omega_setup/' + 'cache/' + ifo + '_' + start + '_' + end + '_frames.wcache')
 
-  print "Created omega scan frame files \n"
+  print("Created omega scan frame files \n")
   
 
 ###############################################################################
--- ./src/inspiral/lalapps_bank_plotter.py	(original)
+++ ./src/inspiral/lalapps_bank_plotter.py	(refactored)
@@ -72,8 +72,8 @@
 
     MAX_HEIGHT_INCHES = 8.0
     if fig_height > MAX_HEIGHT_INCHES:
-        print("WARNING: fig_height too large:" + fig_height + 
-              "so will reduce to" + MAX_HEIGHT_INCHES + "inches.")
+        print(("WARNING: fig_height too large:" + fig_height + 
+              "so will reduce to" + MAX_HEIGHT_INCHES + "inches."))
         fig_height = MAX_HEIGHT_INCHES
 
     params = {'backend': 'ps',
--- ./src/inspiral/lalapps_cbc_pipedown.py	(original)
+++ ./src/inspiral/lalapps_cbc_pipedown.py	(refactored)
@@ -113,11 +113,11 @@
 ##############################################################################
 # Sanity check of input arguments
 if not options.ihope_cache:
-  raise ValueError, "An ihope-cache file is required."
+  raise ValueError("An ihope-cache file is required.")
 if not options.config_file:
-  raise ValueError, "A config-file is required."
+  raise ValueError("A config-file is required.")
 if not options.log_path:
-  raise ValueError, "A log-path is required."
+  raise ValueError("A log-path is required.")
 
 ##############################################################################
 # Create log file
@@ -165,7 +165,7 @@
 ##############################################################################
 # Open the ihope cache and sort THINCA_SECOND files by user_tag
 
-print "Parsing the ihope cache..."
+print("Parsing the ihope cache...")
 coinc_zero_lag_tag = cp.get('pipeline', 'coinc-file-tag')
 coinc_slide_tag = cp.get('pipeline', 'coinc-slide-file-tag')
 search_file_tag = cp.get('pipeline', 'search-file-tag')
@@ -203,7 +203,7 @@
 # Get the time column name (for inspiral, it is end_time; for ringdown, it is 
 # start_time)
 
-print "Getting the name of the time column..."
+print("Getting the name of the time column...")
 time_column = cp.get('pipeline', 'time-column')
 
 ##############################################################################
@@ -388,7 +388,7 @@
 
 for tag in user_tags:
 
-  print "Creating jobs for %s..." % tag
+  print("Creating jobs for %s..." % tag)
   
   # determine whether or not this was an injection run by checking if there
   # is an injection file for this tag
@@ -402,13 +402,13 @@
     inj_file = inj_file[0].url
     sim_tags.append(tag.split('_CAT_')[0])
   else:
-    raise ValueError, "More than one injection file found for %s" % tag
+    raise ValueError("More than one injection file found for %s" % tag)
 
 
   ############################################################################
   # Setup thinca_to_coinc nodes
 
-  print "\tsetting up thinca_to_coinc nodes..."
+  print("\tsetting up thinca_to_coinc nodes...")
 
   # set job options
 
@@ -420,7 +420,7 @@
   if not simulation and \
     len(slide_cache.sieve( description = file_sieve, exact_match = True )) != \
     len(thinca_cache):
-    raise ValueError, "Number of %s slide files doesn't equal number of zero-lag files." % tag
+    raise ValueError("Number of %s slide files doesn't equal number of zero-lag files." % tag)
 
   # also sieve all_inspirals_cache
   file_sieve = tag.split('_CAT_')[0]
@@ -442,7 +442,7 @@
     '.xml' ])
   veto_file = '/'.join([ veto_file_path, veto_file_name ])
   if not os.path.exists( veto_file ):
-    raise ValueError, "Veto file %s could not be found." % veto_file
+    raise ValueError("Veto file %s could not be found." % veto_file)
   # store the veto file for additional later use
   veto_cat = '_'.join(['CAT', cat_num, 'VETO'])
   veto_files[veto_cat] = veto_file
@@ -512,7 +512,7 @@
   ############################################################################
   # Setup a LigolwSqliteNode for putting thinca_to_coincs into a sql db
 
-  print "\tsetting up node to put thinca_to_coinc files into a SQLite database..."
+  print("\tsetting up node to put thinca_to_coinc files into a SQLite database...")
   
   # set node options
   t2c2sql_node = pipeline.LigolwSqliteNode( sql_replace_job )
@@ -527,8 +527,8 @@
     'sqlite' ])
   # check to make sure the database doesn't already exist
   if os.path.exists( raw_result_db ):
-    print "WARNING: Raw result database %s already exists; " % raw_result_db + \
-    "if it isn't moved, it will be overwritten when DAG is submitted."
+    print("WARNING: Raw result database %s already exists; " % raw_result_db + \
+    "if it isn't moved, it will be overwritten when DAG is submitted.")
     
   t2c2sql_node.set_database( raw_result_db )
   
@@ -540,7 +540,7 @@
   ############################################################################
   # Setup a DBSimplifyNode to clean up the output of the t2c2sql_node 
   
-  print "\tsetting up dbsimplify node to clean the database..."
+  print("\tsetting up dbsimplify node to clean the database...")
   
   # set node options
   dbsimplify_node = inspiral.DBSimplifyNode( dbsimplify_job )
@@ -556,7 +556,7 @@
   ############################################################################
   # Make the detection statistic nicer and Kipp and Duncan happier
  
-  print "\tsetting up repop_coinc node to recalculate detection statistic..."
+  print("\tsetting up repop_coinc node to recalculate detection statistic...")
 
   # set node options
   last_node = dbsimplify_node
@@ -577,7 +577,7 @@
   #############################################################################
   # Setup a ClusterCoincsNode to cluster the output of dbsimplify_node
   
-  print "\tsetting up cluster nodes to cluster coincs in the database..."
+  print("\tsetting up cluster nodes to cluster coincs in the database...")
   
   # set node options
   # for the first clustering job, we want the input to be the raw_result_db,
@@ -614,7 +614,7 @@
 
   if simulation:
     # add dbaddinj node
-    print "\tsetting up dbaddinj node to add the injection file..."
+    print("\tsetting up dbaddinj node to add the injection file...")
   
     # set node options
     dbaddinj_node = inspiral.DBAddInjNode( dbaddinj_job )
@@ -629,7 +629,7 @@
     dag.add_node( dbaddinj_node )
 
     # add sqlite extract node
-    print "\tsetting up ligolw_sqlite node to extract the injection database to an xml..."
+    print("\tsetting up ligolw_sqlite node to extract the injection database to an xml...")
     
     # set node options
     simxml_node = pipeline.LigolwSqliteNode( sql_extract_job )
@@ -656,7 +656,7 @@
 # done cycling over tags: Create injfind job and node
 
 # cache the sim xmls by veto category
-print "Creating injfind nodes..."
+print("Creating injfind nodes...")
 
 injfind_nodes = {}
 sim_caches = {}
@@ -702,7 +702,7 @@
 for result_db in result_dbs_cache:
   
   # get tag and veto_cat
-  print "Creating jobs for %s database..." % result_db.description
+  print("Creating jobs for %s database..." % result_db.description)
   tag = result_db.description.replace('_CLUSTERED_CBC_RESULTS', '')
   cat_num = get_veto_cat_from_tag( tag )
   veto_cat = '_'.join([ 'CAT', str(cat_num), 'VETO' ])
@@ -737,7 +737,7 @@
     dbsimplify2_node.add_parent( sim2fulldb_node )
     dag.add_node( dbsimplify2_node )
 
-    print "\tsetting up dbinjfind node to add exact/nearby definitions..."
+    print("\tsetting up dbinjfind node to add exact/nearby definitions...")
 
     # set dbinjfind node options
     last_node = dbsimplify2_node
@@ -777,7 +777,7 @@
   ############################################################################
   # Compute durations in the database
 
-  print "\tsetting up compute_durations node..."
+  print("\tsetting up compute_durations node...")
 
   # set node options
   comp_durs_node = inspiral.ComputeDurationsNode( comp_durs_job)
@@ -794,7 +794,7 @@
   # MVSC Calculation
   if options.run_mvsc:
     if 'FULL_DATA' in tag and veto_cat in sim_caches:
-      print "\tsetting up MVSC dag..."
+      print("\tsetting up MVSC dag...")
       mvsc_dag_name = options.config_file.replace('.ini','')+'_mvsc_'+tag+'_n'+cp.get("mvsc_dag","number-of-trees")+'_l'+cp.get("mvsc_dag","leaf-size")+'_s'+cp.get("mvsc_dag","sampled-parameters")+'_c'+cp.get("mvsc_dag","criterion-for-optimization")+'.dag'
       mvsc_dag_generator_job = inspiral.MVSCDagGenerationJob(cp)
       for key,val in cp.items("mvsc_dag"):
@@ -813,8 +813,8 @@
   ############################################################################
   # Compute the uncombined false alarm rates
   
-  print "\tsetting up cfar nodes:"
-  print "\t\tfor uncombined false alarm rates..."
+  print("\tsetting up cfar nodes:")
+  print("\t\tfor uncombined false alarm rates...")
   
   # set node options: output database is same as input
   ucfar_node = inspiral.CFarNode( ucfar_job )
@@ -834,7 +834,7 @@
   ############################################################################
   # Compute the combined false alarm rates
   
-  print "\t\tfor combined false alarm rates..."
+  print("\t\tfor combined false alarm rates...")
   
   # set node options: output database is same as input
   ccfar_node = inspiral.CFarNode( ccfar_job )
@@ -894,7 +894,7 @@
   # Summary: Setup PrintLC and MiniFollowup Nodes to generate a summary of 
   # loudest non-simulation events
 
-  print "\tsetting up printlc and minifollowup nodes..."
+  print("\tsetting up printlc and minifollowup nodes...")
 
   # set datatypes to generate files for
   if 'PLAYGROUND' in tag:
@@ -904,7 +904,7 @@
 
   lc_nodes = []
   for datatype in datatypes:
-    print "\t\tfor %s..." % datatype
+    print("\t\tfor %s..." % datatype)
     # set file naming type
     type_prefix = tag
     type = '_'.join([ type_prefix, 'LOUDEST', datatype.upper(), 'EVENTS_BY', cp.get('printlc', 'ranking-stat').upper()])
@@ -963,7 +963,7 @@
   # Injection summary: Setup printsims, minifollowup, and printmissed nodes
 
   if 'PLAYGROUND' not in tag:
-    print "\tsetting up injection summary nodes..."
+    print("\tsetting up injection summary nodes...")
 
     # cycle over all the different types of injections
     for sim_tag in sim_tags:
@@ -1141,10 +1141,10 @@
   ############################################################################
   # Plotting: Generate all result plots
   
-  print "\tsetting up plotting jobs..."
+  print("\tsetting up plotting jobs...")
 
   # Write plotslides node
-  print "\t\tcreating plotslides node..."
+  print("\t\tcreating plotslides node...")
   if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
     plotslides_node = inspiral.PlotSlidesNode( plotslides_play_job )
   else:
@@ -1159,7 +1159,7 @@
   dag.add_node( plotslides_node )
 
   # create plotcumhist node
-  print "\t\tcreating plotcumhist node..."
+  print("\t\tcreating plotcumhist node...")
   if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
     plotcumhist_node = inspiral.PlotCumhistNode( plotcumhist_play_job )
   else:
@@ -1174,12 +1174,12 @@
   dag.add_node( plotcumhist_node )
 
   # Write plotifar nodes for different datatypes
-  print "\t\tcreating plotifar node for datatypes:"
+  print("\t\tcreating plotifar node for datatypes:")
   for datatype in ['all_data', 'playground', 'exclude_play']: 
     # only create nodes for non-playground if options.plot-playground-only not set
     if (not options.generate_all_data_plots or 'PLAYGROUND' in tag)  and datatype != 'playground':
       continue
-    print "\t\t\t%s..." % datatype
+    print("\t\t\t%s..." % datatype)
     plotifar_node = inspiral.PlotIfarNode( plotifar_job )
     plotifar_node.set_category('plotifar')
     plotifar_node.set_tmp_space( tmp_space )
@@ -1229,7 +1229,7 @@
 ##############################################################################
 # Final Step: Write the DAG
 
-print "Writing DAG and sub files..."
+print("Writing DAG and sub files...")
 
 # set max-jobs: currently, only minifollowups is set
 dag.add_maxjobs_category('minifollowups', 15)
@@ -1242,8 +1242,8 @@
 process.set_process_end_time(proc_id)
 utils.write_filename(logdoc, basename+'.log.xml', xsl_file = "ligolw.xsl")
 
-print "Finished!"
-print "Now run:\n\tcondor_submit_dag %s" % os.path.basename(dag.get_dag_file())
+print("Finished!")
+print("Now run:\n\tcondor_submit_dag %s" % os.path.basename(dag.get_dag_file()))
 
 sys.exit(0)
 
--- ./src/inspiral/lalapps_cbc_pipedown_ssipe.py	(original)
+++ ./src/inspiral/lalapps_cbc_pipedown_ssipe.py	(refactored)
@@ -111,11 +111,11 @@
 ##############################################################################
 # Sanity check of input arguments
 if not options.ihope_cache:
-  raise ValueError, "An ihope-cache file is required."
+  raise ValueError("An ihope-cache file is required.")
 if not options.config_file:
-  raise ValueError, "A config-file is required."
+  raise ValueError("A config-file is required.")
 if not options.log_path:
-  raise ValueError, "A log-path is required."
+  raise ValueError("A log-path is required.")
 
 ##############################################################################
 # Create log file
@@ -163,7 +163,7 @@
 ##############################################################################
 # Open the ihope cache and create THINCA cache
 
-print "Parsing the ihope cache..."
+print("Parsing the ihope cache...")
 
 coinc_tag = cp.get('pipeline', 'coinc-file-tag')
 ihope_cache = [line for line in file(options.ihope_cache) \
@@ -197,7 +197,7 @@
 # Get the time column name (for inspiral, it is end_time; for ringdown, it is 
 # start_time)
 
-print "Getting the name of the time column..."
+print("Getting the name of the time column...")
 time_column = cp.get('pipeline', 'time-column')
 
 ##############################################################################
@@ -361,7 +361,7 @@
 
 for tag in user_tags:
 
-  print "Creating jobs for %s..." % tag
+  print("Creating jobs for %s..." % tag)
   
   # determine whether or not this was an injection run by checking if there
   # is an injection file for this tag
@@ -376,12 +376,12 @@
     inj_file = inj_file[0].url
     sim_tags.append(tag.split('_CAT_')[0])
   else:
-    raise ValueError, "More than one injection file found for %s" % tag
+    raise ValueError("More than one injection file found for %s" % tag)
 
   ############################################################################
   # Creating the thinca_user_tag cache file and writing it to disk
 
-  print "\tcreating the thinca_usertag cache file..."
+  print("\tcreating the thinca_usertag cache file...")
 
   # sieve thinca_cache for THINCA files with this tag
   file_sieve = '*' + tag
@@ -403,7 +403,7 @@
     '.xml' ])
   veto_file = '/'.join([ veto_file_path, veto_file_name ])
   if not os.path.exists( veto_file ):
-    raise ValueError, "Veto file %s could not be found." % veto_file
+    raise ValueError("Veto file %s could not be found." % veto_file)
   # store the veto file for additional later use
   veto_cat = '_'.join(['CAT', cat_num, 'VETO'])
   veto_files[veto_cat] = veto_file
@@ -421,7 +421,7 @@
   ############################################################################
   # Setup a LigolwSqliteNode for putting thinca into a sql db
 
-  print "\tsetting up node to put thinca files into a SQLite database..."
+  print("\tsetting up node to put thinca files into a SQLite database...")
   
   # set node options
   t2sql_node = pipeline.LigolwSqliteNode( sql_replace_job )
@@ -436,8 +436,8 @@
     'sqlite' ])
   # check to make sure the database doesn't already exist
   if os.path.exists( raw_result_db ):
-    print "WARNING: Raw result database %s already exists; " % raw_result_db + \
-    "if it isn't moved, it will be overwritten when DAG is submitted."
+    print("WARNING: Raw result database %s already exists; " % raw_result_db + \
+    "if it isn't moved, it will be overwritten when DAG is submitted.")
     
   t2sql_node.set_database( raw_result_db )
   
@@ -446,7 +446,7 @@
   ############################################################################
   # Setup a DBSimplifyNode to clean up the output of the t2sql_node 
   
-  print "\tsetting up dbsimplify node to clean the database..."
+  print("\tsetting up dbsimplify node to clean the database...")
   
   # set node options
   dbsimplify_node = inspiral.DBSimplifyNode( dbsimplify_job )
@@ -462,7 +462,7 @@
   ############################################################################
   # Make the detection statistic nicer and Kipp and Duncan happier
  
-  print "\tsetting up repop_coinc node to recalculate detection statistic..."
+  print("\tsetting up repop_coinc node to recalculate detection statistic...")
 
   # set node options
   last_node = dbsimplify_node
@@ -483,7 +483,7 @@
   #############################################################################
   # Setup a ClusterCoincsNode to cluster the output of dbsimplify_node
   
-  print "\tsetting up cluster node to cluster coincs in the database..."
+  print("\tsetting up cluster node to cluster coincs in the database...")
   
   # set node options
   # for the first clustering job, we want the input to be the raw_result_db,
@@ -520,7 +520,7 @@
 
   if simulation:
     # add dbaddinj node
-    print "\tsetting up dbaddinj node to add the injection file..."
+    print("\tsetting up dbaddinj node to add the injection file...")
 
     # set node options
     dbaddinj_node = inspiral.DBAddInjNode( dbaddinj_job )
@@ -534,7 +534,7 @@
     dag.add_node( dbaddinj_node )
 
     # add sqlite extract node
-    print "\tsetting up ligolw_sqlite node to extract the injection database to an xml..."
+    print("\tsetting up ligolw_sqlite node to extract the injection database to an xml...")
     
     # set node options
     simxml_node = pipeline.LigolwSqliteNode( sql_extract_job )
@@ -561,7 +561,7 @@
 # done cycling over tags: Create injfind job and node
 
 # cache the sim xmls by veto category
-print "Creating injfind nodes..."
+print("Creating injfind nodes...")
 
 injfind_nodes = {}
 sim_caches = {}
@@ -607,7 +607,7 @@
 for result_db in result_dbs_cache:
   
   # get tag and veto_cat
-  print "Creating jobs for %s database..." % result_db.description
+  print("Creating jobs for %s database..." % result_db.description)
   tag = result_db.description.replace('_CLUSTERED_CBC_RESULTS', '')
   cat_num = get_veto_cat_from_tag( tag )
   veto_cat = '_'.join([ 'CAT', str(cat_num), 'VETO' ])
@@ -642,7 +642,7 @@
     dbsimplify2_node.add_parent( sim2fulldb_node )
     dag.add_node( dbsimplify2_node )
 
-    print "\tsetting up dbinjfind node to add exact/nearby definitions..."
+    print("\tsetting up dbinjfind node to add exact/nearby definitions...")
 
     # set dbinjfind node options
     last_node = dbsimplify2_node
@@ -682,7 +682,7 @@
   ############################################################################
   # Compute durations in the database
 
-  print "\tsetting up compute_durations node..."
+  print("\tsetting up compute_durations node...")
 
   # set node options
   comp_durs_node = inspiral.ComputeDurationsNode( comp_durs_job)
@@ -699,7 +699,7 @@
   # MVSC Calculation
 
   if 'FULL_DATA' in tag and veto_cat in sim_caches:
-    print "\tsetting up MVSC dag..."
+    print("\tsetting up MVSC dag...")
     mvsc_dag_name = options.config_file.replace('.ini','')+'_mvsc_'+tag+'_n'+cp.get("mvsc_dag","number-of-trees")+'_l'+cp.get("mvsc_dag","leaf-size")+'_s'+cp.get("mvsc_dag","sampled-parameters")+'_c'+cp.get("mvsc_dag","criterion-for-optimization")+'.dag'
     mvsc_dag_generator_job = inspiral.MVSCDagGenerationJob(cp)
     for key,val in cp.items("mvsc_dag"):
@@ -718,8 +718,8 @@
   ############################################################################
   # Compute the uncombined false alarm rates
   
-  print "\tsetting up cfar nodes:"
-  print "\t\tfor uncombined false alarm rates..."
+  print("\tsetting up cfar nodes:")
+  print("\t\tfor uncombined false alarm rates...")
   
   # set node options: output database is same as input
   ucfar_node = inspiral.CFarNode( ucfar_job )
@@ -738,7 +738,7 @@
   ############################################################################
   # Compute the combined false alarm rates
   
-  print "\t\tfor combined false alarm rates..."
+  print("\t\tfor combined false alarm rates...")
   
   # set node options: output database is same as input
   ccfar_node = inspiral.CFarNode( ccfar_job )
@@ -798,7 +798,7 @@
   # Summary: Setup PrintLC and MiniFollowup Nodes to generate a summary of 
   # loudest non-simulation events
 
-  print "\tsetting up printlc and minifollowup nodes..."
+  print("\tsetting up printlc and minifollowup nodes...")
 
   # set datatypes to generate files for
   if 'PLAYGROUND' in tag:
@@ -809,7 +809,7 @@
   lc_nodes = []
 
   for datatype in datatypes:
-    print "\t\tfor %s..." % datatype
+    print("\t\tfor %s..." % datatype)
 
     # set file naming type
     type_prefix = tag
@@ -871,7 +871,7 @@
   # Injection summary: Setup printsims, minifollowup, and printmissed nodes
 
   if 'PLAYGROUND' not in tag:
-    print "\tsetting up injection summary nodes..."
+    print("\tsetting up injection summary nodes...")
 
     # cycle over all the different types of injections
     for sim_tag in sim_tags:
@@ -1049,10 +1049,10 @@
   ############################################################################
   # Plotting: Generate all result plots
   
-  print "\tsetting up plotting jobs..."
+  print("\tsetting up plotting jobs...")
 
   # Write plotslides node
-  print "\t\tcreating plotslides node..."
+  print("\t\tcreating plotslides node...")
   if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
     plotslides_node = inspiral.PlotSlidesNode( plotslides_play_job )
   else:
@@ -1067,7 +1067,7 @@
   dag.add_node( plotslides_node )
 
   # create plotcumhist node
-  print "\t\tcreating plotcumhist node..."
+  print("\t\tcreating plotcumhist node...")
   if not options.generate_all_data_plots or 'PLAYGROUND' in tag:
     plotcumhist_node = inspiral.PlotCumhistNode( plotcumhist_play_job )
   else:
@@ -1082,12 +1082,12 @@
   dag.add_node( plotcumhist_node )
 
   # Write plotifar nodes for different datatypes
-  print "\t\tcreating plotifar node for datatypes:"
+  print("\t\tcreating plotifar node for datatypes:")
   for datatype in ['all_data', 'playground', 'exclude_play']: 
     # only create nodes for non-playground if options.plot-playground-only not set
     if (not options.generate_all_data_plots or 'PLAYGROUND' in tag)  and datatype != 'playground':
       continue
-    print "\t\t\t%s..." % datatype
+    print("\t\t\t%s..." % datatype)
     plotifar_node = inspiral.PlotIfarNode( plotifar_job )
     plotifar_node.set_category('plotifar')
     plotifar_node.set_tmp_space( tmp_space )
@@ -1136,7 +1136,7 @@
 ##############################################################################
 # Final Step: Write the DAG
 
-print "Writing DAG and sub files..."
+print("Writing DAG and sub files...")
 
 # set max-jobs: currently, only minifollowups is set
 dag.add_maxjobs_category('minifollowups', 15)
@@ -1149,8 +1149,8 @@
 process.set_process_end_time(proc_id)
 utils.write_filename(logdoc, basename+'.log.xml', xsl_file = "ligolw.xsl")
 
-print "Finished!"
-print "Now run:\n\tcondor_submit_dag %s" % os.path.basename(dag.get_dag_file())
+print("Finished!")
+print("Now run:\n\tcondor_submit_dag %s" % os.path.basename(dag.get_dag_file()))
 
 sys.exit(0)
 
--- ./src/inspiral/lalapps_cbc_sbank.py	(original)
+++ ./src/inspiral/lalapps_cbc_sbank.py	(refactored)
@@ -126,7 +126,7 @@
 #
 def checkpoint_save(xmldoc, fout, process):
 
-    print >>sys.stderr, "\t[Checkpointing ...]"
+    print("\t[Checkpointing ...]", file=sys.stderr)
 
     # save rng state
     rng_state = np.random.get_state()
@@ -355,12 +355,12 @@
     f_max_orig = max(f_orig)
     if opts.fhigh_max:
         if opts.fhigh_max > f_max_orig:
-            print >> sys.stderr, "Warning: requested fhigh-max (%.3f Hz) exceeds limits of PSD (%.3f Hz). Using PSD limit instead!" \
-                    % (opts.fhigh_max, f_max_orig)
+            print("Warning: requested fhigh-max (%.3f Hz) exceeds limits of PSD (%.3f Hz). Using PSD limit instead!" \
+                    % (opts.fhigh_max, f_max_orig), file=sys.stderr)
             opts.fhigh_max = float(f_max_orig)
     else:
-        print >> sys.stderr, "Warning: fhigh-max not specified, using maximum frequency in the PSD (%.3f Hz)" \
-                % f_max_orig
+        print("Warning: fhigh-max not specified, using maximum frequency in the PSD (%.3f Hz)" \
+                % f_max_orig, file=sys.stderr)
         opts.fhigh_max = float(f_max_orig)
 
     interpolator = UnivariateSpline(f_orig, np.log(psddata), s=0)
@@ -395,7 +395,7 @@
         bank.add_from_sngls(sngl_inspiral, seed_waveform)
 
         if opts.verbose:
-            print>>sys.stdout,"Added %d %s seed templates from %s to initial bank." % (len(sngl_inspiral), approx, seed_file)
+            print("Added %d %s seed templates from %s to initial bank." % (len(sngl_inspiral), approx, seed_file), file=sys.stdout)
 
         tmpdoc.unlink()
         del sngl_inspiral, tmpdoc
@@ -406,7 +406,7 @@
         hdf_fp.close()
 
 if opts.verbose:
-    print>>sys.stdout,"Initialized the template bank to seed with %d precomputed templates." % len(bank)
+    print("Initialized the template bank to seed with %d precomputed templates." % len(bank), file=sys.stdout)
 
 
 #
@@ -420,8 +420,8 @@
     [bank.insort(t) for t in Bank.from_sngls(tbl, tmplt_class, noise_model, opts.flow, opts.use_metric, opts.cache_waveforms, opts.neighborhood_size, opts.neighborhood_param, coarse_match_df=opts.coarse_match_df, iterative_match_df_max=opts.iterative_match_df_max, fhigh_max=opts.fhigh_max)]
 
     if opts.verbose:
-        print >>sys.stdout,"Found checkpoint file %s with %d precomputed templates." % (opts.output_filename + "_checkpoint.gz", len(tbl))
-        print >>sys.stdout, "Resuming from checkpoint with %d total templates..." % len(bank)
+        print("Found checkpoint file %s with %d precomputed templates." % (opts.output_filename + "_checkpoint.gz", len(tbl)), file=sys.stdout)
+        print("Resuming from checkpoint with %d total templates..." % len(bank), file=sys.stdout)
 
     # reset rng state
     rng_state = np.load(opts.output_filename + "_checkpoint.rng.npz")
@@ -542,10 +542,10 @@
         bank.insort(tmplt)
         ks.append(k)
         if opts.verbose:
-            print "\nbank size: %d\t\tproposed: %d\trejection rate: %.6f / (%.6f)" % (len(bank), nprop, 1 - float(len(ks))/float(sum(ks)), 1 - 1./opts.convergence_threshold )
-            print >>sys.stdout, "accepted:\t\t", tmplt
+            print("\nbank size: %d\t\tproposed: %d\trejection rate: %.6f / (%.6f)" % (len(bank), nprop, 1 - float(len(ks))/float(sum(ks)), 1 - 1./opts.convergence_threshold ))
+            print("accepted:\t\t", tmplt, file=sys.stdout)
             if matcher is not None:
-                print >>sys.stdout, "max match (%.4f):\t" % match, matcher
+                print("max match (%.4f):\t" % match, matcher, file=sys.stdout)
         k = 0
 
         # Add to single inspiral table. Do not store templates that
@@ -582,9 +582,9 @@
 
 
 if opts.verbose:
-    print "\ntotal number of proposed templates: %d" % nprop
-    print "total number of match calculations: %d" % bank._nmatch
-    print "final bank size: %d" % len(bank)
+    print("\ntotal number of proposed templates: %d" % nprop)
+    print("total number of match calculations: %d" % bank._nmatch)
+    print("final bank size: %d" % len(bank))
 
 bank.clear()  # clear caches
 
--- ./src/inspiral/lalapps_cbc_sbank_choose_mchirp_boundaries.py	(original)
+++ ./src/inspiral/lalapps_cbc_sbank_choose_mchirp_boundaries.py	(refactored)
@@ -126,4 +126,4 @@
 
 # output
 with open(options.output_file, "w") as outfile:
-    print >>outfile, "\n".join(map(str, boundaries))
+    print("\n".join(map(str, boundaries)), file=outfile)
--- ./src/inspiral/lalapps_cbc_sbank_hdf5_choose_mchirp_boundaries.py	(original)
+++ ./src/inspiral/lalapps_cbc_sbank_hdf5_choose_mchirp_boundaries.py	(refactored)
@@ -121,4 +121,4 @@
 
 # output
 with open(options.output_file, "w") as outfile:
-    print >>outfile, "\n".join(map(str, boundaries))
+    print("\n".join(map(str, boundaries)), file=outfile)
--- ./src/inspiral/lalapps_cbc_sbank_pipe.py	(original)
+++ ./src/inspiral/lalapps_cbc_sbank_pipe.py	(refactored)
@@ -33,7 +33,7 @@
     which = subprocess.Popen(['/usr/bin/which', prog], stdout=subprocess.PIPE)
     out = which.stdout.read().strip()
     if not out:
-        print >>sys.stderr, "ERROR: could not find %s in your path, have you built the proper software and source the proper env. scripts?" % (prog,prog)
+        print("ERROR: could not find %s in your path, have you built the proper software and source the proper env. scripts?" % (prog,prog), file=sys.stderr)
         raise ValueError
     return out
 
--- ./src/inspiral/lalapps_cbc_sbank_sim.py	(original)
+++ ./src/inspiral/lalapps_cbc_sbank_sim.py	(refactored)
@@ -105,7 +105,7 @@
     opts, args = parser.parse_args()
 
     if opts.reference_psd and not opts.instrument:
-        raise ValueError, "--instrument is a required option when specifying reference PSD"
+        raise ValueError("--instrument is a required option when specifying reference PSD")
 
     return opts, args
 
@@ -165,13 +165,13 @@
     bank.add_from_sngls(sngl_inspiral, seed_waveform)
 
     if opts.verbose:
-        print "Added %d %s templates from %s to bank." % (len(sngl_inspiral), approx, seed_file)
+        print("Added %d %s templates from %s to bank." % (len(sngl_inspiral), approx, seed_file))
 
     tmpdoc.unlink()
     del sngl_inspiral, tmpdoc
 
 if opts.verbose:
-    print "Initialized the template bank with %d templates." % len(bank)
+    print("Initialized the template bank with %d templates." % len(bank))
 
 
 #
@@ -208,7 +208,7 @@
 sims = newtbl
 
 if verbose:
-    print "Loaded %d injections" % len(sims)
+    print("Loaded %d injections" % len(sims))
     inj_format = "".join("%s: %s   " % name_format for name_format in zip(inj_approx.param_names, inj_approx.param_formats))
 
 # main worker loop
@@ -221,22 +221,22 @@
     inj_wf = inj_approx.from_sim(sim, bank=inj_bank)
     inj_ind = opts.injection_min + j
     if verbose:
-        print "injection %d/%d" % (j+1, len(sims))
-        print inj_format % inj_wf.params
+        print("injection %d/%d" % (j+1, len(sims)))
+        print(inj_format % inj_wf.params)
 
     # NB: sigmasq set during argmax_match
     match_tup = bank.argmax_match(inj_wf)
 
     if verbose:
-        print "\tbest matching template:  ",
-        print bank._templates[match_tup[1]].params
-        print "\tbest match:  %f\n" % match_tup[0]
+        print("\tbest matching template:  ", end=' ')
+        print(bank._templates[match_tup[1]].params)
+        print("\tbest match:  %f\n" % match_tup[0])
 
     match_map[j] = (match_tup[0], inj_wf.sigmasq)
     tmplts.append(bank._templates[match_tup[1]].to_sngl())
 
 if verbose:
-    print "total number of match calculations:", bank._nmatch
+    print("total number of match calculations:", bank._nmatch)
 
 # write out results
 h5file = H5File("%s.h5" % usertag, "w")
--- ./src/inspiral/lalapps_check_flag.py	(original)
+++ ./src/inspiral/lalapps_check_flag.py	(refactored)
@@ -60,25 +60,25 @@
     options, others = parser.parse_args()
 
     if not options.ifo:
-        raise ValueError, "missing required argument --ifo"
+        raise ValueError("missing required argument --ifo")
 
     if not options.gps_end_time:
-        raise ValueError, "missing required argument --gps-end_time"
+        raise ValueError("missing required argument --gps-end_time")
    
     if not options.gps_start_time:
-        raise ValueError, "missing required argument --gps-start_time"
+        raise ValueError("missing required argument --gps-start_time")
    
     if not options.veto_category:
-        raise ValueError, "missing required argument --veto-category"
+        raise ValueError("missing required argument --veto-category")
    
     if len( [x for x in (options.unclustered, options.thirty_ms, options.sixteen_sec) if x] ) != 1:
-        raise ValueError, "must provide one of [--unclustered | --thirty-ms | --sixteen-sec]"
+        raise ValueError("must provide one of [--unclustered | --thirty-ms | --sixteen-sec]")
 
     if len( [x for x in (options.min_snr, options.min_new_snr) if x] ) != 1:
-        raise ValueError, "must provide exactly one of [--min-snr | --min-new-snr]"
+        raise ValueError("must provide exactly one of [--min-snr | --min-new-snr]")
 
     if len(others) == 0:
-        raise ValueError, "must provide at least one file of segments"
+        raise ValueError("must provide at least one file of segments")
 
     return options, others
 
@@ -158,7 +158,7 @@
     incount = len(trigs)
 
     if incount == 0:
-        print "No triggers found"
+        print("No triggers found")
         sys.exit(0)
 
     for filename in others:
@@ -174,10 +174,10 @@
         efficiency  = (float(incount) - float(outcount)) / float(incount) * 100.0
         deadtime    = float(abs(summ) - abs(new_summary))  / float(abs(summ)) * 100.0
 
-        print "File: ", filename
-        print "Efficiency: %.2f" % efficiency
-        print "Deadtime: %.2f" % deadtime
-        print "Ratio: %s" % (deadtime > 0 and "%.2f" % (efficiency / deadtime) or 'NA')
-        print "Loudest remaining trigger at %d with snr %.2f" % (new_trigs[0][0], new_trigs[0][1])
-        print
+        print("File: ", filename)
+        print("Efficiency: %.2f" % efficiency)
+        print("Deadtime: %.2f" % deadtime)
+        print("Ratio: %s" % (deadtime > 0 and "%.2f" % (efficiency / deadtime) or 'NA'))
+        print("Loudest remaining trigger at %d with snr %.2f" % (new_trigs[0][0], new_trigs[0][1]))
+        print()
 
--- ./src/inspiral/lalapps_check_hipe_times.py	(original)
+++ ./src/inspiral/lalapps_check_hipe_times.py	(refactored)
@@ -69,32 +69,32 @@
 if opts.injection_file:
   flist = [ opts.injection_file ]
   injections = readMeta.metaDataTable( flist, "sim_inspiral")
-  print "No triple injections: %d" % \
-      getSegments(injections, triplelist, "geocent_end_time").nevents()
-  print "No H1H2 injections: %d" % \
-      getSegments(injections, h1h2doublelist, "geocent_end_time").nevents()
-  print "No H1L1 injections: %d" % \
-      getSegments(injections, h1l1doublelist, "geocent_end_time").nevents()
-  print "No H2L1 injections: %d" % \
-      getSegments(injections, h2l1doublelist, "geocent_end_time").nevents()
+  print("No triple injections: %d" % \
+      getSegments(injections, triplelist, "geocent_end_time").nevents())
+  print("No H1H2 injections: %d" % \
+      getSegments(injections, h1h2doublelist, "geocent_end_time").nevents())
+  print("No H1L1 injections: %d" % \
+      getSegments(injections, h1l1doublelist, "geocent_end_time").nevents())
+  print("No H2L1 injections: %d" % \
+      getSegments(injections, h2l1doublelist, "geocent_end_time").nevents())
 
 if opts.glitch_time:
   if triplelist.__contains__(opts.glitch_time):
-    print "Time " + str(opts.glitch_time) + " is in triple time"
+    print("Time " + str(opts.glitch_time) + " is in triple time")
   if h1h2doublelist.__contains__(opts.glitch_time):
-    print "Time " + str(opts.glitch_time) + " is in h1h2 only time"
+    print("Time " + str(opts.glitch_time) + " is in h1h2 only time")
   if h1l1doublelist.__contains__(opts.glitch_time):
-    print "Time " + str(opts.glitch_time) + " is in h1l1 only time"
+    print("Time " + str(opts.glitch_time) + " is in h1l1 only time")
   if h2l1doublelist.__contains__(opts.glitch_time):
-    print "Time " + str(opts.glitch_time) + " is in h2l1 only time"
+    print("Time " + str(opts.glitch_time) + " is in h2l1 only time")
 else:
   tmptime=triplelist.duration()
-  print "Total triple time: %d s, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0))
+  print("Total triple time: %d s, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0)))
   tmptime=h1h2doublelist.duration()
-  print "Total H1H2-only time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0))
+  print("Total H1H2-only time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0)))
   tmptime=h1l1doublelist.duration()
-  print "Total H1L1-only time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0))
+  print("Total H1L1-only time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0)))
   tmptime=h2l1doublelist.duration()
-  print "Total H2L1-only time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0))
+  print("Total H2L1-only time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0)))
   tmptime=triplelist.duration()+ h1h2doublelist.duration() + h1l1doublelist.duration() + h2l1doublelist.duration()
-  print "Total time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0))
+  print("Total time: %d, %f yr" % (tmptime, tmptime/(365.25 * 24.0 * 3600.0)))
--- ./src/inspiral/lalapps_cohPTF_hipe.py	(original)
+++ ./src/inspiral/lalapps_cohPTF_hipe.py	(refactored)
@@ -41,7 +41,7 @@
   try:
     tc = open('tc.data','w')
   except:
-    print >> sys.stderr, "Cannot open transformation catalog for writing"
+    print("Cannot open transformation catalog for writing", file=sys.stderr)
     sys.exit(1)
 
   # write a line to the transformation catalog for each executable
@@ -474,50 +474,50 @@
 #################################
 # if --version flagged
 if opts.version:
-  print "$Id$"
+  print("$Id$")
   sys.exit(0)
 
 #################################
 # Sanity check of input arguments
 if not opts.config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.g1_data and not opts.h1_data and not opts.h2_data and \
     not opts.l1_data and not opts.v1_data and not opts.analyze_all:
-  print >> sys.stderr, "No ifos specified.  Please specify at least one of"
-  print >> sys.stderr, "--g1-data, --h1-data, --h2-data, --l1-data, --v1-data"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
+  print("No ifos specified.  Please specify at least one of", file=sys.stderr)
+  print("--g1-data, --h1-data, --h2-data, --l1-data, --v1-data", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos all data", file=sys.stderr)
   sys.exit(1)
 elif opts.analyze_all:
-  print >> sys.stderr, "The --analyze-all flag is currently not available."
-  print >> sys.stderr, "The code supports quadruple coincidence, so you can"
-  print >> sys.stderr, "choose at most four instruments to analyze."
+  print("The --analyze-all flag is currently not available.", file=sys.stderr)
+  print("The code supports quadruple coincidence, so you can", file=sys.stderr)
+  print("choose at most four instruments to analyze.", file=sys.stderr)
   sys.exit(1)
 
 if opts.g1_data and opts.h1_data and opts.h2_data and opts.l1_data \
     and opts.v1_data:
-  print >> sys.stderr, "Too many IFOs specified. " \
-      "Please choose up to four IFOs, but not five."
+  print("Too many IFOs specified. " \
+      "Please choose up to four IFOs, but not five.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.one_ifo and not opts.two_ifo and not opts.three_ifo and \
     not opts.four_ifo and not opts.analyze_all:
-  print >> sys.stderr, "No number of ifos given. Please specify at least one of"
-  print >> sys.stderr, "--one-ifo, --two-ifo, --three-ifo, --four-ifo"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
+  print("No number of ifos given. Please specify at least one of", file=sys.stderr)
+  print("--one-ifo, --two-ifo, --three-ifo, --four-ifo", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos all data", file=sys.stderr)
   sys.exit(1)
 elif opts.analyze_all:
-  print >> sys.stderr, "The --analyze-all flag can not be used to specify the"
-  print >> sys.stderr, "number of ifos to analyze. The code supports quadruple"
-  print >> sys.stderr, "coincidence, so you can choose at most four instruments"
-  print >> sys.stderr, "to analyze."
+  print("The --analyze-all flag can not be used to specify the", file=sys.stderr)
+  print("number of ifos to analyze. The code supports quadruple", file=sys.stderr)
+  print("coincidence, so you can choose at most four instruments", file=sys.stderr)
+  print("to analyze.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.datafind and not opts.template_bank and \
@@ -529,13 +529,13 @@
      not opts.coire_second_coinc and not opts.sire_second_coinc and \
      not opts.coherent_bank and not opts.coherent_inspiral and \
      not opts.summary_inspiral_triggers and not opts.summary_coinc_triggers: 
-  print >> sys.stderr, """  No steps of the pipeline specified.
+  print("""  No steps of the pipeline specified.
   Please specify at least one of
   --datafind, --template-bank, --inspiral, --sire-inspiral, --coincidence, 
   --coire-coincidence, --trigbank, --inspiral-veto, --sire-inspiral-veto,
   --td-follow-bank, --td-follow-inspiral, --second-coinc, 
   --coire-second-coinc, --sire-second-coinc, --coherent-bank, 
-  --coherent-inspiral, --summary-inspiral-triggers --summary-coinc-triggers"""
+  --coherent-inspiral, --summary-inspiral-triggers --summary-coinc-triggers""", file=sys.stderr)
   sys.exit(1)
    
 ifo_list = ['H1','H2','L1','V1','G1']
@@ -688,11 +688,11 @@
 
   # check the values given
   if startExttrig < 1:
-    print >> sys.stderr, "exttrig-inj-start must be larger than 0."
+    print("exttrig-inj-start must be larger than 0.", file=sys.stderr)
     sys.exit(1)
   if startExttrig > stopExttrig:
-    print >> sys.stderr, "exttrig-inj-stop must be larger than "\
-                         "exttrig-inj-start."
+    print("exttrig-inj-stop must be larger than "\
+                         "exttrig-inj-start.", file=sys.stderr)
     sys.exit(1)
 else:
   exttrigInjections=[0,0]
@@ -808,7 +808,7 @@
 elif play_data_mask == 'all_data':
   playground_only = 0
 else:
-  print "Invalid playground data mask " + play_data_mask + " specified"
+  print("Invalid playground data mask " + play_data_mask + " specified")
   sys.exit(1)
 
 ##############################################################################
@@ -872,7 +872,7 @@
 
 playground_only = 0
 
-print "reading in single ifo science segments and creating master chunks...",
+print("reading in single ifo science segments and creating master chunks...", end=' ')
 sys.stdout.flush()
 
 segments = {}
@@ -891,7 +891,7 @@
     data[ifo].make_chunks_from_unused(length,overlap/2,playground_only,
         0,0,overlap/2,pad)
 
-print "done"
+print("done")
 sys.stdout.flush()
 
 # work out the earliest and latest times that are being analyzed
@@ -900,8 +900,8 @@
   for ifo in ifo_list:
     if data[ifo] and (data[ifo][0].start() < gps_start_time):
       gps_start_time = data[ifo][0].start()
-  print "GPS start time not specified, obtained from segment lists as " + \
-    str(gps_start_time)
+  print("GPS start time not specified, obtained from segment lists as " + \
+    str(gps_start_time))
 
 
 if not gps_end_time:
@@ -909,8 +909,8 @@
   for ifo in ifo_list:
     if data[ifo] and (data[ifo][-1].end() > gps_end_time):
       gps_end_time = data[ifo][0].end()
-  print "GPS end time not specified, obtained from segment lists as " + \
-    str(gps_end_time)
+  print("GPS end time not specified, obtained from segment lists as " + \
+    str(gps_end_time))
 
 ##############################################################################
 #   Step 2: determine analyzable times
@@ -958,8 +958,8 @@
     data_out[ifo].intersection(temp)
     sys.stdout.flush()
   else:
-    raise ValueError, "exttrig-analyze has been set to a bad value.  It "\
-      "must be {on_source | off_source | all} or omitted."
+    raise ValueError("exttrig-analyze has been set to a bad value.  It "\
+      "must be {on_source | off_source | all} or omitted.")
 
   not_data_out[ifo] = copy.deepcopy(data_out[ifo])
   not_data_out[ifo].coalesce()
@@ -999,7 +999,7 @@
 
 prev_df = None
 
-print "setting up jobs to filter " + ifo + " data...",
+print("setting up jobs to filter " + ifo + " data...", end=' ')
 sys.stdout.flush()
 
 chunks_analyzed = analyze_coh(ifo_analyze,data,data_to_do,  
@@ -1008,13 +1008,13 @@
     sbBankFile=splitBankFile,sbNumBanks = splitNumBanks,
     runSpinChecker=opts.spin_checker,sc_job=spincheck_job)
 
-print "done" 
+print("done") 
 
 ##############################################################################
 # Step 12: Write out the LAL cache files for the various output data
 
 if gps_start_time is not None and gps_end_time is not None:
-  print "generating cache files for output data products...",
+  print("generating cache files for output data products...", end=' ')
   cache_fname = ''
   for ifo in ifo_analyze: 
     cache_fname += ifo
@@ -1042,9 +1042,9 @@
       output_data_cache.append(lal.Cache.from_urls([node.get_missed()])[0])
 
   output_data_cache.tofile(open(cache_fname, "w"))
-  print "done"
+  print("done")
 else:
-  print "gps start and stop times not specified: cache files not generated"
+  print("gps start and stop times not specified: cache files not generated")
 
 
 ##############################################################################
@@ -1075,16 +1075,16 @@
 # write a message telling the user that the DAG has been written
 if opts.dax:
   
-  print "\nCreated an abstract DAX file", dag.get_dag_file()
-  print "which can be transformed into a concrete DAG with gencdag."
-  print "\nSee the documentation on http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/pegasus_lsc.html"
+  print("\nCreated an abstract DAX file", dag.get_dag_file())
+  print("which can be transformed into a concrete DAG with gencdag.")
+  print("\nSee the documentation on http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/pegasus_lsc.html")
 
 
 
 else:
-  print "\nCreated a DAG file which can be submitted by executing"
-  print "\n   condor_submit_dag", dag.get_dag_file()
-  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
+  print("\nCreated a DAG file which can be submitted by executing")
+  print("\n   condor_submit_dag", dag.get_dag_file())
+  print("""\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
   If you are running LSCdataFind jobs, do not forget to initialize your grid 
   proxy certificate on the condor submit machine by running the commands
   
@@ -1107,7 +1107,7 @@
   
   Contact the administrator of your cluster to find the hostname and port of the
   LSCdataFind server.
-  """
+  """)
 
 ##############################################################################
 # write out a log file for this script
@@ -1131,38 +1131,38 @@
 log_fh.write( cp.get('pipeline','version') + "\n" )
 log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )
 
-print >> log_fh, "\n===========================================\n"
-print >> log_fh, "Science Segments and master chunks:\n"
+print("\n===========================================\n", file=log_fh)
+print("Science Segments and master chunks:\n", file=log_fh)
 
 for ifo in ifo_list:
-  print >> log_fh, "\n===========================================\n"
-  print >> log_fh, ifo + "Data\n"
+  print("\n===========================================\n", file=log_fh)
+  print(ifo + "Data\n", file=log_fh)
   for seg in data[ifo]:
-    print >> log_fh, " ", seg
+    print(" ", seg, file=log_fh)
     for chunk in seg:
-      print >> log_fh, "   ", chunk
+      print("   ", chunk, file=log_fh)
 
 
 for ifo in ifo_analyze:
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( 
     "Filtering " + str(len(chunks_analyzed[ifo])) + " " + ifo + \
     " master chunks\n" )
   total_time = 0
   for ifo_done in chunks_analyzed[ifo]:
-    print >> log_fh, ifo_done.get_chunk()
+    print(ifo_done.get_chunk(), file=log_fh)
     total_time += len(ifo_done.get_chunk())
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
 for ifo in ifo_analyze:
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( "Writing " + str(len(analyzed_data[ifo])) + " " + ifo + \
     " single IFO science segments\n" )
   total_time = 0
   for seg in analyzed_data[ifo]:
-    print >> log_fh, seg
+    print(seg, file=log_fh)
     total_time += seg.dur()
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
   if opts.output_segs and len(analyzed_data[ifo]):
     if playground_only:
@@ -1176,14 +1176,14 @@
 
 
 for ifos in ifo_coincs:  
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( "Writing " + str(len(analyzed_data[ifos])) + " " + ifos + \
     " coincident segments\n" )
   total_time = 0
   for seg in analyzed_data[ifos]:
-    print >> log_fh, seg
+    print(seg, file=log_fh)
     total_time += seg.dur()
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
   if opts.output_segs and len(analyzed_data[ifos]):
     if playground_only:
--- ./src/inspiral/lalapps_coh_PTF_post_processing.py	(original)
+++ ./src/inspiral/lalapps_coh_PTF_post_processing.py	(refactored)
@@ -343,8 +343,7 @@
     parser.error('Must give --config-file')
 
   if not opts.inj_config_file:
-    print >>sys.stdout,\
-         "Injection config file not given. Running with no injections."
+    print("Injection config file not given. Running with no injections.", file=sys.stdout)
     opts.skip_injfind=True
     opts.skip_injcombiner=True
   #  parser.error('Must give --inj-config-file')
@@ -362,14 +361,14 @@
 
   # load ini files
   if verbose:
-    print >>sys.stdout
-    print >>sys.stdout, 'Initialising post processing driver, '+\
-                        'loading configuration files...'
+    print(file=sys.stdout)
+    print('Initialising post processing driver, '+\
+                        'loading configuration files...', file=sys.stdout)
 
   # get directory
   grbdir = os.path.abspath('%s/GRB%s' % (rundir, grb))
   if not os.path.isdir(grbdir):
-    raise ValueError, 'Cannot find directory GRB%s in %s' % (grb, rundir)
+    raise ValueError('Cannot find directory GRB%s in %s' % (grb, rundir))
 
   # generate post processing directory
   if not os.path.isdir(outdir):
@@ -419,7 +418,7 @@
                 % (grbdir, ifotag, grb)
   datafindglob = glob.glob(datafindstr)
   if len(datafindglob)!=1:
-    raise ValueError, 'Cannot find single datafind cache matching %s' % datafindstr
+    raise ValueError('Cannot find single datafind cache matching %s' % datafindstr)
   datafindcache = datafindglob[0]
 
   datastart, dataduration = map(int, os.path.splitext(datafindcache)[0]\
@@ -500,8 +499,8 @@
   # ==========
 
   if verbose:
-    print >>sys.stdout
-    print >>sys.stdout, "Generating dag..."
+    print(file=sys.stdout)
+    print("Generating dag...", file=sys.stdout)
 
   # initialise uberdag
   dagtag  = os.path.splitext(os.path.basename(inifile))[0]
@@ -667,7 +666,7 @@
     # find buffer segments
     buffseg = '%s/%s' % (grbdir, 'bufferSeg.txt')
     if not os.path.isfile(buffseg):
-      raise ValueError, 'Cannot find buffer segment file as %s' % buffseg
+      raise ValueError('Cannot find buffer segment file as %s' % buffseg)
   
     tag = 'injfinder'
     exe = cp.get('condor', tag)
@@ -761,8 +760,8 @@
           distRun = run
           break
       if not distRun:
-        raise BrokenError, "Cannot find any injections matching %s in ini file"\
-                           % (injpattern)
+        raise BrokenError("Cannot find any injections matching %s in ini file"\
+                           % (injpattern))
       for inc in inclinations:
         injrun = 'injectionsAstro%s_FILTERED_%d' % (injpattern,inc)
         filteredInjRuns.append(injrun)
@@ -914,23 +913,23 @@
   uberdag.write_dag()
 
   # print message
-  print >>sys.stdout
-  print >>sys.stdout, '------------------------------------'
-  print >>sys.stdout, 'Ready. To submit, run:'
-  print >>sys.stdout
+  print(file=sys.stdout)
+  print('------------------------------------', file=sys.stdout)
+  print('Ready. To submit, run:', file=sys.stdout)
+  print(file=sys.stdout)
   subcmd = 'condor_submit_dag '
   if cp.has_option('pipeline', 'maxjobs'):
     subcmd += '-maxjobs %s ' % cp.getint('pipeline', 'maxjobs')
   subcmd += os.path.abspath(uberdag.get_dag_file())
-  print >>sys.stdout, subcmd
-  print >>sys.stdout
-
-  print >>sys.stdout, 'Once submitted, to monitor status, run:'
-  print >>sys.stdout
-  print >>sys.stdout, 'lalapps_ihope_status --dag-file %s'\
-                      % (os.path.abspath(uberdag.get_dag_file()))
-  print >>sys.stdout, '------------------------------------'
-  print >>sys.stdout
+  print(subcmd, file=sys.stdout)
+  print(file=sys.stdout)
+
+  print('Once submitted, to monitor status, run:', file=sys.stdout)
+  print(file=sys.stdout)
+  print('lalapps_ihope_status --dag-file %s'\
+                      % (os.path.abspath(uberdag.get_dag_file())), file=sys.stdout)
+  print('------------------------------------', file=sys.stdout)
+  print(file=sys.stdout)
 
 
 if __name__=='__main__':
--- ./src/inspiral/lalapps_compare_ini.py	(original)
+++ ./src/inspiral/lalapps_compare_ini.py	(refactored)
@@ -44,11 +44,11 @@
 
 # Print a list of sections that do not appear in the config file
 for section in missingsections:
-  print "MISSING items in " + opts.config_file + ": [" + section + "] and options " + str(cpref.items(section))
+  print("MISSING items in " + opts.config_file + ": [" + section + "] and options " + str(cpref.items(section)))
 
 # Print a list of sections that appear in the config file, but not in the reference
 for section in extrasections:
-  print "EXTRA items in " + opts.config_file + ": [" + section + "] and options " + str(cp.items(section))
+  print("EXTRA items in " + opts.config_file + ": [" + section + "] and options " + str(cp.items(section)))
 
 # Loop over the sections that appear in both files
 
@@ -69,15 +69,15 @@
 
   # Print a list of options missing from the config file
   for opt in missingopts:
-    print "MISSING option in " + opts.config_file + " in [" + section + "]: " + opt + " = " + refdict[opt]  
+    print("MISSING option in " + opts.config_file + " in [" + section + "]: " + opt + " = " + refdict[opt])  
 
   # Print a list of extra options in the config file that aren't in the reference
   for opt in extraopts:
-    print "EXTRA option in " + opts.config_file + " in [" + section + "]: "+ opt + " = " + configdict[opt]
+    print("EXTRA option in " + opts.config_file + " in [" + section + "]: "+ opt + " = " + configdict[opt])
 
   # Compare the option values between the two files and print a FAILURE message if they don't match
   for opt in commonopts:
     configvalue = configdict[opt]
     refvalue = refdict[opt]
     if configvalue != refvalue:
-      print "FAILED MATCH: [" + section +"]: " + opt + " ; REFERENCE = " + refvalue + ", CONFIG FILE = " + configvalue
+      print("FAILED MATCH: [" + section +"]: " + opt + " ; REFERENCE = " + refvalue + ", CONFIG FILE = " + configvalue)
--- ./src/inspiral/lalapps_flag_triggers.py	(original)
+++ ./src/inspiral/lalapps_flag_triggers.py	(refactored)
@@ -55,22 +55,22 @@
     options, others = parser.parse_args()
 
     if not options.ifo:
-        raise ValueError, "missing required argument --ifo"
+        raise ValueError("missing required argument --ifo")
 
     if not options.gps_end_time:
-        raise ValueError, "missing required argument --gps-end_time"
+        raise ValueError("missing required argument --gps-end_time")
    
     if not options.gps_start_time:
-        raise ValueError, "missing required argument --gps-start_time"
+        raise ValueError("missing required argument --gps-start_time")
    
     if not options.veto_category:
-        raise ValueError, "missing required argument --veto-category"
+        raise ValueError("missing required argument --veto-category")
    
     if len( [x for x in (options.unclustered, options.thirty_ms, options.sixteen_sec) if x] ) != 1:
-        raise ValueError, "must provide one of [--unclustered | --thirty-ms | --sixteen-sec]"
+        raise ValueError("must provide one of [--unclustered | --thirty-ms | --sixteen-sec]")
 
     if len( [x for x in (options.min_snr, options.min_new_snr) if x] ) != 1:
-        raise ValueError, "must provide exactly one of [--min-snr | --min-new-snr]"
+        raise ValueError("must provide exactly one of [--min-snr | --min-new-snr]")
 
     return options
 
@@ -126,7 +126,7 @@
     all_triggers = get_triggers(basedir, ifo, cluster, cat, start, end, snrifier, filter_func)
     num_triggers = len(all_triggers)
 
-    print "Found %d triggers\n\n" % num_triggers
+    print("Found %d triggers\n\n" % num_triggers)
 
     # divide by 100 = multiply percentage by 100 below
     num_triggers = float(num_triggers)
@@ -146,5 +146,5 @@
     use_percents = sorted(use_percents, cmp=lambda x,y:cmp(len(y[1]),len(x[1])))
 
     for name, triggers in use_percents:
-        print '%-45s        %.2f' % (name[3:], 100.0 * (float(len(triggers)) / num_triggers))
+        print('%-45s        %.2f' % (name[3:], 100.0 * (float(len(triggers)) / num_triggers)))
 
--- ./src/inspiral/lalapps_followup0_page.py	(original)
+++ ./src/inspiral/lalapps_followup0_page.py	(refactored)
@@ -14,7 +14,7 @@
   path = glob.glob('*followup_pipe.ini')
   if path[0]: cp.read(path[0])
   else:
-    print "Could not find a match to *followup_pipe.ini"
+    print("Could not find a match to *followup_pipe.ini")
     sys.exit(1)
   dest = cp.get('followup-output','page')
   server = cp.get('followup-output','url')
@@ -152,14 +152,14 @@
 
   # Make a destination directory if it doesn't exist
   if not os.path.isdir(dest):
-    print "destination:" + dest + " does not exist, I'll make it for you..."
+    print("destination:" + dest + " does not exist, I'll make it for you...")
     os.makedirs(dest)
 
   # copy the data to the webspace - sort of painful probably could omit some
   for sec in sections:
-    print "...copying " + sec + " to " + dest
+    print("...copying " + sec + " to " + dest)
     try: shutil.copytree(sec, os.path.join(dest,sec))
-    except: print "could not copy, destination probably exists"
+    except: print("could not copy, destination probably exists")
 
 # table of contents
 def write_header(wiki):
--- ./src/inspiral/lalapps_followup0_pipe.py	(original)
+++ ./src/inspiral/lalapps_followup0_pipe.py	(refactored)
@@ -232,19 +232,19 @@
 (opts,args) = parser.parse_args()
 
 if opts.version:
-  print "$Id$"
+  print("$Id$")
   sys.exit(0)
 
 ####################### SANITY CHECKS #####################################
 
 if not opts.config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location" 
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location", file=sys.stderr) 
   sys.exit(1)
 
 if not opts.log_path and not opts.write_to_iulgroup:
-  print >> sys.stderr, "No log file path specified"
-  print >> sys.stderr, "Use --log-path PATH to specify a location"
+  print("No log file path specified", file=sys.stderr)
+  print("Use --log-path PATH to specify a location", file=sys.stderr)
   sys.exit(1)
 
 if not opts.write_to_iulgroup and not opts.generate_fu_cache and \
@@ -263,27 +263,27 @@
   and not opts.make_checklist and not opts.create_localcopy \
   and not opts.followup_triggers and not opts.spin_mcmc:
 
-  print >> sys.stderr, "No steps of the pipeline specified."
-  print >> sys.stderr, "Please specify at least one of"
-  print >> sys.stderr, "--generate-fu-cache, --trig-bank, --inspiral, --plots,"
-  print >> sys.stderr, "--datafind, --qscan, --hoft-qscan, --seis-qscan,"
-  print >> sys.stderr, "--background-qscan, --background-hoft-qscan,"
-  print >> sys.stderr, "--background-seis-qscan, --hoft-datafind,"
-  print >> sys.stderr, "--generate-segments, --frame-check, --inspiral-datafind,"
-  print >> sys.stderr, "--analyse-qscan, --analyse-seis-qscan,"
-  print >> sys.stderr, "--analyse-hoft-qscan, --distrib-remote-q,"
-  print >> sys.stderr, "--mcmc, --plot-mcmc, --coh-inspiral, --plot-chia,"
-  print >> sys.stderr, "--sky-map, --sky-map-plot, --followup-triggers,"
-  print >> sys.stderr, "--convert-eventid, --create-localcopy,"
-  print >> sys.stderr, "--ifo-status-check, --single-qevent, --H1H2-qevent"
-  print >> sys.stderr, "--make-checklist or --spin-mcmc or --write-to-iulgroup" 
+  print("No steps of the pipeline specified.", file=sys.stderr)
+  print("Please specify at least one of", file=sys.stderr)
+  print("--generate-fu-cache, --trig-bank, --inspiral, --plots,", file=sys.stderr)
+  print("--datafind, --qscan, --hoft-qscan, --seis-qscan,", file=sys.stderr)
+  print("--background-qscan, --background-hoft-qscan,", file=sys.stderr)
+  print("--background-seis-qscan, --hoft-datafind,", file=sys.stderr)
+  print("--generate-segments, --frame-check, --inspiral-datafind,", file=sys.stderr)
+  print("--analyse-qscan, --analyse-seis-qscan,", file=sys.stderr)
+  print("--analyse-hoft-qscan, --distrib-remote-q,", file=sys.stderr)
+  print("--mcmc, --plot-mcmc, --coh-inspiral, --plot-chia,", file=sys.stderr)
+  print("--sky-map, --sky-map-plot, --followup-triggers,", file=sys.stderr)
+  print("--convert-eventid, --create-localcopy,", file=sys.stderr)
+  print("--ifo-status-check, --single-qevent, --H1H2-qevent", file=sys.stderr)
+  print("--make-checklist or --spin-mcmc or --write-to-iulgroup", file=sys.stderr) 
   sys.exit(1)
 
 if opts.disable_followup:
-  print >> sys.stderr, "Warning: the option disable-followup disables any followup jobs, only qscan datafind and background qscan jobs will be run..."
+  print("Warning: the option disable-followup disables any followup jobs, only qscan datafind and background qscan jobs will be run...", file=sys.stderr)
 
 if opts.read_times:
-  print >> sys.stderr, "Warning: the option read-times disables the standard behaviour of the pipeline. The \"hipe-output-cache\" or \"xml-glob\" files will be ignored. Instead the times to be analysed will be read within the text files specified by the fields \"XXtimes\" of the section [triggers] of the .ini file"
+  print("Warning: the option read-times disables the standard behaviour of the pipeline. The \"hipe-output-cache\" or \"xml-glob\" files will be ignored. Instead the times to be analysed will be read within the text files specified by the fields \"XXtimes\" of the section [triggers] of the .ini file", file=sys.stderr)
 
 #################### READ IN THE CONFIG (.ini) FILE ########################
 cp = ConfigParser.ConfigParser()
@@ -321,11 +321,11 @@
     if not os.access('LOCAL_XML_COPY',os.F_OK):
       os.mkdir('LOCAL_XML_COPY')
     else: pass
-  print "Reading files from cache..."
+  print("Reading files from cache...")
   numtrigs, found, coincs, search = cache.readTriggerFiles(cp,opts)
   missed = None
-  if coincs: print "found " + str(len(coincs)) + " triggers..."
-  else: print "WARNING: NO COINCS FOUND..."
+  if coincs: print("found " + str(len(coincs)) + " triggers...")
+  else: print("WARNING: NO COINCS FOUND...")
   if opts.trig_bank: trigbank_test = 1
   else: trigbank_test = 0
   if opts.disable_ifarsorting: ifar = False
@@ -337,7 +337,7 @@
   for trig in followuptrigs: trig.write_trigger_info(trigInfo)
   trigInfo.close()
 
-  print "\n.......Found " + str(len(followuptrigs)) + " trigs to follow up" 
+  print("\n.......Found " + str(len(followuptrigs)) + " trigs to follow up") 
 
 ############ SET UP THE REQUESTED JOBS ########################################
 
@@ -377,7 +377,7 @@
 qscanBgJob      = qscanJob(opts,cp,'QSCANLITE')
 distribQJob     = distributeQscanJob(cp)
 
-print "\n.......Setting up pipeline jobs"
+print("\n.......Setting up pipeline jobs")
 
 dq_url_pattern = "http://ldas-cit.ligo.caltech.edu/segments/S5/%s/dq_segments.txt"
 
@@ -408,7 +408,7 @@
     if cp.has_option("followup-"+depQscan, depIfoIniConfig):
       qscanConfig = string.strip(cp.get("followup-"+depQscan, depIfoIniConfig))
       if qscanConfig!='':
-        print 'copy '+qscanConfig+' -----> '+depIfoDir+'/CONFIG/'+depQscan+'_config.txt'
+        print('copy '+qscanConfig+' -----> '+depIfoDir+'/CONFIG/'+depQscan+'_config.txt')
         os.system('cp '+qscanConfig+' '+depIfoDir+'/CONFIG/'+depQscan+'_config.txt')
   
   # Copy the scripts used in the remote computing center
@@ -425,7 +425,7 @@
   depIfoWebForeground = string.strip(cp.get('followup-foreground-qscan', depIfoIniWeb))
   if not depIfoWebForeground.startswith('http://virgo.in2p3.fr/followups/'):
     print('\nWARNING for foreground qscans:')
-    print('   wrong web address : '+depIfoWebForeground)
+    print(('   wrong web address : '+depIfoWebForeground))
     print('   The web address for Virgo qscans should start with \"http://virgo.in2p3.fr/followups/\"')
     print('   followed by the name of the submitter of the jobs')
   else:
@@ -436,7 +436,7 @@
   depIfoWebForegroundSeismic = string.strip(cp.get('followup-foreground-seismic-qscan', depIfoIniWeb))
   if not depIfoWebForegroundSeismic.startswith('http://virgo.in2p3.fr/followups/'):
     print('\nWARNING for foreground-seismic qscans:')
-    print('   wrong web address : '+depIfoWebForegroundSeismic)
+    print(('   wrong web address : '+depIfoWebForegroundSeismic))
     print('   The web address for Virgo qscans should start with \"http://virgo.in2p3.fr/followups/\"')
     print('   followed by the name of the submitter of the jobs')
   else:
--- ./src/inspiral/lalapps_generate_upper_limits.py	(original)
+++ ./src/inspiral/lalapps_generate_upper_limits.py	(refactored)
@@ -499,7 +499,7 @@
 # -- get command line arguments
 opts, args = parse_command_line()
 if not opts.config_file:
-  print >> sys.stderr , 'You must specify a config file'
+  print('You must specify a config file', file=sys.stderr)
   sys.exit(1)
 
 ###################################
--- ./src/inspiral/lalapps_glitch_probe.py	(original)
+++ ./src/inspiral/lalapps_glitch_probe.py	(refactored)
@@ -39,22 +39,22 @@
     options, others = parser.parse_args()
 
     if not (options.run_chisq or options.assemble_results):
-        raise ValueError, "Missing required argument, one of --run-chisq or --assemble-results"
+        raise ValueError("Missing required argument, one of --run-chisq or --assemble-results")
 
     if (options.run_chisq and options.assemble_results):
-        raise ValueError, "Please specify only one of --run-chisq or --assemble-results"
+        raise ValueError("Please specify only one of --run-chisq or --assemble-results")
 
     if not options.gps_start_time:
-        raise ValueError, "Missing required argument --gps-start-time"
+        raise ValueError("Missing required argument --gps-start-time")
 
     if not options.gps_end_time:
-        raise ValueError, "Missing required argument --gps-end-time"
+        raise ValueError("Missing required argument --gps-end-time")
 
     if not options.snr_threshold:
-        raise ValueError, "Missing required argument --snr-threshold"
+        raise ValueError("Missing required argument --snr-threshold")
 
     if not options.ifo:
-        raise ValueError, "Missing required argument --ifo"
+        raise ValueError("Missing required argument --ifo")
     
     return options
 
@@ -209,7 +209,7 @@
             f = [l for l in open('%s/daily.dag' % dir) if l.find('UNCLUSTERED_00') > 0 and l.find(timearg) > 0]
 
             if f == []:
-                print "No line in dag to create triggers at time %d" % tme
+                print("No line in dag to create triggers at time %d" % tme)
                 sys.exit(0)
 
             jobid = f[0].split(' ')[1]
@@ -247,14 +247,14 @@
 
             # Create a submit file.
             newsub = open('chisq.sub','w')
-            print >>newsub,"""Executable = /archive/home/cbc/opt/s6b/latest/bin/lalapps_populate_chisq
+            print("""Executable = /archive/home/cbc/opt/s6b/latest/bin/lalapps_populate_chisq
 Args       = %s
 getenv     = True
 Universe   = local
 output     = chisq.$(cluster).$(process).out
 error      = chisq.$(cluster).$(process).error
 Log        = chisq.$(cluster).$(process).log
-Queue""" % job
+Queue""" % job, file=newsub)
 
             newsub.close()
 
@@ -312,12 +312,12 @@
             make_chisq_dir(basedir, ifo, t_end_time)
 
     if options.assemble_results:
-        print '|| OL end time || OL end time ns || OL SNR || NL end time || NL end time ns || NL new SNR || Vetoed at cat ||',
+        print('|| OL end time || OL end time ns || OL SNR || NL end time || NL end time ns || NL new SNR || Vetoed at cat ||', end=' ')
 
         if options.baseurl:
-            print '  Unclustered || 100 millisec clustered || New SNR ||'
+            print('  Unclustered || 100 millisec clustered || New SNR ||')
         else:
-            print
+            print()
 
         cwd = os.getcwd()
 
@@ -332,7 +332,7 @@
             chisq_trig_file       = glob.glob('*CHISQ*')
 
             if chisq_trig_file == []:
-                print >>sys.stderr,"Job in directory %s did not succeed" % chisq_dir
+                print("Job in directory %s did not succeed" % chisq_dir, file=sys.stderr)
                 continue
 
             chisq_trig_file = chisq_trig_file[0]
@@ -357,15 +357,15 @@
                     vetoed = str(cat)
                     break
             
-            print '|| %d.%d || %.2f || %.2f || %.2f || %d.%d || %.2f || %s ||' % (t_end_time, t_end_time_ns, snr, original_chisq, original_new_snr, loudest_t, loudest_t_ns, loudest_new_snr, vetoed),
+            print('|| %d.%d || %.2f || %.2f || %.2f || %d.%d || %.2f || %s ||' % (t_end_time, t_end_time_ns, snr, original_chisq, original_new_snr, loudest_t, loudest_t_ns, loudest_new_snr, vetoed), end=' ')
 
             if options.baseurl:
                 u = options.baseurl
                 t = t_end_time
 
-                print ' [[%s/%d_unclustered.png|img]] || [[%s/%d_clustered.png|img]] || [[%s/%d_chisq.png|img]]  ||' % (u, t, u, t, u, t)
+                print(' [[%s/%d_unclustered.png|img]] || [[%s/%d_clustered.png|img]] || [[%s/%d_chisq.png|img]]  ||' % (u, t, u, t, u, t))
             else:
-                print
-
-
-
+                print()
+
+
+
--- ./src/inspiral/lalapps_ihope.py	(original)
+++ ./src/inspiral/lalapps_ihope.py	(refactored)
@@ -184,38 +184,38 @@
 ##############################################################################
 # Sanity check of input arguments
 if not opts.config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.config_file[-4:]==".ini":
-  print >> sys.stderr, "Configuration file name must end in '.ini'!" 
+  print("Configuration file name must end in '.ini'!", file=sys.stderr) 
   sys.exit(1)
 
 if not opts.log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 if opts.run_pipedown:
   if not opts.node_local_dir:
-    print >> sys.stderr, "No local dir specified. If running with pipedown"
-    print >> sys.stderr, "use --node-local-dir to specify a local directory"
-    print >> sys.stderr, "Use --help for more information"
+    print("No local dir specified. If running with pipedown", file=sys.stderr)
+    print("use --node-local-dir to specify a local directory", file=sys.stderr)
+    print("Use --help for more information", file=sys.stderr)
     sys.exit(1)
 
 if not opts.gps_start_time:
-  print >> sys.stderr, "No GPS start time specified for the analysis"
-  print >> sys.stderr, "Use --gps-start-time GPS_START to specify a location."
+  print("No GPS start time specified for the analysis", file=sys.stderr)
+  print("Use --gps-start-time GPS_START to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.gps_end_time:
-  print >> sys.stderr, "No GPS end time specified for the analysis"
-  print >> sys.stderr, "Use --gps-end-time GPS_END to specify a location."
+  print("No GPS end time specified for the analysis", file=sys.stderr)
+  print("Use --gps-end-time GPS_END to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 if opts.gps_end_time < opts.gps_start_time:
-  print >> sys.stderr, "The GPS end time must be after the GPS start time"
+  print("The GPS end time must be after the GPS start time", file=sys.stderr)
   sys.exit(1)
 
 opts.complete_cache = (opts.run_data_quality or opts.run_plots or opts.run_pipedown or opts.run_search)
@@ -226,9 +226,9 @@
 def check_grid_proxy(path):
   try:
     proxy = M2Crypto.X509.load_cert(path)
-  except Exception, e:
+  except Exception as e:
     msg = "Unable to load proxy from path %s : %s" % (path, e)
-    raise RuntimeError, msg
+    raise RuntimeError(msg)
 
   try:
     proxy.get_ext("proxyCertInfo")
@@ -236,16 +236,16 @@
     subject = proxy.get_subject().as_text()
     if re.search(r'.+CN=proxy$', subject):
       msg = "Proxy %s is not RFC compliant" % path
-      raise RuntimeError, msg
+      raise RuntimeError(msg)
 
   try:
     expireASN1 = proxy.get_not_after().__str__()
     expireGMT  = time.strptime(expireASN1, "%b %d %H:%M:%S %Y %Z")
     expireUTC  = calendar.timegm(expireGMT)
     now = int(time.time())
-  except Exception, e:
+  except Exception as e:
     msg = "could not determine time left on proxy: %s" % e
-    raise RuntimeError, msg
+    raise RuntimeError(msg)
 
   return expireUTC - now
 
@@ -278,15 +278,15 @@
   # check that the proxy is valid and that enough time remains
   time_left = check_grid_proxy(proxy_path)
   if time_left < 0:
-    raise RuntimeError, "Proxy has expired."
+    raise RuntimeError("Proxy has expired.")
   elif time_left < time_needed:
     msg = "Not enough time left on grid proxy (%d seconds)." % time_left
-    raise RuntimeError, msg
+    raise RuntimeError(msg)
   else:
     os.environ['X509_USER_PROXY'] = proxy_path
 
 except:
-  print >> sys.stderr, """
+  print("""
 Error: Could not find a valid grid proxy. Please run 
 
    ligo-proxy-init albert.einstein
@@ -301,7 +301,7 @@
   grid-proxy-info
 
 At least %d seconds must be left before your proxy expires to run ihope.
-""" % time_needed
+""" % time_needed, file=sys.stderr)
   raise
 
 ##############################################################################
@@ -385,16 +385,15 @@
   if cp.has_option("ifo-details",option): ifos.append(option[0:2].upper() )
 
 if cp.has_option("ifo-details","analyze-all"): 
-  print >> sys.stderr, \
-      "The inspiral pipeline does not yet support coincidence between"
-  print >> sys.stderr, "all five IFOs. Do not use the analyze-all option."
+  print("The inspiral pipeline does not yet support coincidence between", file=sys.stderr)
+  print("all five IFOs. Do not use the analyze-all option.", file=sys.stderr)
   sys.exit(1)
 
 ifos.sort()
 
-print "Setting up an analysis for " + str(ifos) + " from " + \
-    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time)
-print
+print("Setting up an analysis for " + str(ifos) + " from " + \
+    str(opts.gps_start_time) + " to "  + str(opts.gps_end_time))
+print()
 sys.stdout.flush()
 
 
@@ -517,7 +516,7 @@
 # Run lalapps_inspiral_hipe for datafind and template bank generation
 
 if opts.run_datafind or opts.run_tmpltbank:
-  print "Running inspiral hipe for datafind/template bank run"
+  print("Running inspiral hipe for datafind/template bank run")
   hipeDfNode = inspiralutils.hipe_setup("datafind", cp, ifos, \
       opts.log_path, dataFind = opts.run_datafind, tmpltBank = opts.run_tmpltbank,
       dax=opts.dax, local_exec_dir=tmp_exec_dir, static_pfn_cache=peg_frame_cache,
@@ -548,7 +547,7 @@
   injSection = "injections"
 
   if opts.reverse_analysis:
-    print "Setting up dags for a reverse chirp analysis"
+    print("Setting up dags for a reverse chirp analysis")
     cp.set("inspiral", "reverse-chirp-bank","")
     # Add reverse to directories to specify that a reverse chirp search is being done
     playDir += "_reverse"
@@ -561,11 +560,11 @@
       break
 
     if opts.run_search:
-      print "Setting up the category " + str(category) + " veto dags"
+      print("Setting up the category " + str(category) + " veto dags")
 
     if opts.run_playground:
       if opts.run_search:
-        print "Running inspiral hipe for playground with vetoes"
+        print("Running inspiral hipe for playground with vetoes")
         hipePlayVetoNode[category] = inspiralutils.hipe_setup(
             playDir, cp, ifos, opts.log_path, \
             playOnly = True, vetoCat = category, vetoFiles = dqVetoes, \
@@ -594,13 +593,13 @@
         if os.path.isfile(playDir + "/" + cacheFile):
           cachelist.append(playDir + "/" + cacheFile)
         else:
-          print>>sys.stderr, "WARNING: Cache file " + playDir + "/" + cacheFile
-          print>>sys.stderr, "does not exist! This might cause later failures."
+          print("WARNING: Cache file " + playDir + "/" + cacheFile, file=sys.stderr)
+          print("does not exist! This might cause later failures.", file=sys.stderr)
 
 
     if opts.run_full_data:
       if opts.run_search:
-        print "Running inspiral hipe for full data with vetoes"
+        print("Running inspiral hipe for full data with vetoes")
         hipeAnalysisVetoNode[category] = inspiralutils.hipe_setup(
             fullDir, cp, ifos, opts.log_path, \
             vetoCat = category, vetoFiles = dqVetoes, \
@@ -631,8 +630,8 @@
         if os.path.isfile(fullDir + "/" + cacheFile):
           cachelist.append(fullDir + "/" + cacheFile)
         else:
-          print>>sys.stderr, "WARNING: Cache file " + fullDir + "/" + cacheFile
-          print>>sys.stderr, "does not exist! This might cause later failures."
+          print("WARNING: Cache file " + fullDir + "/" + cacheFile, file=sys.stderr)
+          print("does not exist! This might cause later failures.", file=sys.stderr)
 
 
     if opts.run_injections:
@@ -641,7 +640,7 @@
 
       for (injDir, injSeed) in cp.items( injSection ):
         if opts.run_search:
-          print "Running inspiral hipe for " + injDir + " with vetoes"
+          print("Running inspiral hipe for " + injDir + " with vetoes")
           hipeInjVetoNode[category][injDir] = inspiralutils.hipe_setup(
               injDir, cp, ifos, opts.log_path, \
               injSeed = injSeed, vetoCat = category, vetoFiles = dqVetoes, \
@@ -672,8 +671,8 @@
           if os.path.isfile(injDir + "/" + cacheFile):
             cachelist.append(injDir + "/" + cacheFile)
           else:
-            print>>sys.stderr, "WARNING: Cache file " + injDir + "/" + cacheFile
-            print>>sys.stderr, "does not exist! This might cause later failures."
+            print("WARNING: Cache file " + injDir + "/" + cacheFile, file=sys.stderr)
+            print("does not exist! This might cause later failures.", file=sys.stderr)
 
 
 ##############################################################################
@@ -705,7 +704,7 @@
 
   playgroundOnly = False
 
-  print "Running lalapps_pipedown"
+  print("Running lalapps_pipedown")
   dag = inspiralutils.pipedownSetup(dag,cp,opts.log_path,"pipedown",\
                             "../" + cachefilename,parentNodes,playgroundOnly,opts.run_mvsc)
 
@@ -739,7 +738,7 @@
       parentVetoNodes = [hipePlayVetoNodeWithout1]
     else: parentVetoNodes = None
     
-    print "Making plots depending only on the playground"
+    print("Making plots depending only on the playground")
     dag = inspiralutils.zeroSlidePlots(dag, "playground_summary_plots", cp, \
         opts.log_path, "PLAYGROUND", "PLAYGROUND", "../" + cachefilename, \
         opts.do_dag_categories, parentNodes, parentVetoNodes, \
@@ -759,7 +758,7 @@
       parentVetoNodes = [hipeAnalysisVetoNodeWithout1]
     else: parentVetoNodes = None
 
-    print "Making full data zero lag plots"
+    print("Making full data zero lag plots")
     dag = inspiralutils.zeroSlidePlots(dag, "full_data_summary_plots", cp, \
         opts.log_path, "FULL_DATA", "FULL_DATA", "../" + cachefilename, \
         opts.do_dag_categories, parentNodes, parentVetoNodes, \
@@ -777,7 +776,7 @@
       parentVetoNodes = [hipePlayVetoNodeWithout1, hipeAnalysisVetoNodeWithout1]
     else: parentVetoNodes = None
 
-    print "Making plots with full data slides, playground zero lag"
+    print("Making plots with full data slides, playground zero lag")
     dag = inspiralutils.zeroSlidePlots(dag, "full_data_slide_summary_plots", \
         cp, opts.log_path, "PLAYGROUND", "FULL_DATA", "../" + cachefilename, \
         opts.do_dag_categories, parentNodes, parentVetoNodes, \
@@ -788,7 +787,7 @@
 
     hipeInjNode = {}
 
-    print "Making plots depending on the injection runs"
+    print("Making plots depending on the injection runs")
 
     if opts.run_full_data:
       slideSuffix = "FULL_DATA"
@@ -829,7 +828,7 @@
           "../" + cachefilename, opts.do_dag_categories, parentNodes, 
           parentVetoNodes, vetocats_without1, ifos)
 
-    print "Making plots of all injection runs together"
+    print("Making plots of all injection runs together")
 
     # set up parents
     if opts.run_search:
@@ -861,7 +860,7 @@
 
 if opts.run_hardware_inj:
 
-  print "Setting up hardware injection summary jobs"
+  print("Setting up hardware injection summary jobs")
 
   # set up parents
   parentNodes = []
@@ -918,10 +917,10 @@
 dag.write_sub_files()
 dag.write_dag()
 
-print 
-print "Created a workflow file which can be submitted by executing"
-print "\n    cd " + analysisDirectory
-print """
+print() 
+print("Created a workflow file which can be submitted by executing")
+print("\n    cd " + analysisDirectory)
+print("""
 and then
 
     ./pegasus_submit_dax
@@ -939,7 +938,7 @@
     pegasus-analyzer -t -i `./pegasus_basedir`
 
 to debug any failed jobs.
-"""
+""")
 
 ##############################################################################
 # write out a log file for this script
--- ./src/inspiral/lalapps_ihope_daily_page.py	(original)
+++ ./src/inspiral/lalapps_ihope_daily_page.py	(refactored)
@@ -179,22 +179,22 @@
 
     def render_job(self, out, follow=True):
         if self.local:
-            print >>out, 'JOB %s daily_ihope_page_local.sub' % self.id
+            print('JOB %s daily_ihope_page_local.sub' % self.id, file=out)
         else:
-            print >>out, 'JOB %s daily_ihope_page.sub' % self.id
-
-        print >>out, 'RETRY %s 3' % self.id
-        print >>out, 'VARS %s macroaction="%s" macroconfig="%s" macroifo="%s" macrocategory="%d" macrocluster="%s" macrogpsstarttime="%s" macrogpsendtime="%s" macroflower="%f"' % (self.id, self.action, self.config['filename'], self.ifo, self.level, self.cluster, self.start_time, self.end_time, self.flower)
+            print('JOB %s daily_ihope_page.sub' % self.id, file=out)
+
+        print('RETRY %s 3' % self.id, file=out)
+        print('VARS %s macroaction="%s" macroconfig="%s" macroifo="%s" macrocategory="%d" macrocluster="%s" macrogpsstarttime="%s" macrogpsendtime="%s" macroflower="%f"' % (self.id, self.action, self.config['filename'], self.ifo, self.level, self.cluster, self.start_time, self.end_time, self.flower), file=out)
 
 	if self.action in ['make_glitch_page','make_hwinj_page']:
-	    print >>out, 'CATEGORY %s database' % self.id
+	    print('CATEGORY %s database' % self.id, file=out)
 
         for c in self.children:
             c.render_job(out)
 
     def render_relationships(self, out):
         for c in self.children:
-            print >>out, 'PARENT %s CHILD %s' % (self.id, c.id)
+            print('PARENT %s CHILD %s' % (self.id, c.id), file=out)
             c.render_relationships(out)
 
 
@@ -267,12 +267,12 @@
     for root in roots:
         root.render_relationships(out)
 
-    print >>out, "MAXJOBS database 4"
+    print("MAXJOBS database 4", file=out)
     out.close()
 
 
     out = open(config['tmp_dir'] + '/daily_ihope_page.sub','w')
-    print >>out, """universe = vanilla
+    print("""universe = vanilla
 executable = %s
 arguments = " --config $(macroconfig) --action $(macroaction) --ifos $(macroifo) --veto-categories $(macrocategory) --cluster-categories $(macrocluster) --gps-start-time $(macrogpsstarttime) --gps-end-time $(macrogpsendtime) --flower $(macroflower)"
 priority     = 20
@@ -281,11 +281,11 @@
 error        = logs/daily_plot-$(cluster)-$(process).err
 output       = logs/daily_plot-$(cluster)-$(process).out
 notification = never
-queue 1""" % (config['ihope_daily_page'], config['tmp_dir'])
+queue 1""" % (config['ihope_daily_page'], config['tmp_dir']), file=out)
     out.close()
 
     out = open(config['tmp_dir'] + '/daily_ihope_page_local.sub','w')
-    print >>out, """universe = local
+    print("""universe = local
 executable = %s
 arguments = " --config $(macroconfig) --action $(macroaction) --ifos $(macroifo) --veto-categories $(macrocategory) --cluster-categories $(macrocluster) --gps-start-time $(macrogpsstarttime) --gps-end-time $(macrogpsendtime) "
 priority = 20
@@ -294,7 +294,7 @@
 error = logs/daily_plot-$(cluster)-$(process).err
 output = logs/daily_plot-$(cluster)-$(process).out
 notification = never
-queue 1""" % (config['ihope_daily_page'], config['tmp_dir'])
+queue 1""" % (config['ihope_daily_page'], config['tmp_dir']), file=out)
     out.close()
 
 
@@ -325,10 +325,10 @@
     options, filenames = parser.parse_args()
 
     if not options.action:
-        print >>sys.stderr, "Please specify --action"
+        print("Please specify --action", file=sys.stderr)
         sys.exit(-1)
     if not options.ifos:
-        print >>sys.stderr, "Please specify --ifos"
+        print("Please specify --ifos", file=sys.stderr)
         sys.exit(-1)
 
     return options
@@ -341,11 +341,11 @@
 #########################
 def parse_config(filename):
     if not os.path.exists(filename):
-        print >>sys.stderr, "Config file %s does not exist" % filename
+        print("Config file %s does not exist" % filename, file=sys.stderr)
         sys.exit(1)
 
     if not os.path.isfile(filename):
-        print >>sys.stderr, "Config file %s is not a file" % filename
+        print("Config file %s is not a file" % filename, file=sys.stderr)
         sys.exit(1)
 
     ret = {'filename':filename}
@@ -488,7 +488,7 @@
     sys.stdout.flush()
 
     f = open('%s/%s-INSPIRAL_%s.cache' % (config['tmp_dir'], ifo, cluster),'w')
-    print >>f, out.strip()
+    print(out.strip(), file=f)
     f.close()
 
     # templates
@@ -497,7 +497,7 @@
     out, err = proc.communicate("\n".join(files))
 
     f = open('%s/%s-TMPLTBANK_%s.cache' % (config['tmp_dir'], ifo, cluster),'w')
-    print >>f, out.strip()
+    print(out.strip(), file=f)
     f.close()
 
 
@@ -531,8 +531,8 @@
             for l in in_f:
                 end_time = int(l.split(',')[0])
                 if end_time >= start and end_time < end:
-                    print >>out_f, l,
-                    print l,
+                    print(l, end=' ', file=out_f)
+                    print(l, end=' ')
             in_f.close()
             out_f.close()
             os.remove('%s/%s-0-INSPIRAL_%s_tmp.csv' % (config['tmp_dir'], ifo, cluster))
@@ -552,7 +552,7 @@
                 # if they don't overlap this will fail
                 try:
                     overlap = search_seg & seg
-                    print >>out_f, '%d,%d' % (overlap[0], overlap[1])
+                    print('%d,%d' % (overlap[0], overlap[1]), file=out_f)
                 except:
                     pass
 
@@ -587,7 +587,7 @@
     for l in infile:
         end_time = int(l[0:9])   # assumes that end_time is the first field and has only 9 digits!
         if end_time not in vetoed_times:
-            print >>outfile, l,
+            print(l, end=' ', file=outfile)
 
     infile.close()
     outfile.close()
@@ -601,7 +601,7 @@
 
     new_summary = summary - vetoed_times
     for l in new_summary:
-        print >>outfile, '%d,%d' % (l[0], l[1])
+        print('%d,%d' % (l[0], l[1]), file=outfile)
 
     infile.close()
     outfile.close()
@@ -675,10 +675,10 @@
     analyzed_time = abs(t)
 
     out = open('%s/%s-%d_%s_summary_table.html' % (config['out_dir'], ifo, veto_level, cluster), 'w')
-    print >>out, """<table>
+    print("""<table>
 <tr><th>Veto level</th><th>Analyzed Time (s)</th><th>Num. Triggers</th></tr>
 <tr><td>%d</td><td>%d</td><td>%d</td></tr>
-</table>""" % (veto_level, analyzed_time, num_triggers)
+</table>""" % (veto_level, analyzed_time, num_triggers), file=out)
     out.close()
 
 
@@ -727,34 +727,34 @@
                 seg_start = -1
                 seg_end   = -1
 
-    print >>out, "<h3>Times when trigger rate over 1 s interval exceeds 500 Hz</h3>"
-    print >>out, '<table>'
-    print >>out, '<tr><th>GPS Start</th><th>GPS End</th><th>Duration (sec)</th><th>Avg. rate (Hz)</th><th>UTC Start</th><th>UTC End</th></tr>'
+    print("<h3>Times when trigger rate over 1 s interval exceeds 500 Hz</h3>", file=out)
+    print('<table>', file=out)
+    print('<tr><th>GPS Start</th><th>GPS End</th><th>Duration (sec)</th><th>Avg. rate (Hz)</th><th>UTC Start</th><th>UTC End</th></tr>', file=out)
 
     for r in sorted(results, cmp = lambda x,y: cmp(y[2], x[2])):
         if r[2] > 500:
-            print >>out, '<tr><td>%d</td><td>%d</td><td>%d</td><td>%.0f</td><td>%s</td><td>%s</td></tr>' % (r[0], r[1], r[3], r[2], tconvert(r[0]), tconvert(r[1])) 
-
-    print >>out, '</table>'
-
-
-    print >>out, "<h3>Times when trigger rate exceedes 200 Hz for more than 10 seconds</h3>"
-    print >>out, '<table>'
-    print >>out, '<tr><th>GPS Start</th><th>GPS End</th><th>Duration (sec)</th><th>Avg. rate (Hz)</th><th>UTC Start</th><th>UTC End</th></tr>'
+            print('<tr><td>%d</td><td>%d</td><td>%d</td><td>%.0f</td><td>%s</td><td>%s</td></tr>' % (r[0], r[1], r[3], r[2], tconvert(r[0]), tconvert(r[1])), file=out) 
+
+    print('</table>', file=out)
+
+
+    print("<h3>Times when trigger rate exceedes 200 Hz for more than 10 seconds</h3>", file=out)
+    print('<table>', file=out)
+    print('<tr><th>GPS Start</th><th>GPS End</th><th>Duration (sec)</th><th>Avg. rate (Hz)</th><th>UTC Start</th><th>UTC End</th></tr>', file=out)
 
     long = sorted( [r for r in results if r[3] >= 10], cmp = lambda x, y: cmp(y[3], x[3]) )
     for r in long:
-        print >>out, '<tr><td>%d</td><td>%d</td><td>%d</td><td>%.0f</td><td>%s</td><td>%s</td></tr>' % (r[0], r[1], (r[1] - r[0]), r[2], tconvert(r[0]), tconvert(r[1])) 
-
-    print >>out, '</table>'
-
-    print >>out, '<a href="%s-%d_%s_glitchy_times.txt">All times with rates greater than 200 Hz</a>' % (ifo, veto_level, cluster)
+        print('<tr><td>%d</td><td>%d</td><td>%d</td><td>%.0f</td><td>%s</td><td>%s</td></tr>' % (r[0], r[1], (r[1] - r[0]), r[2], tconvert(r[0]), tconvert(r[1])), file=out) 
+
+    print('</table>', file=out)
+
+    print('<a href="%s-%d_%s_glitchy_times.txt">All times with rates greater than 200 Hz</a>' % (ifo, veto_level, cluster), file=out)
     out.close()
 
     out = open('%s/%s-%d_%s_glitchy_times.txt' % (config['out_dir'], ifo, veto_level, cluster), 'w')
-    print >>out, "# start time, end time, avg rate, duration"
+    print("# start time, end time, avg rate, duration", file=out)
     for r in results:
-        print >>out, "%d\t%d\t%.0f\t%d" % r
+        print("%d\t%d\t%.0f\t%d" % r, file=out)
     out.close()
 
 
@@ -784,22 +784,22 @@
     seglists       = [(y[0], segmentlist([segment(x[0],x[1]) for x in y[1]])) for y in vetoes]
 
     if veto_level == 1:
-        print >>out, "<h3>Efficiency of category 1 vetoes</h3>"
+        print("<h3>Efficiency of category 1 vetoes</h3>", file=out)
     else:
-        print >>out, "<h3>Efficiency of category %d vetoes on triggers that passed category %d</h3>" % (veto_level, veto_level - 1)
-
-    print >>out
+        print("<h3>Efficiency of category %d vetoes on triggers that passed category %d</h3>" % (veto_level, veto_level - 1), file=out)
+
+    print(file=out)
 
     if total_triggers == 0:
-        print >>out, "Note: no triggers at this veto level<p>"
+        print("Note: no triggers at this veto level<p>", file=out)
 
     atime = float(abs(analysis_time))
 
     if atime == 0.0:
-        print >>out, "Note: no analysis time at this veto level<p>"
-
-    print >>out, "<table>"
-    print >>out, "<tr><th>Name:Version</th><th>Efficiency (%)</th><th>Deadtime (%)</th><th>Efficiency / Deadtime</th></tr>"
+        print("Note: no analysis time at this veto level<p>", file=out)
+
+    print("<table>", file=out)
+    print("<tr><th>Name:Version</th><th>Efficiency (%)</th><th>Deadtime (%)</th><th>Efficiency / Deadtime</th></tr>", file=out)
 
     for seg in seglists:
         if len(seg[1]) == 0:
@@ -827,9 +827,9 @@
         else:
             ratio = 'NA'
 
-        print >>out, "<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>" % (seg[0], efficiency, dead_time, ratio)
-
-    print >>out, "</table>"
+        print("<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>" % (seg[0], efficiency, dead_time, ratio), file=out)
+
+    print("</table>", file=out)
 
     out.close()
 
@@ -840,15 +840,15 @@
 
     out = open('%s/%s_CAT%d_%s_Glitch.html' % (config['out_dir'], ifo, veto_level, cluster), 'w')
 
-    print >>out, '<html>'
-    print >>out, '<head>'
-    print >>out, '  <link media="all" href="../../auxfiles/ihope_daily_style.css" type="text/css" rel="stylesheet" />'
-    print >>out, '</head>'
-
-    print >>out, '<body>'
+    print('<html>', file=out)
+    print('<head>', file=out)
+    print('  <link media="all" href="../../auxfiles/ihope_daily_style.css" type="text/css" rel="stylesheet" />', file=out)
+    print('</head>', file=out)
+
+    print('<body>', file=out)
 
     if cluster != '16SEC_CLUSTERED':
-        print >>out, "<p><p><H3>The glitch page is only available for triggers that have been clustered with a 16-second window</H3>"
+        print("<p><p><H3>The glitch page is only available for triggers that have been clustered with a 16-second window</H3>", file=out)
     else:
         tmpfile = '%s/%s-%d_glitchout.html' % (config['tmp_dir'], ifo, veto_level)
 
@@ -871,10 +871,10 @@
 
         os.remove(tmpfile)
 
-        print >>out, glitch_text
-
-    print >>out, '</body>'
-    print >>out, '</html>'
+        print(glitch_text, file=out)
+
+    print('</body>', file=out)
+    print('</html>', file=out)
     out.close()
 
 
@@ -896,15 +896,15 @@
 
     if cluster != '16SEC_CLUSTERED':
         out = open('%s/%s_CAT%d_%s_Hwinj.html' % (config['out_dir'], ifo, veto_level, cluster), 'w')
-        print >>out, '<html>'
-        print >>out, '<head>'
-        print >>out, '  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />'
-        print >>out, '</head>'
-
-        print >>out, '<body>'
-        print >>out, "<p><p><H3>The hardware injection page is only available for triggers that have been clustered with a 16-second window</H3>"
-        print >>out, '</body>'
-        print >>out, '</html>'
+        print('<html>', file=out)
+        print('<head>', file=out)
+        print('  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />', file=out)
+        print('</head>', file=out)
+
+        print('<body>', file=out)
+        print("<p><p><H3>The hardware injection page is only available for triggers that have been clustered with a 16-second window</H3>", file=out)
+        print('</body>', file=out)
+        print('</html>', file=out)
         out.close()
     else:
         cmd  = '%s ' % config['ligolw_cbc_hardware_inj_page']
@@ -931,7 +931,7 @@
     template          = html_template % map
 
     out = open('%s/index.html' % config['out_dir'],'w')
-    print >>out, template
+    print(template, file=out)
     out.close()
 
     for ifo in ifos:
@@ -940,92 +940,92 @@
                 # Make the summary section
 
                 out = open('%s/%s_CAT%d_%s_Summary.html' % (config['out_dir'], ifo, cat, cluster), 'w')
-                print >>out, '<html>'
-                print >>out, '<head>'
-                print >>out, '  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />'
-                print >>out, '</head>'
-
-                print >>out, '<body>'
+                print('<html>', file=out)
+                print('<head>', file=out)
+                print('  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />', file=out)
+                print('</head>', file=out)
+
+                print('<body>', file=out)
 
                 for name in summary_names:
                     in_f = open(config['out_dir'] + '/' + name % (ifo, cat, cluster))
 
                     for l in in_f:
-                        print >>out, l
+                        print(l, file=out)
 
                     in_f.close()
 
-                print >>out, '</body>'
-                print >>out, '</html>'
+                print('</body>', file=out)
+                print('</html>', file=out)
                 out.close()
 
                 # Make the bank chisq page
                 out = open('%s/%s_CAT%d_%s_bankchisq.html' % (config['out_dir'], ifo, cat, cluster), 'w')
-                print >>out, '<html>'
-                print >>out, '<head>'
-                print >>out, '  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />'
-                print >>out, '</head>'
-                print >>out, '<body>'
+                print('<html>', file=out)
+                print('<head>', file=out)
+                print('  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />', file=out)
+                print('</head>', file=out)
+                print('<body>', file=out)
 
                 #imgs = sorted(glob.glob('%s/%s_%d_%s_bank_veto_dof_*.png' % (config['out_dir'], ifo, cat, cluster)))
                 #for i in imgs:
                 #    print >>out, '<img src="%s"><p>' % i.split('/')[-1]
-                print >>out, '<img src="%s_%d_%s_chisq.png"><p>' % (ifo, cat, cluster)
-                print >>out, '<img src="%s_%d_%s_bank_veto.png"><p>' % (ifo, cat, cluster)
-                print >>out, '<img src="%s_%d_%s_cont_veto.png"><p>' % (ifo, cat, cluster)
-
-                print >>out, '</body>'
-                print >>out, '</html>'
+                print('<img src="%s_%d_%s_chisq.png"><p>' % (ifo, cat, cluster), file=out)
+                print('<img src="%s_%d_%s_bank_veto.png"><p>' % (ifo, cat, cluster), file=out)
+                print('<img src="%s_%d_%s_cont_veto.png"><p>' % (ifo, cat, cluster), file=out)
+
+                print('</body>', file=out)
+                print('</html>', file=out)
                 out.close()
 
                 # Add the images
                 for name in image_names:
                     out = open('%s/%s_CAT%d_%s_%s.html' % (config['out_dir'], ifo, cat, cluster, name), 'w')
 
-                    print >>out, '<html>'
-                    print >>out, '<head>'
-                    print >>out, '  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />'
-                    print >>out, '</head>'
-                    print >>out, '<body>'
+                    print('<html>', file=out)
+                    print('<head>', file=out)
+                    print('  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />', file=out)
+                    print('</head>', file=out)
+                    print('<body>', file=out)
 
                     if name == 'glitchgram':
-                        print >>out, '<p><img src="%s_%d_%s_new_glitchgram.png"><p>' % (ifo, cat, cluster)
+                        print('<p><img src="%s_%d_%s_new_glitchgram.png"><p>' % (ifo, cat, cluster), file=out)
 
                     if name == 'snr_hist':
-                        print >>out, '<p><img src="%s_%d_%s_new_snr_hist.png"><p>' % (ifo, cat, cluster)
-                        print >>out, '<p><img src="%s_%d_%s_%s_all.png">' % (ifo, cat, cluster, name)
-
-                    print >>out, '<img src="%s_%d_%s_%s.png">' % (ifo, cat, cluster, name)
+                        print('<p><img src="%s_%d_%s_new_snr_hist.png"><p>' % (ifo, cat, cluster), file=out)
+                        print('<p><img src="%s_%d_%s_%s_all.png">' % (ifo, cat, cluster, name), file=out)
+
+                    print('<img src="%s_%d_%s_%s.png">' % (ifo, cat, cluster, name), file=out)
 
                     if name == 'rate_vs_time':
-                        print >>out, '<img src="%s_%d_%s_newsnr_vs_time.png"><p>' % (ifo, cat, cluster)
-                        print >>out, '<img src="%s_%d_%s_snr_vs_time.png">' % (ifo, cat, cluster)
+                        print('<img src="%s_%d_%s_newsnr_vs_time.png"><p>' % (ifo, cat, cluster), file=out)
+                        print('<img src="%s_%d_%s_snr_vs_time.png">' % (ifo, cat, cluster), file=out)
 
                         in_f = open('%s/%s-%d_%s_glitchy_times_table.html' % (config['tmp_dir'], ifo, cat, cluster))
                         for l in in_f:
-                            print >>out, l, 
+                            print(l, end=' ', file=out) 
                         in_f.close()
 
-                    print >>out, '</body>'
-                    print >>out, '</html>'
+                    print('</body>', file=out)
+                    print('</html>', file=out)
                     out.close()
 
                 # And the template page
                 out = open('%s/%s_CAT%d_%s_template.html' % (config['out_dir'], ifo, cat, cluster), 'w')
-                print >>out, '<html>'
-                print >>out, '<head>'
-                print >>out, '  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />'
-                print >>out, '</head>'
-
-                print >>out, '<body>'
-
-                print >>out, '<img src="%s_%d_%s_mass_hist.png">' % (ifo, cat, cluster)
-                print >>out, '<img src="%s_%d_%s_tmpl_hist.png">' % (ifo, cat, cluster)
-                print >>out, '<img src="%s_%d_%s_mass_hist_norm.png">' % (ifo, cat, cluster)
-                print >>out, '<img src="%s_%d_%s_template_counts.png">' % (ifo, cat, cluster)
-                print >>out, '<img src="%s_%d_%s_hexmass.png">' % (ifo, cat, cluster)
-                print >>out, '</body>'
-                print >>out, '</html>'
+                print('<html>', file=out)
+                print('<head>', file=out)
+                print('  <link media="all" href="ihope_daily_style.css" type="text/css" rel="stylesheet" />', file=out)
+                print('</head>', file=out)
+
+                print('<body>', file=out)
+
+                print('<img src="%s_%d_%s_mass_hist.png">' % (ifo, cat, cluster), file=out)
+                print('<img src="%s_%d_%s_tmpl_hist.png">' % (ifo, cat, cluster), file=out)
+                print('<img src="%s_%d_%s_mass_hist_norm.png">' % (ifo, cat, cluster), file=out)
+                print('<img src="%s_%d_%s_template_counts.png">' % (ifo, cat, cluster), file=out)
+                print('<img src="%s_%d_%s_hexmass.png">' % (ifo, cat, cluster), file=out)
+                print('</body>', file=out)
+                print('</html>', file=out)
                 out.close()
 
     # Copy asset files
@@ -1080,7 +1080,7 @@
         pylab.text(time[maxindex], snr[maxindex]*1.04, 'Max: GPS %.3f' % (gps[maxindex]+1e-9*gpsns[maxindex]), size='medium')
         if len(infTimes):
             pylab.text(0.5, max(snr)*1.07, 'One or more SNRs were inf!! See .err file for a list of times')
-            print >>sys.stderr, 'Times with SNR=inf:', infTimes
+            print('Times with SNR=inf:', infTimes, file=sys.stderr)
 
     pylab.title(make_timeless_title(ifo, veto_level, cluster, start_time))
     pylab.xlabel(make_time_label(start_time))
@@ -1122,7 +1122,7 @@
         count -= 1
 
     if not os.path.exists(filename):
-        print >>sys.stderr, "Needed file %s not found, aborting" % filename
+        print("Needed file %s not found, aborting" % filename, file=sys.stderr)
         sys.exit(1)
 
     # wait for it to stabalize
@@ -1409,7 +1409,7 @@
         non_gaussianity = gtotal / total_count
 
         f_out = open('%s/%s-%d-%s_nongaussianity.txt' % (config['out_dir'], ifo, veto_level, cluster), 'w')
-        print >>f_out, non_gaussianity
+        print(non_gaussianity, file=f_out)
         f_out.close()
 
         # Plot up to 200, with a cumulative dot showing the remaining
@@ -1481,7 +1481,7 @@
             try:
                 bins.append(1e-40)
             except:
-                print "Unable to allocate %f" % newsnr
+                print("Unable to allocate %f" % newsnr)
 
         bins[newsnr] += 1
 
@@ -1722,7 +1722,7 @@
 
     for r in ranges:
         subset = pylab.logical_and(snr>=r[0], snr<r[1])
-        print 'Plotting trigs for snr between', r[0], r[1]
+        print('Plotting trigs for snr between', r[0], r[1])
         if len(time[subset]):
             pylab.scatter(time[subset], duration[subset], edgecolor='none', c=r[2], s=r[3])
     snrmax_index = pylab.argmax(snr)
@@ -1749,7 +1749,7 @@
 
     for r in ranges:
         subset = pylab.logical_and(newsnr>=r[0], newsnr<r[1])
-        print 'Plotting trigs for newsnr between', r[0], r[1]
+        print('Plotting trigs for newsnr between', r[0], r[1])
         if len(time[subset]):
             pylab.scatter(time[subset], duration[subset], edgecolor='none', c=r[2], s=r[3])
     newsnrmax_index = pylab.argmax(newsnr)
--- ./src/inspiral/lalapps_ihope_status.py	(original)
+++ ./src/inspiral/lalapps_ihope_status.py	(refactored)
@@ -119,7 +119,7 @@
           totals[7] += 1 
 
   if output != 0:
-    print output[:-1]
+    print(output[:-1])
 
   return dag_status
 
@@ -140,7 +140,7 @@
     by itself, and in such case we search for the .dagman.out file.
 """
 
-print 'Parsing ' + options.dag_file + '...'
+print('Parsing ' + options.dag_file + '...')
 sleep(1.5)
 
 # Look for sub dags
@@ -168,15 +168,15 @@
 dagFile.close()
 
 if len(filenames)==0:
-  print 'No dag files found in ' + options.dag_file
-  print 'Assuming that ' + options.dag_file + ' is the dag file you want to parse'
+  print('No dag files found in ' + options.dag_file)
+  print('Assuming that ' + options.dag_file + ' is the dag file you want to parse')
   sub_dags = 0
 else:
   sub_dags = len( filenames )
-  print 'Found', str( sub_dags ), 'subdags'
+  print('Found', str( sub_dags ), 'subdags')
 done_dags = 0  
 
-print 'Parsing the dag files for status...\n'
+print('Parsing the dag files for status...\n')
 ### we found some dag files hopefully
 
 sleep(1.5)
@@ -186,39 +186,39 @@
 for i in xrange(0,len(filenames), 1):
   filename = filenames[i]
 
-  print '-------------------------------------------------------------------------'
+  print('-------------------------------------------------------------------------')
   # How many jobs are completed/failed ?
-  print "Parsing " + filenames[i]
+  print("Parsing " + filenames[i])
 
   status = get_status(filename, totals)
   
   if status==1: 
     done_dags += 1 
-    print "COMPLETE :)"
+    print("COMPLETE :)")
   elif status==0:
-    print "incomplete :("
+    print("incomplete :(")
   elif status==-1:
-   print "dag not yet started!"
+   print("dag not yet started!")
   elif status==-2:
-   print "dag pending and not yet planned!"
+   print("dag pending and not yet planned!")
 
 
 # Print totals
-print "  --------------------------- "
-print "      Done    =", totals[0]
+print("  --------------------------- ")
+print("      Done    =", totals[0])
 #print " Pre     =", totals[1]
-print "      Queued  =", totals[2]
+print("      Queued  =", totals[2])
 #print " Post    =", totals[3]
-print "      Ready   =", totals[4]
-print "      Unready =", totals[5]
-print "      Failed  =", totals[6]
-print "  ----------------------------------- "
-print "      Completed Jobs = " + str( totals[0] )
-print "      Submitted Jobs = " + str( sum( totals[:7] ) )
-print "      Total Jobs     = " + str( sum(totals[:]) )
+print("      Ready   =", totals[4])
+print("      Unready =", totals[5])
+print("      Failed  =", totals[6])
+print("  ----------------------------------- ")
+print("      Completed Jobs = " + str( totals[0] ))
+print("      Submitted Jobs = " + str( sum( totals[:7] ) ))
+print("      Total Jobs     = " + str( sum(totals[:]) ))
 if sub_dags != 0:
-  print "      Sub-dags       =", str( done_dags ) + "/" + str( sub_dags )
-print "  ----------------------------------- "
+  print("      Sub-dags       =", str( done_dags ) + "/" + str( sub_dags ))
+print("  ----------------------------------- ")
 
 # Confirm if completed
 dagFile = open(options.dag_file + '.dagman.out', 'r')
@@ -228,10 +228,10 @@
   if "EXITING WITH STATUS 0" in line:
     uber = 1
 if uber == 1:
-  print "  ihope status... COMPLETED!"
+  print("  ihope status... COMPLETED!")
 else:
-  print "  ihope status... incomplete."
-print ""
+  print("  ihope status... incomplete.")
+print("")
   
 
 
--- ./src/inspiral/lalapps_inspinjfind.py	(original)
+++ ./src/inspiral/lalapps_inspinjfind.py	(refactored)
@@ -146,7 +146,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d:" % (n, len(filenames)),
+		print("%d/%d:" % (n, len(filenames)), end=' ', file=sys.stderr)
 	xmldoc = ligolw_utils.load_filename(filename, contenthandler = inspinjfind.LIGOLWContentHandler, verbose = options.verbose)
 
 	#
@@ -162,13 +162,13 @@
 
 		if ligolw_process.doc_includes_process(xmldoc, process_program_name):
 			if options.verbose:
-				print >>sys.stderr, "warning: %s already processed," % (filename or "stdin"),
+				print("warning: %s already processed," % (filename or "stdin"), end=' ', file=sys.stderr)
 			if not options.force:
 				if options.verbose:
-					print >>sys.stderr, "skipping (use --force to force)"
+					print("skipping (use --force to force)", file=sys.stderr)
 				continue
 			if options.verbose:
-				print >>sys.stderr, "continuing by --force"
+				print("continuing by --force", file=sys.stderr)
 
 		#
 		# add process metadata to document
--- ./src/inspiral/lalapps_inspiral_hipe.py	(original)
+++ ./src/inspiral/lalapps_inspiral_hipe.py	(refactored)
@@ -341,7 +341,7 @@
   try:
     tc = open('tc.data','w')
   except:
-    print >> sys.stderr, "Cannot open transformation catalog for writing"
+    print("Cannot open transformation catalog for writing", file=sys.stderr)
     sys.exit(1)
 
   # write a line to the transformation catalog for each executable
@@ -457,7 +457,7 @@
   if ifo_name == 'G1':
     try: GeoBank = cp.get('input','geo-bank')
     except: GeoBank = None
-    if GeoBank: print "For G1 we use bank ", GeoBank
+    if GeoBank: print("For G1 we use bank ", GeoBank)
 
   data_opts, type, channel = get_data_options(cp,ifo_name)
 
@@ -473,7 +473,7 @@
   # see if we are using calibrated data
   if cp.has_section(data_opts) and cp.has_option(data_opts,'calibrated-data'):
     calibrated = True
-    print "we use calibrated data for ", ifo_name 
+    print("we use calibrated data for ", ifo_name) 
   else: calibrated = False
 
   # prepare the exttrig injection filename
@@ -1805,55 +1805,55 @@
 #################################
 # if --version flagged
 if opts.version:
-  print "$Id$"
+  print("$Id$")
   sys.exit(0)
 
 #################################
 # Sanity check of input arguments
 if not opts.config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.g1_data and not opts.h1_data and not opts.h2_data and \
     not opts.l1_data and not opts.v1_data and not opts.analyze_all:
-  print >> sys.stderr, "No ifos specified.  Please specify at least one of"
-  print >> sys.stderr, "--g1-data, --h1-data, --h2-data, --l1-data, --v1-data"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
+  print("No ifos specified.  Please specify at least one of", file=sys.stderr)
+  print("--g1-data, --h1-data, --h2-data, --l1-data, --v1-data", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos all data", file=sys.stderr)
   sys.exit(1)
 elif opts.analyze_all:
-  print >> sys.stderr, "The --analyze-all flag is currently not available."
-  print >> sys.stderr, "The code supports quadruple coincidence, so you can"
-  print >> sys.stderr, "choose at most four instruments to analyze."
+  print("The --analyze-all flag is currently not available.", file=sys.stderr)
+  print("The code supports quadruple coincidence, so you can", file=sys.stderr)
+  print("choose at most four instruments to analyze.", file=sys.stderr)
   sys.exit(1)
 
 if opts.g1_data and opts.h1_data and opts.h2_data and opts.l1_data \
     and opts.v1_data:
-  print >> sys.stderr, "Too many IFOs specified. " \
-      "Please choose up to four IFOs, but not five."
+  print("Too many IFOs specified. " \
+      "Please choose up to four IFOs, but not five.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.one_ifo and not opts.two_ifo and not opts.three_ifo and \
     not opts.four_ifo and not opts.analyze_all:
-  print >> sys.stderr, "No number of ifos given. Please specify at least one of"
-  print >> sys.stderr, "--one-ifo, --two-ifo, --three-ifo, --four-ifo"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
+  print("No number of ifos given. Please specify at least one of", file=sys.stderr)
+  print("--one-ifo, --two-ifo, --three-ifo, --four-ifo", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos all data", file=sys.stderr)
   sys.exit(1)
 elif opts.analyze_all:
-  print >> sys.stderr, "The --analyze-all flag can not be used to specify the"
-  print >> sys.stderr, "number of ifos to analyze. The code supports quadruple"
-  print >> sys.stderr, "coincidence, so you can choose at most four instruments"
-  print >> sys.stderr, "to analyze."
+  print("The --analyze-all flag can not be used to specify the", file=sys.stderr)
+  print("number of ifos to analyze. The code supports quadruple", file=sys.stderr)
+  print("coincidence, so you can choose at most four instruments", file=sys.stderr)
+  print("to analyze.", file=sys.stderr)
   sys.exit(1)
 
 if opts.inspiral_veto and opts.td_follow_inspiral:
-  print >> sys.stderr, "Please specify only one of"
-  print >> sys.stderr, "--inspiral-veto, --td-follow-inspiral."
+  print("Please specify only one of", file=sys.stderr)
+  print("--inspiral-veto, --td-follow-inspiral.", file=sys.stderr)
  
 if not opts.datafind and not opts.template_bank and not opts.write_script and \
      not opts.inspiral and not opts.sire_inspiral and \
@@ -1865,14 +1865,14 @@
      not opts.coherent_bank and not opts.coherent_inspiral and \
      not opts.summary_inspiral_triggers and not opts.summary_coinc_triggers and \
      not opts.cohire and not opts.summary_coherent_inspiral_triggers:
-  print >> sys.stderr, """  No steps of the pipeline specified.
+  print("""  No steps of the pipeline specified.
   Please specify at least one of
   --datafind, --template-bank, --inspiral, --sire-inspiral, --coincidence,
   --coire-coincidence, --trigbank, --inspiral-veto, --sire-inspiral-veto,
   --td-follow-bank, --td-follow-inspiral, --second-coinc,
   --coire-second-coinc, --sire-second-coinc, --coherent-bank,
   --coherent-inspiral, --summary-inspiral-triggers --summary-coinc-triggers,
-  --cohire, --summary-coherent-inspiral-triggers"""
+  --cohire, --summary-coherent-inspiral-triggers""", file=sys.stderr)
   sys.exit(1)
 
 ifo_list = ['H1','H2','L1','V1','G1']
@@ -2039,11 +2039,11 @@
 
   # check the values given
   if startExttrig < 1:
-    print >> sys.stderr, "exttrig-inj-start must be larger than 0."
+    print("exttrig-inj-start must be larger than 0.", file=sys.stderr)
     sys.exit(1)
   if startExttrig > stopExttrig:
-    print >> sys.stderr, "exttrig-inj-stop must be larger than "\
-                         "exttrig-inj-start."
+    print("exttrig-inj-stop must be larger than "\
+                         "exttrig-inj-start.", file=sys.stderr)
     sys.exit(1)
 else:
   exttrigInjections=[0,0]
@@ -2375,7 +2375,7 @@
     job.add_opt('data-type','all_data')
 
 else:
-  print "Invalid playground data mask " + play_data_mask + " specified"
+  print("Invalid playground data mask " + play_data_mask + " specified")
   sys.exit(1)
 
  
@@ -2442,7 +2442,7 @@
 #   Step 1: read science segs that are greater or equal to a chunk 
 #   from the input file
 
-print "reading in single ifo science segments and creating master chunks...",
+print("reading in single ifo science segments and creating master chunks...", end=' ')
 sys.stdout.flush()
 
 segments = {}
@@ -2461,7 +2461,7 @@
     data[ifo].make_chunks_from_unused(length,overlap/2,playground_only,
         0,0,overlap/2,pad)
 
-print "done"
+print("done")
 
 # work out the earliest and latest times that are being analyzed
 if not gps_start_time:
@@ -2469,8 +2469,8 @@
   for ifo in ifo_list:
     if data[ifo] and (data[ifo][0].start() < gps_start_time):
       gps_start_time = data[ifo][0].start()
-  print "GPS start time not specified, obtained from segment lists as " + \
-    str(gps_start_time)
+  print("GPS start time not specified, obtained from segment lists as " + \
+    str(gps_start_time))
 
 
 if not gps_end_time:
@@ -2478,8 +2478,8 @@
   for ifo in ifo_list:
     if data[ifo] and (data[ifo][-1].end() > gps_end_time):
       gps_end_time = data[ifo][0].end()
-  print "GPS end time not specified, obtained from segment lists as " + \
-    str(gps_end_time)
+  print("GPS end time not specified, obtained from segment lists as " + \
+    str(gps_end_time))
 
 ##############################################################################
 #   Step 2: determine analyzable times
@@ -2546,7 +2546,7 @@
   inspinj.set_end(gps_end_time)
   inspinj.set_seed(0)
   inj_file = inspinj.get_output()
-  print inj_file
+  print(inj_file)
   shutil.copy( inj_file_loc, inj_file)
   inspinj = None
 elif seed:
@@ -2601,7 +2601,7 @@
 prev_df = None
 
 for ifo in ifo_list:
-  print "setting up jobs to filter " + ifo + " data...",
+  print("setting up jobs to filter " + ifo + " data...", end=' ')
   sys.stdout.flush()
 
   (prev_df,chunks_analyzed[ifo]) = analyze_ifo(ifo,data[ifo],data_to_do[ifo],  
@@ -2620,14 +2620,14 @@
         ifotag="SUMMARY_FIRST", usertag = usertag, 
         inspinjNode=inspinj)
 
-  print "done" 
+  print("done") 
 
 
 ##############################################################################
 # Step 5: Run inca in single ifo mode on the single ifo triggers.
 
 if not opts.ringdown:
-  print "setting up jobs to inca single IFO data...",
+  print("setting up jobs to inca single IFO data...", end=' ')
   sys.stdout.flush()
 
   single_coinc_nodes = {}
@@ -2641,7 +2641,7 @@
         opts.coire_coincidence, opts.coincidence, ifo,
         inj_file=inj_file, usertag=usertag, ifotag="FIRST", inspinjNode = inspinj)
 
-  print "done"
+  print("done")
   
  
 ##############################################################################
@@ -2651,7 +2651,7 @@
 coinc_slide_nodes = {}
 
 for ifos in ifo_coincs:
-  print "setting up thinca jobs on " + ifos + " data...",
+  print("setting up thinca jobs on " + ifos + " data...", end=' ')
   sys.stdout.flush()
 
   if cp.has_section('thinca-1'): thinca_jobs[ifos].add_ini_opts(cp, 'thinca-1')
@@ -2710,7 +2710,7 @@
   # Concatenate the zerolag and slide nodes
   coinc_nodes[ifos] = coinc_nodes[ifos] + coinc_slide_nodes[ifos]
   
-  print "done"
+  print("done")
 
 
 ##############################################################################
@@ -2723,7 +2723,7 @@
     approximants = cp.get('veto-inspiral', 'approximant')
     cp.remove_option('veto-inspiral', 'approximant')
   except ConfigParser.NoOptionError:
-    print "No approximant in veto-inspiral - using the main one..."
+    print("No approximant in veto-inspiral - using the main one...")
     approximants = cp.get('inspiral', 'approximant')
 
   approximants = approximants.split(',')
@@ -2747,8 +2747,8 @@
  
   for ifo in ifo_list:
     if ifo in ifos:
-      print "setting up jobs to filter " + ifo + \
-        " data with coinc trigs from " + ifos + " times ...",
+      print("setting up jobs to filter " + ifo + \
+        " data with coinc trigs from " + ifos + " times ...", end=' ')
       sys.stdout.flush()
 
       if opts.td_follow_bank or opts.td_follow_inspiral:
@@ -2769,7 +2769,7 @@
               inj_file = inj_file, usertag = usertag, 
               ifotag="SNGL_SECOND_" + ifo, inspinjNode = inspinj)
 
-      print "done"
+      print("done")
 
 
 ##############################################################################
@@ -2781,7 +2781,7 @@
 coire2_slide_nodes = {}
 
 for ifos in ifo_coincs:
-  print "setting up second thinca jobs on " + ifos + " data...",
+  print("setting up second thinca jobs on " + ifos + " data...", end=' ')
   sys.stdout.flush()
 
   if cp.has_section('thinca-2'):
@@ -2884,7 +2884,7 @@
                   ifotag="SUMMARY_SECOND_" + ifos, usertag = usertag)
 
 
-  print "done"
+  print("done")
 
 ##############################################################################
 # Step 9: Create coherent (template) banks:
@@ -2894,7 +2894,7 @@
   if num_slides: cohbank_slide_nodes = {}
 
   for ifos in ifo_coincs:
-    print "setting up coherent-bank jobs on " + ifos + " data...",
+    print("setting up coherent-bank jobs on " + ifos + " data...", end=' ')
     sys.stdout.flush()
 
     if cp.has_section('thinca-2') :
@@ -2917,7 +2917,7 @@
       cohbank_nodes[ifos] = coherent_bank(ifos,cb_job,dag,coinc2_nodes[ifos],
           exttrigInjections,0,0,usertag=usertag)
 
-    print "done"
+    print("done")
 
 ##############################################################################
 # Step 10: Construct single-ifo CData frame files for the coherent analyses:
@@ -2935,8 +2935,8 @@
       if ifo in ifos:
         data_opts[ifo] = {}
 
-        print "setting up jobs to filter " + ifo + \
-          " data with cohbank templates from " + ifos + " times ...",
+        print("setting up jobs to filter " + ifo + \
+          " data with cohbank templates from " + ifos + " times ...", end=' ')
         sys.stdout.flush()
 
         # add ifo specific options
@@ -2983,7 +2983,7 @@
 	    data[ifo],ifo,cohbank_slide_nodes[ifos],ifos,trig_coh_jobs[ifo],
 	    insp_coh_jobs[ifo],dag,calibrated,exttrigInjections,1,usertag,inspinj)
 
-        print "done"
+        print("done")
 
 ##############################################################################
 # Step 11: Create network trigger banks with inspiral-coherent triggers:
@@ -2998,7 +2998,7 @@
     cohinspbank_job.add_ini_opts(cp, 'cohinspbank')
 
   for ifos in ifo_coincs:
-    print "setting up network-trigger-bank jobs on " + ifos + " data...",
+    print("setting up network-trigger-bank jobs on " + ifos + " data...", end=' ')
     sys.stdout.flush()
 
     cohinspbank_nodes[ifos] = coherent_inspiral_bank(ifos,cohinspbank_job,dag,
@@ -3018,7 +3018,7 @@
         analyzed_trig_coh_slide_data[ifos],exttrigInjections,
         num_slides,1,usertag=usertag)
 
-    print "done"
+    print("done")
 
 ##############################################################################
 # Step 12: Do the coherent analyses:
@@ -3040,7 +3040,7 @@
       cohire_slide_summary_job.add_ini_opts(cp, 'cohire')
 
   for ifos in ifo_coincs:
-    print "setting up coherent-analysis jobs on " + ifos + " data...",
+    print("setting up coherent-analysis jobs on " + ifos + " data...", end=' ')
     sys.stdout.flush()
 
     if cp.has_section('data') and not opts.ringdown:
@@ -3087,13 +3087,13 @@
              num_slides, inj_file=inj_file, ifotag="SUMMARY_COHERENT_" + ifos,
              usertag=usertag,inspinjNode=inspinj)
 
-    print "done"
+    print("done")
 
 ##############################################################################
 # Step 13: Write out the LAL cache files for the various output data
 
 if gps_start_time is not None and gps_end_time is not None:
-  print "generating cache files for output data products...",
+  print("generating cache files for output data products...", end=' ')
   cache_fname = ''
   for ifo in ifo_analyze: 
     cache_fname += ifo
@@ -3117,9 +3117,9 @@
       output_data_cache.append(lal.Cache.from_urls([node.get_missed()])[0])
 
   output_data_cache.tofile(open(cache_fname, "w"))
-  print "done"
+  print("done")
 else:
-  print "gps start and stop times not specified: cache files not generated"
+  print("gps start and stop times not specified: cache files not generated")
 
 
 ##############################################################################
@@ -3146,16 +3146,16 @@
 # write a message telling the user that the DAG has been written
 if opts.dax:
   
-  print "\nCreated an abstract DAX file", dag.get_dag_file()
-  print "which can be transformed into a concrete DAG with gencdag."
-  print "\nSee the documentation on http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/pegasus_lsc.html"
+  print("\nCreated an abstract DAX file", dag.get_dag_file())
+  print("which can be transformed into a concrete DAG with gencdag.")
+  print("\nSee the documentation on http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/pegasus_lsc.html")
 
 
 
 else:
-  print "\nCreated a DAG file which can be submitted by executing"
-  print "\n   condor_submit_dag", dag.get_dag_file()
-  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
+  print("\nCreated a DAG file which can be submitted by executing")
+  print("\n   condor_submit_dag", dag.get_dag_file())
+  print("""\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
   If you are running LSCdataFind jobs, do not forget to initialize your grid 
   proxy certificate on the condor submit machine by running the commands
   
@@ -3178,7 +3178,7 @@
   
   Contact the administrator of your cluster to find the hostname and port of the
   LSCdataFind server.
-  """
+  """)
 
 ##############################################################################
 # write out a log file for this script
@@ -3202,38 +3202,38 @@
 log_fh.write( cp.get('pipeline','version') + "\n" )
 log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )
 
-print >> log_fh, "\n===========================================\n"
-print >> log_fh, "Science Segments and master chunks:\n"
+print("\n===========================================\n", file=log_fh)
+print("Science Segments and master chunks:\n", file=log_fh)
 
 for ifo in ifo_list:
-  print >> log_fh, "\n===========================================\n"
-  print >> log_fh, ifo + "Data\n"
+  print("\n===========================================\n", file=log_fh)
+  print(ifo + "Data\n", file=log_fh)
   for seg in data[ifo]:
-    print >> log_fh, " ", seg
+    print(" ", seg, file=log_fh)
     for chunk in seg:
-      print >> log_fh, "   ", chunk
+      print("   ", chunk, file=log_fh)
 
 
 for ifo in ifo_list:
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( 
     "Filtering " + str(len(chunks_analyzed[ifo])) + " " + ifo + \
     " master chunks\n" )
   total_time = 0
   for ifo_done in chunks_analyzed[ifo]:
-    print >> log_fh, ifo_done.get_chunk()
+    print(ifo_done.get_chunk(), file=log_fh)
     total_time += len(ifo_done.get_chunk())
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
 for ifo in ifo_list:
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( "Writing " + str(len(analyzed_data[ifo])) + " " + ifo + \
     " single IFO science segments\n" )
   total_time = 0
   for seg in analyzed_data[ifo]:
-    print >> log_fh, seg
+    print(seg, file=log_fh)
     total_time += seg.dur()
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
   if opts.output_segs and len(analyzed_data[ifo]):
     if playground_only:
@@ -3247,14 +3247,14 @@
 
 
 for ifos in ifo_coincs:  
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( "Writing " + str(len(analyzed_data[ifos])) + " " + ifos + \
     " coincident segments\n" )
   total_time = 0
   for seg in analyzed_data[ifos]:
-    print >> log_fh, seg
+    print(seg, file=log_fh)
     total_time += seg.dur()
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
   if opts.output_segs and len(analyzed_data[ifos]):
     if playground_only:
--- ./src/inspiral/lalapps_inspiral_online_pipe.py	(original)
+++ ./src/inspiral/lalapps_inspiral_online_pipe.py	(refactored)
@@ -119,39 +119,39 @@
 
 
 def get_valid_segments(segment_url, base_dir, ifo, science_flag, start_time, end_time):
-    print "Finding valid analysis times for %s, please hold..." % ifo
+    print("Finding valid analysis times for %s, please hold..." % ifo)
 
     cmd  = 'ligolw_segment_query --query-segments --segment-url %s --include-segments %s --gps-start-time %d --gps-end-time %d | ligolw_print -t segment -c start_time -c end_time' % (segment_url, science_flag, start_time, end_time)
     pipe = os.popen(cmd)
 
-    print cmd
+    print(cmd)
 
     results   = [x.strip().split(',') for x in pipe]
     science   = segments.segmentlist([segments.segment(int(x[0]), int(x[1])) for x in results])
     science.coalesce()
 
-    print "Science: "
+    print("Science: ")
     for s in science:
-       print s[0], s[1]
+       print(s[0], s[1])
 
     framedir  = base_dir + '/' + ifo[0] + '1'
     chunks    = [f.split('.')[0].split('-') for f in get_all_files_in_range(framedir, start_time, end_time)]
     available = segments.segmentlist([ segments.segment( int(x[-2]), int(x[-2]) + int(x[-1]) ) for x in chunks if len(x) == 6 ])
     available.coalesce()
 
-    print "Available:"
+    print("Available:")
     for s in available:
-       print s[0], s[1]
+       print(s[0], s[1])
 
     result = science & available
 
     result.coalesce()
 
-    print "Result:"
+    print("Result:")
     for s in result:
-       print s[0], s[1]
-
-    print "done."
+       print(s[0], s[1])
+
+    print("done.")
 
     return result
 
@@ -176,7 +176,7 @@
   -l, --log-path PATH            directory to write condor log file
   -p, --coh-ptf                 Use coh_PTF_inspiral.c instead of inspiral.c
 """
-  print >> sys.stderr, msg
+  print(msg, file=sys.stderr)
 
 # pasrse the command line options to figure out what we should do
 shortop = "hvs:e:a:b:f:t:c:l:p:"
@@ -212,7 +212,7 @@
 
 for o, a in opts:
   if o in ("-v", "--version"):
-    print git_version.verbose_msg
+    print(git_version.verbose_msg)
     sys.exit(0)
   elif o in ("-h", "--help"):
     usage()
@@ -236,38 +236,38 @@
   elif o in ("-p", "--coh-ptf"):
     doCohPTF = True
   else:
-    print >> sys.stderr, "Unknown option:", o
+    print("Unknown option:", o, file=sys.stderr)
     usage()
     sys.exit(1)
 
 if not gps_start_time:
-  print >> sys.stderr, "No GPS start time specified."
-  print >> sys.stderr, "Use --gps-start-time SEC to specify start time."
+  print("No GPS start time specified.", file=sys.stderr)
+  print("Use --gps-start-time SEC to specify start time.", file=sys.stderr)
   sys.exit(1)
 
 if not gps_end_time:
-  print >> sys.stderr, "No GPS end time specified."
-  print >> sys.stderr, "Use --gps-end-time SEC to specify end time."
+  print("No GPS end time specified.", file=sys.stderr)
+  print("Use --gps-end-time SEC to specify end time.", file=sys.stderr)
   sys.exit(1)
 
 if not dag_file_name:
-  print >> sys.stderr, "No DAG file name specified."
-  print >> sys.stderr, "Use --dag-file-name NAME to specify output DAG file."
+  print("No DAG file name specified.", file=sys.stderr)
+  print("Use --dag-file-name NAME to specify output DAG file.", file=sys.stderr)
   sys.exit(1)
 
 if not aux_data_path:
-  print >> sys.stderr, "No auxiliary data path specified."
-  print >> sys.stderr, "Use --aux-data-path PATH to specify directory."
+  print("No auxiliary data path specified.", file=sys.stderr)
+  print("Use --aux-data-path PATH to specify directory.", file=sys.stderr)
   sys.exit(1)
 
 if not config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 try: os.mkdir('cache')
@@ -309,7 +309,7 @@
     available_segments = get_valid_segments(cp.get('segfind','segment-url'), cp.get('framefind','base-dir'), ifo, cp.get('segments',ifo.lower() + '-analyze'), gps_start_time, gps_end_time)
 
     if not available_segments:
-        print "No available segments for %s, skipping" % ifo
+        print("No available segments for %s, skipping" % ifo)
         continue
     
     
@@ -529,8 +529,8 @@
                 [instrument, type, start, duration, extension, gz] = tmp
                 extension = extension + "." + gz
 
-            print >> insp_cache_file, instrument[0], type, start, duration, \
-                     os.path.join(path , file)
+            print(instrument[0], type, start, duration, \
+                     os.path.join(path , file), file=insp_cache_file)
 
             lwadd.add_parent(insp)
 
@@ -590,7 +590,7 @@
         a = a.replace('macrotype="ER_C00_L1"','macrotype="L1_ER_C00_L1"')
         a = a.replace('macrotype="DMT_C00_L2"','macrotype="L1_DMT_C00_L2"')
 
-    print >>out_f, a,
+    print(a, end=' ', file=out_f)
 
 in_f.close()
 out_f.close()
--- ./src/inspiral/lalapps_inspiral_online_pipe_inject.py	(original)
+++ ./src/inspiral/lalapps_inspiral_online_pipe_inject.py	(refactored)
@@ -33,7 +33,7 @@
   -f, --config-file FILE   use configuration file FILE
   -l, --log-path PATH      directory to write condor log file
 """
-  print >> sys.stderr, msg
+  print(msg, file=sys.stderr)
 
 # pasrse the command line options to figure out what we should do
 shortop = "hvf:l:"
@@ -56,7 +56,7 @@
 
 for o, a in opts:
   if o in ("-v", "--version"):
-    print "$Id$"
+    print("$Id$")
     sys.exit(0)
   elif o in ("-h", "--help"):
     usage()
@@ -66,18 +66,18 @@
   elif o in ("-l", "--log-path"):
     log_path = a
   else:
-    print >> sys.stderr, "Unknown option:", o
+    print("Unknown option:", o, file=sys.stderr)
     usage()
     sys.exit(1)
 
 if not config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 try: os.mkdir('cache')
@@ -272,13 +272,13 @@
 for seg in data:
   for chunk in seg:
     total_data += len(chunk)
-print >> log_fh, "total data =", total_data
-
-print >> log_fh, "\n===========================================\n"
-print >> log_fh, data
+print("total data =", total_data, file=log_fh)
+
+print("\n===========================================\n", file=log_fh)
+print(data, file=log_fh)
 for seg in data:
-  print >> log_fh, seg
+  print(seg, file=log_fh)
   for chunk in seg:
-    print >> log_fh, chunk
+    print(chunk, file=log_fh)
 
 sys.exit(0)
--- ./src/inspiral/lalapps_inspiral_pipe.py	(original)
+++ ./src/inspiral/lalapps_inspiral_pipe.py	(refactored)
@@ -120,7 +120,7 @@
 
   -x, --dax                write abstract DAX file
 """
-  print >> sys.stderr, msg
+  print(msg, file=sys.stderr)
 
 # pasrse the command line options to figure out what we should do
 shortop = "hvdtiTICpj:u:P:f:l:x"
@@ -165,7 +165,7 @@
 
 for o, a in opts:
   if o in ("-v", "--version"):
-    print "$Id$"
+    print("$Id$")
     sys.exit(0)
   elif o in ("-h", "--help"):
     usage()
@@ -197,18 +197,18 @@
   elif o in ("-x", "--dax"): 	 
     dax = True
   else:
-    print >> sys.stderr, "Unknown option:", o
+    print("Unknown option:", o, file=sys.stderr)
     usage()
     sys.exit(1)
 
 if not config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 # try and make a directory to store the cache files and job logs
@@ -490,17 +490,17 @@
 
 # write a message telling the user that the DAG has been written
 if dax: 	 
-  print """\nCreated a DAX file which can be submitted to the Grid using 	 
+  print("""\nCreated a DAX file which can be submitted to the Grid using 	 
 Pegasus. See the page: 	 
   	 
   http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/vds_howto.html 	 
   	 
 for instructions.
-"""
+""")
 else:
-  print "\nCreated a DAG file which can be submitted by executing"
-  print "\n   condor_submit_dag", dag.get_dag_file()
-  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
+  print("\nCreated a DAG file which can be submitted by executing")
+  print("\n   condor_submit_dag", dag.get_dag_file())
+  print("""\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
 If you are running LSCdataFind jobs, do not forget to initialize your grid 
 proxy certificate on the condor submit machine by running the commands
 
@@ -523,7 +523,7 @@
 
 Contact the administrator of your cluster to find the hostname and port of the
 LSCdataFind server.
-"""
+""")
 
 # write out a log file for this script
 if usertag:
@@ -543,14 +543,14 @@
 for seg in data:
   for chunk in seg:
     total_data += len(chunk)
-print >> log_fh, "total data =", total_data
-
-print >> log_fh, "\n===========================================\n"
-print >> log_fh, data
+print("total data =", total_data, file=log_fh)
+
+print("\n===========================================\n", file=log_fh)
+print(data, file=log_fh)
 for seg in data:
-  print >> log_fh, seg
+  print(seg, file=log_fh)
   for chunk in seg:
-    print >> log_fh, chunk
+    print(chunk, file=log_fh)
 
 sys.exit(0)
 
--- ./src/inspiral/lalapps_inspiral_ssipe.py	(original)
+++ ./src/inspiral/lalapps_inspiral_ssipe.py	(refactored)
@@ -111,7 +111,7 @@
   # we may use a fixed bank specified in ini file
   try:
     FixedBank = cp.get('input','fixed-bank')
-    print "For %s we use bank %s"%(ifo_name, FixedBank)
+    print("For %s we use bank %s"%(ifo_name, FixedBank))
   except:
     FixedBank = None
 
@@ -130,7 +130,7 @@
   # see if we are using calibrated data
   if cp.has_section(data_opts) and cp.has_option(data_opts,'calibrated-data'):
     calibrated = True
-    print "we use calibrated data for", ifo_name
+    print("we use calibrated data for", ifo_name)
   else: calibrated = False
 
   # prepare the injection filename
@@ -576,57 +576,57 @@
 #################################
 # if --version flagged
 if opts.version:
-  print "$Id$"
+  print("$Id$")
   sys.exit(0)
 
 #################################
 # Sanity check of input arguments
 if not opts.config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.g1_data and not opts.h1_data and not opts.h2_data and \
     not opts.l1_data and not opts.v1_data and not opts.analyze_all:
-  print >> sys.stderr, "No ifos specified.  Please specify at least one of"
-  print >> sys.stderr, "--g1-data, --h1-data, --h2-data, --l1-data, --v1-data"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
+  print("No ifos specified.  Please specify at least one of", file=sys.stderr)
+  print("--g1-data, --h1-data, --h2-data, --l1-data, --v1-data", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos all data", file=sys.stderr)
   sys.exit(1)
 elif opts.analyze_all:
-  print >> sys.stderr, "The --analyze-all flag is currently not available."
-  print >> sys.stderr, "The code supports quadruple coincidence, so you can"
-  print >> sys.stderr, "choose at most four instruments to analyze."
+  print("The --analyze-all flag is currently not available.", file=sys.stderr)
+  print("The code supports quadruple coincidence, so you can", file=sys.stderr)
+  print("choose at most four instruments to analyze.", file=sys.stderr)
   sys.exit(1)
 
 if opts.g1_data and opts.h1_data and opts.h2_data and opts.l1_data \
     and opts.v1_data:
-  print >> sys.stderr, "Too many IFOs specified. " \
-      "Please choose up to four IFOs, but not five."
+  print("Too many IFOs specified. " \
+      "Please choose up to four IFOs, but not five.", file=sys.stderr)
   sys.exit(1)
 
 if not opts.one_ifo and not opts.two_ifo and not opts.three_ifo and \
     not opts.four_ifo and not opts.analyze_all:
-  print >> sys.stderr, "No number of ifos given. Please specify at least one of"
-  print >> sys.stderr, "--one-ifo, --two-ifo, --three-ifo, --four-ifo"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos all data"
+  print("No number of ifos given. Please specify at least one of", file=sys.stderr)
+  print("--one-ifo, --two-ifo, --three-ifo, --four-ifo", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos all data", file=sys.stderr)
   sys.exit(1)
 elif opts.analyze_all:
-  print >> sys.stderr, "The --analyze-all flag can not be used to specify the"
-  print >> sys.stderr, "number of ifos to analyze. The code supports quadruple"
-  print >> sys.stderr, "coincidence, so you can choose at most four instruments"
-  print >> sys.stderr, "to analyze."
+  print("The --analyze-all flag can not be used to specify the", file=sys.stderr)
+  print("number of ifos to analyze. The code supports quadruple", file=sys.stderr)
+  print("coincidence, so you can choose at most four instruments", file=sys.stderr)
+  print("to analyze.", file=sys.stderr)
   sys.exit(1)
 
 if not (opts.datafind or opts.template_bank or opts.inspiral \
     or opts.sire_inspiral or opts.coincidence):
-  print >> sys.stderr, """  No steps of the pipeline specified.
+  print("""  No steps of the pipeline specified.
   Please specify at least one of
-  --datafind, --template-bank, --inspiral, --sire-inspiral, --coincidence"""
+  --datafind, --template-bank, --inspiral, --sire-inspiral, --coincidence""", file=sys.stderr)
   sys.exit(1)
    
 ifo_list = ['H1','H2','L1','V1','G1']
@@ -897,7 +897,7 @@
     job.add_opt('data-type','all_data')
 
 else:
-  print "Invalid playground data mask " + play_data_mask + " specified"
+  print("Invalid playground data mask " + play_data_mask + " specified")
   sys.exit(1)
 
  
@@ -945,7 +945,7 @@
 #   Step 1: read science segs that are greater or equal to a chunk 
 #   from the input file
 
-print "reading in single ifo science segments and creating master chunks...",
+print("reading in single ifo science segments and creating master chunks...", end=' ')
 sys.stdout.flush()
 
 segments = {}
@@ -964,7 +964,7 @@
     data[ifo].make_chunks_from_unused(length,overlap/2,playground_only,
         0,0,overlap/2,pad)
 
-print "done"
+print("done")
 
 # work out the earliest and latest times that are being analyzed
 if not gps_start_time:
@@ -972,8 +972,8 @@
   for ifo in ifo_list:
     if data[ifo] and (data[ifo][0].start() < gps_start_time):
       gps_start_time = data[ifo][0].start()
-  print "GPS start time not specified, obtained from segment lists as " + \
-    str(gps_start_time)
+  print("GPS start time not specified, obtained from segment lists as " + \
+    str(gps_start_time))
 
 
 if not gps_end_time:
@@ -981,8 +981,8 @@
   for ifo in ifo_list:
     if data[ifo] and (data[ifo][-1].end() > gps_end_time):
       gps_end_time = data[ifo][0].end()
-  print "GPS end time not specified, obtained from segment lists as " + \
-    str(gps_end_time)
+  print("GPS end time not specified, obtained from segment lists as " + \
+    str(gps_end_time))
 
 ##############################################################################
 #   Step 2: determine analyzable times
@@ -1046,7 +1046,7 @@
   inspinj.set_end(gps_end_time)
   inspinj.set_seed(0)
   inj_file = inspinj.get_output()
-  print inj_file
+  print(inj_file)
   shutil.copy( inj_file_loc, inj_file)
   inspinj = None
 elif seed:
@@ -1082,7 +1082,7 @@
 prev_df = None
 
 for ifo in ifo_list:
-  print "setting up jobs to filter " + ifo + " data..."
+  print("setting up jobs to filter " + ifo + " data...")
   sys.stdout.flush()
 
   (prev_df,chunks_analyzed[ifo]) = analyze_ifo(ifo,data[ifo],data_to_do[ifo],  
@@ -1099,14 +1099,14 @@
         gps_start_time, gps_end_time, inj_file = inj_file, 
         ifotag="SUMMARY_FIRST", usertag = usertag, inspinjNode=inspinj)
 
-  print "done" 
+  print("done") 
 
 
 ##############################################################################
 # Step 6: Run thinca on each of the disjoint sets of coincident data
 
 if opts.coincidence:
-  print "setting up thinca jobs..."
+  print("setting up thinca jobs...")
   sys.stdout.flush()
 
   # create a cache of the inspiral jobs
@@ -1144,7 +1144,7 @@
 
     # create cafe caches
     cafe_extent_limit = float(cp.get("ligolw_cafe","extentlimit"))
-    print "\tsetting up cafe caches for tisi file %s"%tisi_file_name
+    print("\tsetting up cafe caches for tisi file %s"%tisi_file_name)
     cafe_caches = ligolw_cafe.ligolw_cafe(inspiral_cache,
         ligolw_tisi.load_time_slides(tisi_file_name,
             gz = tisi_file_name.endswith(".gz")).values(),
@@ -1165,14 +1165,14 @@
 
     coinc_nodes.extend(new_coinc_nodes)
 
-  print "done"
+  print("done")
 
 
 ##############################################################################
 # Step 7: Write out the LAL cache files for the various output data
 
 if gps_start_time is not None and gps_end_time is not None:
-  print "generating cache files for output data products...",
+  print("generating cache files for output data products...", end=' ')
   cache_fname = ''
   for ifo in ifo_analyze:
     cache_fname += ifo
@@ -1197,9 +1197,9 @@
       output_data_cache.append(lal.Cache.from_urls([node.get_missed()])[0])
 
   output_data_cache.tofile(open(cache_fname, "w"))
-  print "done"
+  print("done")
 else:
-  print "gps start and stop times not specified: cache files not generated"     
+  print("gps start and stop times not specified: cache files not generated")     
 
 ##############################################################################
 # Step 8: Setup the Maximum number of jobs for different categories
@@ -1225,16 +1225,16 @@
 # write a message telling the user that the DAG has been written
 if opts.dax:
   
-  print "\nCreated an abstract DAX file", dag.get_dag_file()
-  print "which can be transformed into a concrete DAG with gencdag."
-  print "\nSee the documentation on http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/pegasus_lsc.html"
+  print("\nCreated an abstract DAX file", dag.get_dag_file())
+  print("which can be transformed into a concrete DAG with gencdag.")
+  print("\nSee the documentation on http://www.lsc-group.phys.uwm.edu/lscdatagrid/griphynligo/pegasus_lsc.html")
 
 
 
 else:
-  print "\nCreated a DAG file which can be submitted by executing"
-  print "\n   condor_submit_dag", dag.get_dag_file()
-  print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
+  print("\nCreated a DAG file which can be submitted by executing")
+  print("\n   condor_submit_dag", dag.get_dag_file())
+  print("""\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
   If you are running LSCdataFind jobs, do not forget to initialize your grid 
   proxy certificate on the condor submit machine by running the commands
   
@@ -1257,7 +1257,7 @@
   
   Contact the administrator of your cluster to find the hostname and port of the
   LSCdataFind server.
-  """
+  """)
 
 ##############################################################################
 # write out a log file for this script
@@ -1281,38 +1281,38 @@
 log_fh.write( cp.get('pipeline','version') + "\n" )
 log_fh.write( cp.get('pipeline','cvs-tag') + "\n\n" )
 
-print >> log_fh, "\n===========================================\n"
-print >> log_fh, "Science Segments and master chunks:\n"
+print("\n===========================================\n", file=log_fh)
+print("Science Segments and master chunks:\n", file=log_fh)
 
 for ifo in ifo_list:
-  print >> log_fh, "\n===========================================\n"
-  print >> log_fh, ifo + "Data\n"
+  print("\n===========================================\n", file=log_fh)
+  print(ifo + "Data\n", file=log_fh)
   for seg in data[ifo]:
-    print >> log_fh, " ", seg
+    print(" ", seg, file=log_fh)
     for chunk in seg:
-      print >> log_fh, "   ", chunk
+      print("   ", chunk, file=log_fh)
 
 
 for ifo in ifo_list:
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( 
     "Filtering " + str(len(chunks_analyzed[ifo])) + " " + ifo + \
     " master chunks\n" )
   total_time = 0
   for ifo_done in chunks_analyzed[ifo]:
-    print >> log_fh, ifo_done.get_chunk()
+    print(ifo_done.get_chunk(), file=log_fh)
     total_time += len(ifo_done.get_chunk())
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
 for ifo in ifo_list:
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( "Writing " + str(len(analyzed_data[ifo])) + " " + ifo + \
     " single IFO science segments\n" )
   total_time = 0
   for seg in analyzed_data[ifo]:
-    print >> log_fh, seg
+    print(seg, file=log_fh)
     total_time += seg.dur()
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
   if opts.output_segs and len(analyzed_data[ifo]):
     if playground_only:
@@ -1326,14 +1326,14 @@
 
 
 for ifos in ifo_coincs:  
-  print >> log_fh, "\n===========================================\n"
+  print("\n===========================================\n", file=log_fh)
   log_fh.write( "Writing " + str(len(analyzed_data[ifos])) + " " + ifos + \
     " coincident segments\n" )
   total_time = 0
   for seg in analyzed_data[ifos]:
-    print >> log_fh, seg
+    print(seg, file=log_fh)
     total_time += seg.dur()
-  print >> log_fh, "\n total time", total_time, "seconds"
+  print("\n total time", total_time, "seconds", file=log_fh)
 
   if opts.output_segs and len(analyzed_data[ifos]):
     if playground_only:
--- ./src/inspiral/lalapps_link_old_ihope.py	(original)
+++ ./src/inspiral/lalapps_link_old_ihope.py	(refactored)
@@ -96,12 +96,12 @@
 if opts.old_cache_file is not None:
   cache = lal.Cache.fromfile(open(opts.old_cache_file))
 else:
-  print "Must specify a cache file"
+  print("Must specify a cache file")
   exit(1)
 
 if not opts.run_names:
-  print "Must specify the names of the runs. For example, --run-names "\
-    "playground,inj001,inj002"
+  print("Must specify the names of the runs. For example, --run-names "\
+    "playground,inj001,inj002")
   exit(1)
 
 # Split the names of the runs into a list. On the command line,
@@ -166,14 +166,14 @@
   # them in the found and missed lists.
   (tmpfound,tmpmissed) = tmpcache.checkfilesexist()
   if len(tmpmissed)>0:
-    print "Warning: These files do not exist on disk, but are listed in the cache."
+    print("Warning: These files do not exist on disk, but are listed in the cache.")
     for missingfile in tmpmissed:
-      print "Missing file: " + str(missingfile)
+      print("Missing file: " + str(missingfile))
   if len(tmpfound)==0:
-    print "Nothing matched the sieve pattern: "+filetype
+    print("Nothing matched the sieve pattern: "+filetype)
   # Make the links and append the new file name to filelist
   if opts.make_links and len(tmpfound)>0:
-    print "Linking the " + filetype + " files..."
+    print("Linking the " + filetype + " files...")
     for file in tmpfound.pfnlist():
       os.symlink(file,filedir+"/"+os.path.basename(file))
       filelist.append(filedir+"/"+os.path.basename(file))
--- ./src/inspiral/lalapps_make_nr_hdf_catalog.py	(original)
+++ ./src/inspiral/lalapps_make_nr_hdf_catalog.py	(refactored)
@@ -56,7 +56,7 @@
             elif entry == 'simulation_id' or entry == 'process_id':
                 continue
             else:
-                print >> sys.stderr, "Column %s not recognized" %(entry)
+                print("Column %s not recognized" %(entry), file=sys.stderr)
                 raise ValueError
 
 _desc = __doc__[1:]
--- ./src/inspiral/lalapps_multi_hipe.py	(original)
+++ ./src/inspiral/lalapps_multi_hipe.py	(refactored)
@@ -251,7 +251,7 @@
 
 ##############################################################################
 # write command line to a file
-print sys.argv[0:]
+print(sys.argv[0:])
 
 
 ##############################################################################
@@ -319,7 +319,7 @@
 hipe_arguments_tmp=" ".join("--%s %s" % (name.replace("_","-"), value) for name, value in opts.__dict__.items() if value is not None and value is not False and name not in multi_hipe_options)
 
 hipe_arguments=hipe_arguments_tmp + " --config-file config.ini"
-print hipe_arguments
+print(hipe_arguments)
 
 ##############################################################################
 # loop over the intervals, constructing overlapping segment lists,
@@ -347,10 +347,10 @@
         modifiedstart = max( \
             min( tmpseg[1] - minsciseg, interval[0] - overlap/2 - paddata - 1 ),\
             tmpseg[0] )
-    except ValueError, e:
+    except ValueError as e:
       modifiedstart = interval[0]
       if opts.verbose:
-        print ifo + ": No segment containing interval start " + str(e)
+        print(ifo + ": No segment containing interval start " + str(e))
 
     # ....... and now the one overlapping the end time .......
     try:
@@ -362,10 +362,10 @@
         modifiedend = min( \
             max( tmpseg[0] + minsciseg, interval[1] + overlap/2 + paddata + 1 ),\
             tmpseg[1] )
-    except ValueError, e:
+    except ValueError as e:
       modifiedend = interval[1]
       if opts.verbose:
-        print ifo + ": No segment containing interval end " + str(e)
+        print(ifo + ": No segment containing interval end " + str(e))
 
     modifiedinterval = segments.segmentlist(\
         [segments.segment(modifiedstart,modifiedend)])
--- ./src/inspiral/lalapps_plot_hipe.py	(original)
+++ ./src/inspiral/lalapps_plot_hipe.py	(refactored)
@@ -43,7 +43,7 @@
     try:
       cp.set(this_section, common_option,  string.strip(cp.get('common',common_option)))
     except:
-      print "warning:", common_option, "is strongly recommended in the [common] section of your ini file"
+      print("warning:", common_option, "is strongly recommended in the [common] section of your ini file")
       pass
 
   return cp
@@ -175,7 +175,7 @@
 #################################
 # if --version flagged
 if opts.version:
-  print "$Id$"
+  print("$Id$")
   sys.exit(0)
 
 #################################
@@ -184,37 +184,37 @@
 
 # Checks for config file
 if not opts.config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 # Checks for log path
 if not opts.log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 # Checks for at least one ifo is specified  
 if not opts.g1_data and not opts.h1_data and not opts.h2_data and \
     not opts.l1_data and not opts.v1_data:
-  print >> sys.stderr, "No ifos specified.  Please specify at least one of"
-  print >> sys.stderr, "--g1-data, --h1-data, --h2-data, --l1-data, --v1-data"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos"
+  print("No ifos specified.  Please specify at least one of", file=sys.stderr)
+  print("--g1-data, --h1-data, --h2-data, --l1-data, --v1-data", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos", file=sys.stderr)
   sys.exit(1)
 
 # Checks for H1 and H2 data when running ploteffdistcut
 if opts.ploteffdistcut and not (opts.h1_data and opts.h2_data):
-  print >> sys.stderr, "How can I plot effdistcut when H1 and H2 are not"
-  print >> sys.stderr, "being analysed??? I will not run ploteffdistcut."
+  print("How can I plot effdistcut when H1 and H2 are not", file=sys.stderr)
+  print("being analysed??? I will not run ploteffdistcut.", file=sys.stderr)
   opts.ploteffdistcut = False
 
 # Checks for plots which require more than two ifos
 if ((opts.plotthinca or opts.plotethinca or opts.plotinspmissed or \
     opts.plotinspinj or opts.plotsnrchi or opts.ploteffdistcut) and \
     not (opts.two_ifo or opts.three_ifo or opts.four_ifo or opts.analyze_all)):
-  print >> sys.stderr, "No number of ifos given. Please specify at least one of"
-  print >> sys.stderr, "--two-ifo, --three-ifo, --four-ifo"
-  print >> sys.stderr, "or use --analyze-all to analyze all ifos"
+  print("No number of ifos given. Please specify at least one of", file=sys.stderr)
+  print("--two-ifo, --three-ifo, --four-ifo", file=sys.stderr)
+  print("or use --analyze-all to analyze all ifos", file=sys.stderr)
   sys.exit(1)
 
 ##############################################################################
@@ -428,7 +428,7 @@
   plotnumtemplates_jobs.set_sub_file( basename + '.plotnumtemplates_' + subsuffix)
   pattern_dict = determine_sieve_patterns(cp, this_section, "", input_user_tag)
   for opt, val in pattern_dict.iteritems():
-    print >> sys.stderr, opt, val
+    print(opt, val, file=sys.stderr)
     if opt=="bank-pattern": val = "TMPLTBANK"
     plotnumtemplates_jobs.add_opt(opt, val)
   
@@ -600,7 +600,7 @@
   cp = set_common_options(cp, this_section)
 
   if opts.first_stage:
-    print >>sys.stderr, "warning: plotsnrchi can only run with second stage inputs.  Skipping..."
+    print("warning: plotsnrchi can only run with second stage inputs.  Skipping...", file=sys.stderr)
   if opts.second_stage:
     for ifo in ifolist:   
       plotsnrchi_jobs[ifo] = inspiral.PlotSnrchiJob(cp)
@@ -776,9 +776,9 @@
 
 ##############################################################################  
 # write a message telling the user that the DAG has been written
-print "\nCreated a DAG file which can be submitted by executing"
-print "\n   condor_submit_dag", dag.get_dag_file()
-print "\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)"
+print("\nCreated a DAG file which can be submitted by executing")
+print("\n   condor_submit_dag", dag.get_dag_file())
+print("\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)")
 
 ##############################################################################
 # write out a log file for this script
--- ./src/inspiral/lalapps_run_sqlite.py	(original)
+++ ./src/inspiral/lalapps_run_sqlite.py	(refactored)
@@ -68,7 +68,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d:" % (n, len(databases)),
+		print("%d/%d:" % (n, len(databases)), end=' ', file=sys.stderr)
 	if filename.endswith(".xml") or filename.endswith(".xml.gz"):
 		# load XML file into in-ram database for processing
 		fileformat = "xml"
@@ -96,16 +96,16 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "Executing SQL ..."
+		print("Executing SQL ...", file=sys.stderr)
 	cursor = connection.cursor()
 	for statement in sql:
 		if options.verbose:
-			print >>sys.stderr, statement
+			print(statement, file=sys.stderr)
 		cursor.execute(statement)
 		connection.commit()
 	cursor.close()
 	if options.verbose:
-		print >>sys.stderr, "... Done."
+		print("... Done.", file=sys.stderr)
 
 	#
 	# commit changes
--- ./src/inspiral/lalapps_thinca.py	(original)
+++ ./src/inspiral/lalapps_thinca.py	(refactored)
@@ -163,7 +163,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d:" % (n, len(filenames)),
+		print("%d/%d:" % (n, len(filenames)), end=' ', file=sys.stderr)
 	xmldoc = ligolw_utils.load_filename(filename, verbose = options.verbose, contenthandler = ligolw.LIGOLWContentHandler)
 
 	#
@@ -172,13 +172,13 @@
 
 	if ligolw_process.doc_includes_process(xmldoc, process_program_name):
 		if options.verbose:
-			print >>sys.stderr, "warning: %s already processed," % (filename or "stdin"),
+			print("warning: %s already processed," % (filename or "stdin"), end=' ', file=sys.stderr)
 		if not options.force:
 			if options.verbose:
-				print >>sys.stderr, "skipping"
+				print("skipping", file=sys.stderr)
 			continue
 		if options.verbose:
-			print >>sys.stderr, "continuing by --force"
+			print("continuing by --force", file=sys.stderr)
 
 	#
 	# Add an entry to the process table.
@@ -214,11 +214,11 @@
 
 	if not ligolw_segments.has_segment_tables(xmldoc):
 		if options.verbose:
-			print >>sys.stderr, "warning: no segment definitions found, vetoes will not be applied"
+			print("warning: no segment definitions found, vetoes will not be applied", file=sys.stderr)
 		vetoes = None
 	elif not ligolw_segments.has_segment_tables(xmldoc, name = options.vetoes_name):
 		if options.verbose:
-			print >>sys.stderr, "warning: document contains segment definitions but none named \"%s\", vetoes will not be applied" % options.vetoes_name
+			print("warning: document contains segment definitions but none named \"%s\", vetoes will not be applied" % options.vetoes_name, file=sys.stderr)
 		vetoes = None
 	else:
 		vetoes = ligolw_segments.segmenttable_get_by_name(xmldoc, options.vetoes_name).coalesce()
--- ./src/inspiral/lalapps_trigger_hipe.py	(original)
+++ ./src/inspiral/lalapps_trigger_hipe.py	(refactored)
@@ -41,7 +41,7 @@
     os.mkdir(directory)
   except OSError:
     if overwrite:
-      print "Warning: Overwriting contents in existing directory %s" % directory
+      print("Warning: Overwriting contents in existing directory %s" % directory)
     else:
       raise
 
@@ -173,7 +173,7 @@
       str(jitter_sigma_deg), "--output-file", new_injFile,
       new_injFile + ".prejitter"])
     if verbose:
-      print " ".join(cmd)
+      print(" ".join(cmd))
     p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
     out, err = p.communicate()
     if p.returncode != 0:
@@ -186,7 +186,7 @@
     cmd = ["ligolw_cbc_align_total_spin", "--output-file", new_injFile,
       new_injFile + ".prealign"]
     if verbose:
-      print " ".join(cmd)
+      print(" ".join(cmd))
     p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
     out, err = p.communicate()
     if p.returncode != 0:
@@ -232,7 +232,7 @@
     # determine how many ifos to analyze
     n = len(self._ifos)
     if n < 1 or n > 4:
-      raise ValueError, "cannot handle less than one or more than four IFOs"
+      raise ValueError("cannot handle less than one or more than four IFOs")
     number_words = {1: "one", 2: "two", 3: "three", 4: "four"}
     for i in range(n):
       self._hipe_args.append("--%s-ifo" % number_words[i+1])
@@ -273,7 +273,7 @@
 
     if only_run is not None:
       if not (set(only_run) <= allowed_stages):
-        raise ValueError, "stage not in allowed stages"
+        raise ValueError("stage not in allowed stages")
       self._stages = only_run
     else:
       if dont_run is not None:
@@ -428,7 +428,7 @@
 
     if only_run is not None:
       if not (set(only_run) <= allowed_stages):
-        raise ValueError, "stage not in allowed stages"
+        raise ValueError("stage not in allowed stages")
       self._stages = only_run
     else:
       if dont_run is not None:
@@ -489,7 +489,7 @@
   # determine the files to ligolw_add
   sub_cache = orig_cache.sieve(description=pattern)
   if len(sub_cache) == 0:
-    print >>sys.stderr, "warning: no files on which to run ligolw_add"
+    print("warning: no files on which to run ligolw_add", file=sys.stderr)
     return None
 
   # create the cache-file
@@ -604,9 +604,9 @@
 required_opts = ["name", "time", "padding_time", "config_file", "injection_config", "log_path"]
 for opt in required_opts:
     if getattr(opts, opt) is None:
-        raise ValueError, "--%s is a required option" % opt
+        raise ValueError("--%s is a required option" % opt)
 if not opts.grb_file and (not opts.time or not opts.name):
-    raise ValueError, "Either a valid GRB xml file must be specified or the GPS time and name of the GRB!"
+    raise ValueError("Either a valid GRB xml file must be specified or the GPS time and name of the GRB!")
 
 ##############################################################################
 # find available data
@@ -634,7 +634,7 @@
 ext_trigs = grbsummary.load_external_triggers('grb%s.xml'%opts.name[0])
 
 if ext_trigs is None:
-  print >>sys.stderr, "No external triggers found.  Nothing to do."
+  print("No external triggers found.  Nothing to do.", file=sys.stderr)
   sys.exit()
 
 if len(opts.name) > 0:
@@ -644,12 +644,12 @@
 
   if len(ext_trigs) != len(opts.name):
     missing = set(opts.name) - set([row.event_number_grb for row in ext_trigs])
-    raise ValueError, "could not find the following requested GRBs: " \
-      + "".join(missing)
+    raise ValueError("could not find the following requested GRBs: " \
+      + "".join(missing))
 
 if opts.verbose:
-  print "Will construct workflows to analyze:", \
-    ", ".join(["GRB" + trig.event_number_grb for trig in ext_trigs])
+  print("Will construct workflows to analyze:", \
+    ", ".join(["GRB" + trig.event_number_grb for trig in ext_trigs]))
 
 if opts.do_coh_PTF:
   for program in ['coh_PTF_inspiral','coh_PTF_spin_checker']:
@@ -678,7 +678,7 @@
   # name and the directory
   idirectory = "GRB" + str(grb.event_number_grb)
   if opts.verbose:
-    print "* Constructing workflow for", idirectory
+    print("* Constructing workflow for", idirectory)
   mkdir(idirectory, opts.overwrite_dir)
 
   ##############################################################################
@@ -695,10 +695,10 @@
   ifo_times = "".join(grb_ifolist)
 
   if offSourceSegment is None:
-    print >>sys.stderr, "Warning: insufficient multi-IFO data to construct an off-source segment for GRB %s; skipping" % grb.event_number_grb
+    print("Warning: insufficient multi-IFO data to construct an off-source segment for GRB %s; skipping" % grb.event_number_grb, file=sys.stderr)
     continue
   elif opts.verbose:
-    print "Sufficient off-source data has been found in", ifo_times, "time."
+    print("Sufficient off-source data has been found in", ifo_times, "time.")
 
   # write out the segment list to a segwizard file
   offsource_segfile = idirectory + "/offSourceSeg.txt"
@@ -715,8 +715,8 @@
                             segments.segmentlist([bufferSegment]))
 
   if opts.verbose:
-    print "on-source segment: ", onSourceSegment
-    print "off-source segment: ", offSourceSegment
+    print("on-source segment: ", onSourceSegment)
+    print("off-source segment: ", offSourceSegment)
 
   ############################################################################
   # set up the analysis dag for this interval
@@ -736,7 +736,7 @@
   hipe_caches = []
 
   if opts.do_datafind:
-    if opts.verbose: print "Creating datafind DAG"
+    if opts.verbose: print("Creating datafind DAG")
     datafind_run = ["datafind"]
     if opts.do_coh_PTF:
       datafind_run.append("template-bank")
@@ -752,7 +752,7 @@
     datafind_node = None
 
   if opts.do_onoff:
-    if opts.verbose: print "Creating DAG for zero lag analysis"
+    if opts.verbose: print("Creating DAG for zero lag analysis")
     if opts.do_coh_PTF:
       onoff_analysis = ptf_run(idirectory + "/onoff", cp, grb_ifolist,
                         opts.log_path, source_file, "zero_lag",
@@ -772,7 +772,7 @@
     hipe_caches.append(onoff_analysis.get_cache_name())
 
   if opts.do_slides:
-    if opts.verbose: print "Creating DAG for time slide analysis"
+    if opts.verbose: print("Creating DAG for time slide analysis")
     if opts.do_coh_PTF:
       slide_analysis = ptf_run(idirectory + "/timeslides", cp, grb_ifolist,
                         opts.log_path, source_file, "slides",
@@ -798,7 +798,7 @@
     ############################################################################
     # set up the injection dag for this interval
     for injrun in cpinj.sections():
-      if opts.verbose: print "Creating DAG for", injrun
+      if opts.verbose: print("Creating DAG for", injrun)
       hipe_dir = os.path.join(idirectory, injrun)
       usertag = idirectory + "_" + injrun
 
@@ -814,7 +814,7 @@
                     "coire-coincidence"])
 
       # create the master injection file and get its content
-      if opts.verbose: print "  Creating injections..."
+      if opts.verbose: print("  Creating injections...")
       if opts.ipn:
         ipnstart = ext_trigs[0].start_time
       else:
@@ -827,7 +827,7 @@
       new_injfiles = [injFile]
 
       # split the injections
-      if opts.verbose: print "  Splitting injections..."
+      if opts.verbose: print("  Splitting injections...")
       safetyInterval = 800 # safety time in seconds between two injections
       deltaIndex = int(safetyInterval // injInterval) + 1
       for i in range(deltaIndex):
@@ -839,7 +839,7 @@
           new_injfiles.append(inj_file_name)
 
       # fixing the right parameters
-      if opts.verbose: print "  Writing DAG..."
+      if opts.verbose: print("  Writing DAG...")
       if not opts.do_coh_PTF:
         injection_analysis.set_numslides(0)
       else:
@@ -889,13 +889,13 @@
   concatenate_files(hipe_caches, grb_cache_name)
   grb_caches.append(grb_cache_name)
 
-if opts.verbose: print "Writing sub files and uber-DAG..."
+if opts.verbose: print("Writing sub files and uber-DAG...")
 uberdag.write_sub_files()
 uberdag.write_dag()
 
 concatenate_files(grb_caches, tag + ".cache")
 
-print """
+print("""
 If you run the uber-dag, you probably have to tell Condor not to
 complain that your DAG logs are on NFS volumes:
 
@@ -904,4 +904,4 @@
 
 tcsh users:
 setenv _CONDOR_DAGMAN_LOG_ON_NFS_IS_ERROR FALSE
-"""
+""")
--- ./src/inspiral/lalapps_write_ihope_page.py	(original)
+++ ./src/inspiral/lalapps_write_ihope_page.py	(refactored)
@@ -67,7 +67,7 @@
   """
   global h2_num
   global h3_num
-  print "...Processing " + section
+  print("...Processing " + section)
   subPage = initialize_page(section,style,script)
   subPage.p(e.br())
   subPage.add("<!-- beginning of a sub section -->")
@@ -104,7 +104,7 @@
     co = sys._getframe(nFramesUp+1).f_code
     msg = "%s (%s @ %d)" % (co.co_name, co.co_filename, co.co_firstlineno)
     if msg.startswith("?") is False:
-      print "-->ERROR in function: " + msg
+      print("-->ERROR in function: " + msg)
   except:
     msg="?"
 
@@ -126,7 +126,7 @@
     msg= "<"+tag+">"+text+"</"+tag+">\n"
     logfile.write(msg)
     if opts.verbose is True:
-      print >>sys.stdout,text
+      print(text, file=sys.stdout)
   elif tag=="error":
     msg = "<"+tag+">"+text
     logfile.write(msg)
@@ -139,12 +139,12 @@
         logfile.write("</"+tag+">\n")
         msg=[]
       i=i+1
-    print >>sys.stderr, text
+    print(text, file=sys.stderr)
   else :
     msg = "<"+tag+">"+text+"</"+tag+">\n"
     logfile.write( msg )
     if opts.verbose is True:
-      print >>sys.stdout,text
+      print(text, file=sys.stdout)
 
 # ***************************************************************************
 def patternFoundInFilename(filename, pattern):
@@ -192,23 +192,23 @@
 
   """
   if show_command and opts.verbose is True: 
-    print "--- Trying this command :" + command
+    print("--- Trying this command :" + command)
 
   stdin, out, err = os.popen3(command)
   pid, status = os.wait()
   this_output = out.read()
   if show_error & status != 0:
-    print >>sys.stderr, "External call failed."
-    print >>sys.stderr, "  status: %d" % status
-    print >>sys.stderr, "  stdout: %s" % this_output
-    print >>sys.stderr, "  stderr: %s" % err.read()
-    print >>sys.stderr, "  command: %s" % command
+    print("External call failed.", file=sys.stderr)
+    print("  status: %d" % status, file=sys.stderr)
+    print("  stdout: %s" % this_output, file=sys.stderr)
+    print("  stderr: %s" % err.read(), file=sys.stderr)
+    print("  command: %s" % command, file=sys.stderr)
     sys.exit(status)
   if show_stdout:
     if this_output[0:1]=='\n': 
-      print  this_output[1:]  #first character is \n
+      print(this_output[1:])  #first character is \n
     else:
-      print this_output
+      print(this_output)
 
   stdin.close()
   out.close()
@@ -223,7 +223,7 @@
   @param newdir : name of directory to be created (type: string)
   """
   if os.path.isdir(newdir): 
-    print >>sys.stdout,"WARNING: this directory already exists (" + newdir +")."
+    print("WARNING: this directory already exists (" + newdir +").", file=sys.stdout)
     pass
   elif os.path.isfile(newdir):
     raise OSError("a file with the same name as the desired " \
@@ -269,7 +269,7 @@
   """
 
   if opts.verbose is True:
-    print >>sys.stdout,"--------------------- Creating section "+section
+    print("--------------------- Creating section "+section, file=sys.stdout)
   if section=='general':
     page = write_general(page, opts)
   elif section=='datainfo':
@@ -393,7 +393,7 @@
 
   # The search parameters
   if opts.verbose is True: 
-    print "Extracting the version and tag of the executables..." 
+    print("Extracting the version and tag of the executables...") 
   
   # todo : get the list of executables  from the ini file
   executables = []
@@ -528,7 +528,7 @@
     ZLdir='full_data_summary_plots/'
     ifartag=['FULL_DATA','ALL_DATA','OPEN_BOX']
   else:
-    print 'Error in write_analysis function: thisSearch is not valid'
+    print('Error in write_analysis function: thisSearch is not valid')
     sys.exit(1)
   IJdir = injection + '_summary_plots/'
   PDdir = pipedown + '/'
@@ -838,7 +838,7 @@
     ifartag=['FULL_DATA','ALL_DATA','OPEN_BOX']
     FARtext="""the combined FAR of the loudest full data event."""
   else:
-    print 'Error in write_analysis function: thisSearch is not valid'
+    print('Error in write_analysis function: thisSearch is not valid')
     sys.exit(1)
   PDdir = pipedown + '/'
 
@@ -972,7 +972,7 @@
     images_dir='full_data_summary_plots/'
     ifartag=['FULL_DATA','ALL_DATA','OPEN_BOX']
   else:
-    print 'Error in write_analysis function: thisSearch is not valid'
+    print('Error in write_analysis function: thisSearch is not valid')
     sys.exit(1)
   PDdir = pipedown + '/'
   if opts.symlink_plots:
@@ -1814,7 +1814,7 @@
   elif heading == 'h3':
     section_num = str(h2_num)+"."+str(h3_num)+"."
   else:
-    raise ValueError, "heading must be either h2 or h3 in heading function"
+    raise ValueError("heading must be either h2 or h3 in heading function")
 
 
   page.add("<"+heading+">"+ section_num+ title)
@@ -2203,7 +2203,7 @@
   # At the moment this flag is unused but is kept as it may be used again.
   if not os.path.isdir(webDir):
     mkdir(webDir)
-    print >> sys.stdout, ' .../ copying all' + dataDir + ' files to ' + webDir
+    print(' .../ copying all' + dataDir + ' files to ' + webDir, file=sys.stdout)
     # Copy the html and cache files
     copy_files(dataDir,webDir,'*html')
     copy_files(dataDir,webDir,'*cache')
@@ -2232,7 +2232,7 @@
       errMsg = "Directory %s does not exist." %(from_path,)
       logText(logfile, errMsg, "warning")
 
-    print >> sys.stdout, '... Done'
+    print('... Done', file=sys.stdout)
 
 def symlink_plot_directory(dataDir,webDir):
   '''
@@ -2245,10 +2245,10 @@
     if webDir[-1] == '/':
       webDir = webDir[:-1]
     command = 'ln -s ' + dataDir + ' ' + webDir
-    print command
-    print >> sys.stdout, ' .../symlinking all' + dataDir + ' files to ' + webDir
+    print(command)
+    print(' .../symlinking all' + dataDir + ' files to ' + webDir, file=sys.stdout)
     make_external_call(command, opts.debug, opts.debug, True)
-    print >> sys.stdout, '... Done'
+    print('... Done', file=sys.stdout)
 
     
 # ***************************************************************************
@@ -2278,7 +2278,7 @@
   cachefileList =  glob.glob(thisglob)
   
   if opts.verbose is True: 
-    print "        Searching for files with this(ese) tag(s): " +str(image_tag)
+    print("        Searching for files with this(ese) tag(s): " +str(image_tag))
 
   # for each cachefile we create a div figure section with all 
   # figures whose name is part of the image_tag list
@@ -2292,7 +2292,7 @@
     href = None
     #for each file contained in the cachefile
     if opts.debug is True :
-      print >>sys.stdout, "        --> Copying files from " +eachcache
+      print("        --> Copying files from " +eachcache, file=sys.stdout)
 
     for filename in this:
       filename = filename.replace("\n","")
@@ -2368,7 +2368,7 @@
       cacheFiles[ifo] = file
       fileListDict.append(ifo)
     else:
-      print >> sys.stderr,"WARNING: file doesn't seem to match an ifo combo!"
+      print("WARNING: file doesn't seem to match an ifo combo!", file=sys.stderr)
 
   for file in htmlfileList:
     ifo = None
@@ -2378,12 +2378,12 @@
     if ifo:
       htmlFiles[ifo] = file
     else:
-      print >> sys.stderr,"WARNING: file doesn't seem to match an ifo combo!"
+      print("WARNING: file doesn't seem to match an ifo combo!", file=sys.stderr)
     if ifo not in cacheFiles.keys():
-      print >> sys.stderr,"WARNING: file doesn't seem to correspond to cache file list"
+      print("WARNING: file doesn't seem to correspond to cache file list", file=sys.stderr)
 
   if opts.verbose is True:
-    print "        Searching for files with this(ese) tag(s): " +str(image_tag)
+    print("        Searching for files with this(ese) tag(s): " +str(image_tag))
 
   for ifo in fileListDict:
     if ifo in htmlFiles.keys():
@@ -2396,7 +2396,7 @@
       #for each file contained in the cachefile
 
       if opts.debug is True:
-        print >>sys.stdout, "        --> Copying files from " +cacheFiles[ifo]
+        print("        --> Copying files from " +cacheFiles[ifo], file=sys.stdout)
 
       for filename in this:
         filename = filename.replace("\n","")
@@ -2463,7 +2463,7 @@
   htmlfileList =  glob.glob(thisglob)
 
   if opts.verbose is True:
-    print "        Copying this html file : " +str(html_tag)
+    print("        Copying this html file : " +str(html_tag))
 
   # for each cachefile we create a div figure section with all 
   # figures whose name is part of the image_tag list
@@ -2512,7 +2512,7 @@
     return tmp[len(tmp)-1]
     
   except:
-    print sys.stderr()<< 'could not copy the style file'
+    print(sys.stderr()<< 'could not copy the style file')
     pass
     return ""
 
@@ -2604,10 +2604,10 @@
 configcp = glue.pipeline.DeepCopyableConfigParser()
 configcp.read(config)
 maxdiv = 200
-print >>sys.stdout, "|------------------- Initialisation"
+print("|------------------- Initialisation", file=sys.stdout)
 # First, we open an xml file, for the log file
 logfile_name = __name__+".xml"
-print >>sys.stdout,"Openning the log file (" +logfile_name+")."
+print("Openning the log file (" +logfile_name+").", file=sys.stdout)
 logfile = open(logfile_name, "w")
 logfile.write("""<?xml version="1.0" encoding="ISO-8859-1"?>
 <?xml-stylesheet type="text/xsl" href="write_ihope_page.xsl"?>
@@ -2616,7 +2616,7 @@
 
 #---------------------------------------
 # then, we parse the write_ihope_page.ini file
-print >>sys.stdout,"Parsing the ini file: " + opts.config
+print("Parsing the ini file: " + opts.config, file=sys.stdout)
 try:
   opts.config_file 	= configcp.get("main", "ihope-ini-file")
   opts.gps_start_time 	= configcp.get("main", "gps-start-time")
@@ -2629,9 +2629,9 @@
   Second                = configcp.get("main", "second").replace( '"', '' )
   Notes                 = configcp.get("main", "notes").replace( '"', '' )
 except:
-  print >> sys.stderr, "ERROR::The ini file does not have the proper field in the [main] section" 
-  print >> sys.stderr, """       Consider adding one of those fields if missing: ihope-ini-file, \
-	gps-start-time,gps-end-time, ihope-directory, title,url, username, output"""
+  print("ERROR::The ini file does not have the proper field in the [main] section", file=sys.stderr) 
+  print("""       Consider adding one of those fields if missing: ihope-ini-file, \
+	gps-start-time,gps-end-time, ihope-directory, title,url, username, output""", file=sys.stderr)
   raise
   
 #------------------------------------
@@ -2647,19 +2647,19 @@
 
 #----------------------
 # openning the html file
-print >>sys.stdout,"Openning the HTML (" + opts.output+")"
+print("Openning the HTML (" + opts.output+")", file=sys.stdout)
 try:
   html_file = file(opts.output,"w")
 except:
   msg = "Cannot open %" % opts.output
-  print >>sys.stderr, msg
+  print(msg, file=sys.stderr)
   raise
 
 
 #-----------------------------------------
 # here is the directory we want to extract information from
 msg = "Entering this directory (where we will get all the relevant information)" + opts.datadir
-print >> sys.stdout, msg
+print(msg, file=sys.stdout)
 if not  os.path.isdir(opts.datadir):
   raise  "%s is not a valid directory. Check your gps time." % opts.datadir
 # which physical name is 
@@ -2724,8 +2724,8 @@
     html_sections['allinj'] = 'All Injections Combined'
   else:
     opts.injection = False
-    print >> sys.stderr, "No injections section found in ini file."
-    print >> sys.stderr, "Now running with --skip-injections"
+    print("No injections section found in ini file.", file=sys.stderr)
+    print("Now running with --skip-injections", file=sys.stderr)
 if opts.hardware_injection is True: 
   html_sections['hardware_injections'] = "Hardware Injection"
 if opts.full_data is True: html_sections['full_data'] = "Full Data"
@@ -2808,9 +2808,9 @@
 
 # that's it for the html creation. let us copy it to the requested directory  
 
-print '---------------------FINISHED ---------------------'
-print '--- HTML file created. '
-print '--- Copying html documents in ' +opts.physdir
+print('---------------------FINISHED ---------------------')
+print('--- HTML file created. ')
+print('--- Copying html documents in ' +opts.physdir)
 make_external_call('mv  '+opts.output +' ' + opts.physdir, opts.debug, opts.debug, True)
 make_external_call( 'mv toggle.js '+ opts.physdir, opts.debug, opts.debug,  True)
 
@@ -2822,13 +2822,13 @@
 
 if status==0:
   if int(output)==0:
-    print 'No warnings'
+    print('No warnings')
   else:
-    print '\n\n\nThere are warnings : '+str(int(output))+' . Check the log file '+logfile
+    print('\n\n\nThere are warnings : '+str(int(output))+' . Check the log file '+logfile)
   
   output, status = make_external_call('mv '+logfile + " "+opts.physdir, True,True,True) 
 else:
-  print 'Could not find the log file ' +logfile
+  print('Could not find the log file ' +logfile)
   
  
 #Finally create the xsl for the log xml file
--- ./src/lalapps/lalapps_path2cache.py	(original)
+++ ./src/lalapps/lalapps_path2cache.py	(refactored)
@@ -121,7 +121,7 @@
 			continue
 		else:
 			raise e
-	print >>dst, str(cache_entry)
+	print(str(cache_entry), file=dst)
 	path_count += 1
 	if cache_entry.segment is not None:
 		seglists |= cache_entry.segmentlistdict.coalesce()
@@ -133,12 +133,12 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "Size of cache: %d URLs" % path_count
+	print("Size of cache: %d URLs" % path_count, file=sys.stderr)
 	for instrument, seglist in seglists.items():
 		ext = seglist.extent()
 		dur = abs(seglist)
-		print >>sys.stderr, "Interval spanned by %s: %s (%s s total, %.4g%% duty cycle)" % (instrument, str(ext), str(dur), 100.0 * float(dur) / float(abs(ext)))
+		print("Interval spanned by %s: %s (%s s total, %.4g%% duty cycle)" % (instrument, str(ext), str(dur), 100.0 * float(dur) / float(abs(ext))), file=sys.stderr)
 	span = seglists.union(seglists)
 	ext = span.extent()
 	dur = abs(span)
-	print >>sys.stderr, "Interval spanned by union: %s (%s s total, %.4g%% duty cycle)" % (str(ext), str(dur), 100.0 * float(dur) / float(abs(ext)))
+	print("Interval spanned by union: %s (%s s total, %.4g%% duty cycle)" % (str(ext), str(dur), 100.0 * float(dur) / float(abs(ext))), file=sys.stderr)
--- ./src/lalapps/lalapps_searchsum2cache.py	(original)
+++ ./src/lalapps/lalapps_searchsum2cache.py	(refactored)
@@ -125,7 +125,7 @@
 for n, filename in enumerate(filenames):
 	# load document and extract search summary table
 	if options.verbose:
-		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
+		print("%d/%d:" % (n + 1, len(filenames)), end=' ', file=sys.stderr)
 	xmldoc = utils.load_filename(filename, verbose = options.verbose, contenthandler = ContentHandler)
 	searchsumm = lsctables.SearchSummaryTable.get_table(xmldoc)
 
@@ -139,11 +139,11 @@
 	# extract segment lists
 	seglists = searchsumm.get_out_segmentlistdict(process_ids).coalesce()
 	if not seglists:
-		raise ValueError, "%s: no matching rows found in search summary table" % filename
+		raise ValueError("%s: no matching rows found in search summary table" % filename)
 	if None in seglists:
 		if options.program is not None:
-			raise ValueError, "%s: null value in ifos column in search_summary table" % filename
-		raise ValueError, "%s: null value in ifos column in search_summary table, try using --program" % filename
+			raise ValueError("%s: null value in ifos column in search_summary table" % filename)
+		raise ValueError("%s: null value in ifos column in search_summary table, try using --program" % filename)
 
 	# extract observatory
 	observatory = (options.observatory and options.observatory.strip()) or "+".join(sorted(seglists))
@@ -157,16 +157,16 @@
 		else:
 			description = set(row.comment for row in searchsumm if row.process_id in process_ids)
 		if len(description) < 1:
-			raise ValueError, "%s: no matching rows found in search summary table" % filename
+			raise ValueError("%s: no matching rows found in search summary table" % filename)
 		if len(description) > 1:
-			raise ValueError, "%s: comments in matching rows of search summary table are not identical" % filename
+			raise ValueError("%s: comments in matching rows of search summary table are not identical" % filename)
 		description = description.pop().strip() or None
 
 	# set URL
 	url = "file://localhost" + os.path.abspath(filename)
 
 	# write cache entry
-	print >>options.output, str(CacheEntry(observatory, description, seglists.extent_all(), url))
+	print(str(CacheEntry(observatory, description, seglists.extent_all(), url)), file=options.output)
 
 	# allow garbage collection
 	xmldoc.unlink()
--- ./src/power/lalapps_binj_pic.py	(original)
+++ ./src/power/lalapps_binj_pic.py	(refactored)
@@ -114,7 +114,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "time-frequency tiles have %g degrees of freedom" % (2 * options.delta_t * options.delta_f)
+	print("time-frequency tiles have %g degrees of freedom" % (2 * options.delta_t * options.delta_f), file=sys.stderr)
 
 
 xmldoc = ligolw.Document()
@@ -123,7 +123,7 @@
 time_slide_table = xmldoc.childNodes[-1].appendChild(lsctables.TimeSlideTable.get_table(ligolw_utils.load_filename(options.time_slide_xml, verbose = options.verbose, contenthandler = LIGOLWContentHandler)))
 time_slide_id = time_slide_table[0].time_slide_id
 if options.verbose:
-	print >>sys.stderr, "associating injections with time slide ID \"%s\":  %s" % (time_slide_id, time_slide_table.as_dict()[time_slide_id])
+	print("associating injections with time slide ID \"%s\":  %s" % (time_slide_id, time_slide_table.as_dict()[time_slide_id]), file=sys.stderr)
 for row in time_slide_table:
 	row.process_id = process.process_id
 sim_burst_tbl = xmldoc.childNodes[-1].appendChild(lsctables.New(lsctables.SimBurstTable, ["process_id", "simulation_id", "time_slide_id", "waveform", "waveform_number", "ra", "dec", "psi", "time_geocent_gps", "time_geocent_gps_ns", "duration", "frequency", "bandwidth", "egw_over_rsquared", "pol_ellipse_angle", "pol_ellipse_e"]))
@@ -131,13 +131,13 @@
 
 for filename in filenames:
 	if options.verbose:
-		print >>sys.stderr, "loading %s ..." % filename
+		print("loading %s ..." % filename, file=sys.stderr)
 	img = Image.open(filename)
 
 	width, height = img.size
 	width, height = int(round(width / float(height) * options.height)), options.height
 	if options.verbose:
-		print >>sys.stderr, "converting to %dx%d grayscale ... " % (width, height)
+		print("converting to %dx%d grayscale ... " % (width, height), file=sys.stderr)
 	img = img.resize((width, height)).convert("L")
 
 	for i in xrange(width):
@@ -175,9 +175,9 @@
 				row.egw_over_rsquared *= row.hrss / lalsimulation.MeasureHrss(*lalburst.GenerateSimBurst(row, 1.0 / 16384))
 				sim_burst_tbl.append(row)
 			if options.verbose:
-				print >>sys.stderr, "generating sim_burst table ... %d injections\r" % len(sim_burst_tbl),
+				print("generating sim_burst table ... %d injections\r" % len(sim_burst_tbl), end=' ', file=sys.stderr)
 	if options.verbose:
-		print >>sys.stderr
+		print(file=sys.stderr)
 
 
 ligolw_utils.write_filename(xmldoc, options.output, gz = (options.output or "stdout").endswith(".gz"), verbose = options.verbose)
--- ./src/power/lalapps_binjfind.py	(original)
+++ ./src/power/lalapps_binjfind.py	(refactored)
@@ -146,7 +146,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
+		print("%d/%d:" % (n + 1, len(filenames)), end=' ', file=sys.stderr)
 	xmldoc = utils.load_filename(filename, verbose = options.verbose, contenthandler = ligolw.LIGOLWContentHandler)
 	binjfind.lsctables.table.InterningRowBuilder.strings.clear()
 
@@ -156,13 +156,13 @@
 
 	if ligolw_process.doc_includes_process(xmldoc, binjfind.process_program_name):
 		if options.verbose:
-			print >>sys.stderr, "warning: %s already processed," % (filename or "stdin"),
+			print("warning: %s already processed," % (filename or "stdin"), end=' ', file=sys.stderr)
 		if not options.force:
 			if options.verbose:
-				print >>sys.stderr, "skipping (use --force to force)"
+				print("skipping (use --force to force)", file=sys.stderr)
 			continue
 		if options.verbose:
-			print >>sys.stderr, "continuing by --force"
+			print("continuing by --force", file=sys.stderr)
 
 	#
 	# add process metadata to document
--- ./src/power/lalapps_bucut.py	(original)
+++ ./src/power/lalapps_bucut.py	(refactored)
@@ -365,21 +365,21 @@
 	removed_ids = set()
 	if veto_segments:
 		if verbose:
-			print >>sys.stderr, "applying veto segment list ..."
+			print("applying veto segment list ...", file=sys.stderr)
 		removed_ids |= remove_events_by_segment(contents, veto_segments)
 	if verbose:
-		print >>sys.stderr, "filtering sngl_burst rows by parameters ..."
+		print("filtering sngl_burst rows by parameters ...", file=sys.stderr)
 	removed_ids |= remove_events_by_parameters(contents, burst_test_func)
 	if del_skipped_injections:
 		if verbose:
-			print >>sys.stderr, "removing injections that weren't performed ..."
+			print("removing injections that weren't performed ...", file=sys.stderr)
 		remove_skipped_injections(contents)
 	if verbose:
-		print >>sys.stderr, "removing broken coincidences ..."
+		print("removing broken coincidences ...", file=sys.stderr)
 	clean_coinc_tables(contents, removed_ids)
 	if del_non_coincs:
 		if verbose:
-			print >>sys.stderr, "removing non-coincident events ..."
+			print("removing non-coincident events ...", file=sys.stderr)
 		remove_non_coincidences(contents)
 
 
--- ./src/power/lalapps_burca.py	(original)
+++ ./src/power/lalapps_burca.py	(refactored)
@@ -163,7 +163,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
+		print("%d/%d:" % (n + 1, len(filenames)), end=' ', file=sys.stderr)
 	xmldoc = ligolw_utils.load_filename(filename, verbose = options.verbose, contenthandler = LIGOLWContentHandler)
 
 	#
@@ -172,13 +172,13 @@
 
 	if ligolw_process.doc_includes_process(xmldoc, process_program_name):
 		if options.verbose:
-			print >>sys.stderr, "warning: %s already processed," % (filename or "stdin"),
+			print("warning: %s already processed," % (filename or "stdin"), end=' ', file=sys.stderr)
 		if not options.force:
 			if options.verbose:
-				print >>sys.stderr, "skipping"
+				print("skipping", file=sys.stderr)
 			continue
 		if options.verbose:
-			print >>sys.stderr, "continuing by --force"
+			print("continuing by --force", file=sys.stderr)
 
 	#
 	# Add an entry to the process table.
--- ./src/power/lalapps_burca_tailor.py	(original)
+++ ./src/power/lalapps_burca_tailor.py	(refactored)
@@ -138,7 +138,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
--- ./src/power/lalapps_gen_timeslides.py	(original)
+++ ./src/power/lalapps_gen_timeslides.py	(refactored)
@@ -138,7 +138,7 @@
 	time_slide_table = lsctables.TimeSlideTable.get_table(ligolw_utils.load_filename(filename, verbose = options.verbose, contenthandler = LIGOLWContentHandler))
 	extra_time_slides = time_slide_table.as_dict().values()
 	if options.verbose:
-		print >>sys.stderr, "Loaded %d time slides." % len(extra_time_slides)
+		print("Loaded %d time slides." % len(extra_time_slides), file=sys.stderr)
 	for offsetvect in extra_time_slides:
 		time_slides[lsctables.TimeSlideTable.get_next_id()] = offsetvect
 
@@ -149,7 +149,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "Computing new time slides ..."
+	print("Computing new time slides ...", file=sys.stderr)
 
 # dictionary mapping time_slide_id --> (dictionary mapping insrument --> offset)
 
@@ -160,7 +160,7 @@
 		time_slides[lsctables.TimeSlideTable.get_next_id()] = offsetvect
 
 if options.verbose:
-	print >>sys.stderr, "Total of %d time slides." % len(time_slides)
+	print("Total of %d time slides." % len(time_slides), file=sys.stderr)
 
 
 #
@@ -169,12 +169,12 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "Identifying and removing duplicates ..."
+	print("Identifying and removing duplicates ...", file=sys.stderr)
 
 map(time_slides.pop, ligolw_time_slide.time_slides_vacuum(time_slides, verbose = options.verbose).keys())
 
 if options.verbose:
-	print >>sys.stderr, "%d time slides remain." % len(time_slides)
+	print("%d time slides remain." % len(time_slides), file=sys.stderr)
 
 
 #
@@ -184,14 +184,14 @@
 
 if options.remove_zero_lag:
 	if options.verbose:
-		print >>sys.stderr, "Identifying and removing zero-lag ..."
+		print("Identifying and removing zero-lag ...", file=sys.stderr)
 
 	null_ids = [time_slide_id for time_slide_id, offsetvect in time_slides.items() if not any(offsetvect.deltas.values())]
 	for time_slide_id in null_ids:
 		del time_slides[time_slide_id]
 
 	if options.verbose:
-		print >>sys.stderr, "%d time slides remain." % len(time_slides)
+		print("%d time slides remain." % len(time_slides), file=sys.stderr)
 
 
 #
@@ -210,7 +210,7 @@
 
 if options.normalize:
 	if options.verbose:
-		print >>sys.stderr, "Normalizing the time slides ..."
+		print("Normalizing the time slides ...", file=sys.stderr)
 	constraints = parse_normalize(options.normalize)
 	time_slides = [offsetvect.normalize(**constraints) for offsetvect in time_slides]
 
--- ./src/power/lalapps_plot_tisi.py	(original)
+++ ./src/power/lalapps_plot_tisi.py	(refactored)
@@ -159,10 +159,10 @@
 	xmldoc = ligolw_utils.load_filename(filename, verbose = options.verbose, contenthandler = ligolw.LIGOLWContentHandler)
 
 	if options.verbose:
-		print >>sys.stderr, "plotting ..."
+		print("plotting ...", file=sys.stderr)
 	plot = Plot(xmldoc, options.x_instrument, options.y_instrument, require_instruments = options.require_instruments)
 
 	output = options.output.replace("%n", "%d" % n)
 	if options.verbose:
-		print >>sys.stderr, "writing %s ..." % output
+		print("writing %s ..." % output, file=sys.stderr)
 	plot.fig.savefig(output)
--- ./src/power/lalapps_power_calc_likelihood.py	(original)
+++ ./src/power/lalapps_power_calc_likelihood.py	(refactored)
@@ -132,7 +132,7 @@
 
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
 	connection.execute("PRAGMA temp_store_directory = '%s';" % dbtables.tempfile.gettempdir())
--- ./src/power/lalapps_power_final.py	(original)
+++ ./src/power/lalapps_power_final.py	(refactored)
@@ -238,7 +238,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "building file list ..."
+		print("building file list ...", file=sys.stderr)
 	filenames = sorted(filename for g in globs for filename in glob.glob(g))
 
 	#
@@ -259,7 +259,7 @@
 		#
 
 		if verbose:
-			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+			print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 		working_filename = dbtables.get_connection_filename(filename, tmp_path = tmp_path, verbose = verbose)
 		connection = sqlite3.connect(working_filename)
 		connection.create_function("coinc_detection_statistic", 2, coinc_detection_statistic)
@@ -318,22 +318,22 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "writing scatter plot data ..."
+		print("writing scatter plot data ...", file=sys.stderr)
 
 	f = file("lalapps_excesspowerfinal_background_scatter.dat", "w")
 	for a, l, c in background:
-		print >>f, "%.16g %.16g" % (l, c)
+		print("%.16g %.16g" % (l, c), file=f)
 
 	f = file("lalapps_excesspowerfinal_zero_lag_scatter.dat", "w")
 	for a, l, c in zero_lag:
-		print >>f, "%.16g %.16g" % (l, c)
+		print("%.16g %.16g" % (l, c), file=f)
 
 	f = file("lalapps_excesspowerfinal_injections_scatter.dat", "w")
 	for a, l, c in injections:
-		print >>f, "%.16g %.16g" % (l, c)
+		print("%.16g %.16g" % (l, c), file=f)
 
 	if verbose:
-		print >>sys.stderr, "done."
+		print("done.", file=sys.stderr)
 
 
 #
@@ -362,7 +362,7 @@
 
 	def read_and_plot(filename, colour, verbose = False):
 		if verbose:
-			print >>sys.stderr, "reading '%s' ..." % filename
+			print("reading '%s' ..." % filename, file=sys.stderr)
 		X = []
 		Y = []
 		for line in file(filename):
@@ -372,7 +372,7 @@
 			X.append(x)
 			Y.append(y)
 		if verbose:
-			print >>sys.stderr, "plotting ..."
+			print("plotting ...", file=sys.stderr)
 		return axes.plot(X, Y, colour)
 
 	set1 = read_and_plot("lalapps_excesspowerfinal_injections_scatter.dat", "r+", verbose = verbose)
@@ -390,7 +390,7 @@
 		return numpy.exp(slope * numpy.log(x) + lnb)
 
 	if verbose:
-		print >>sys.stderr, "plotting contours ..."
+		print("plotting contours ...", file=sys.stderr)
 	ymin, ymax = axes.get_ylim()
 	for lnb in range(10, 110, 10):
 		x = 10**numpy.arange(0.85, 3.0, 0.01)
@@ -411,7 +411,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "writing 'lalapps_excesspowerfinal_scatter.png' ..."
+		print("writing 'lalapps_excesspowerfinal_scatter.png' ...", file=sys.stderr)
 	fig.savefig("lalapps_excesspowerfinal_scatter.png")
 
 	#
@@ -419,7 +419,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "done."
+		print("done.", file=sys.stderr)
 
 
 #
@@ -451,7 +451,7 @@
 		#
 
 		if verbose:
-			print >>sys.stderr, "measuring live time ..."
+			print("measuring live time ...", file=sys.stderr)
 		zero_lag_time_slides, background_time_slides = SnglBurstUtils.get_time_slides(contents.connection)
 		self.zero_lag_live_time += SnglBurstUtils.time_slides_livetime(contents.seglists, zero_lag_time_slides.values(), verbose = verbose)
 		self.background_live_time += SnglBurstUtils.time_slides_livetime(contents.seglists, background_time_slides.values(), verbose = verbose)
@@ -462,7 +462,7 @@
 		#
 
 		if verbose:
-			print >>sys.stderr, "retrieving sngl_burst<-->sngl_burst coincs ..."
+			print("retrieving sngl_burst<-->sngl_burst coincs ...", file=sys.stderr)
 		for id, likelihood, confidence, is_background in bb_id_likelihood_confidence_background(contents):
 			record = coinc_detection_statistic(likelihood, confidence)
 			if is_background:
@@ -476,7 +476,7 @@
 			else:
 				self.zero_lag_amplitudes.append((record, filename, id))
 		if verbose:
-			print >>sys.stderr, "done"
+			print("done", file=sys.stderr)
 
 
 	def finish(self, zero_lag_survivors, open_box = False, verbose = False):
@@ -603,7 +603,7 @@
 		#
 
 		if verbose:
-			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+			print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 		working_filename = dbtables.get_connection_filename(filename, tmp_path = tmp_path, verbose = verbose)
 		database = SnglBurstUtils.CoincDatabase(sqlite3.connect(working_filename), live_time_program)
 		if verbose:
@@ -627,7 +627,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "finishing rate vs. threshold measurement ..."
+		print("finishing rate vs. threshold measurement ...", file=sys.stderr)
 	rate_vs_threshold_data.finish(n_survivors, open_box = open_box, verbose = verbose)
 
 	#
@@ -643,25 +643,25 @@
 
 
 def print_rate_vs_threshold_data(rate_vs_threshold_data, confidence_contour_slope):
-	print >>sys.stderr
-	print >>sys.stderr, "=== Threshold Summary ==="
-	print >>sys.stderr
-	print >>sys.stderr, "threshold definition:  ln likelihood > %.16g ln confidence + %.16g" % (confidence_contour_slope, rate_vs_threshold_data.amplitude_threshold)
-	print >>sys.stderr, "total live time in background = %.16g s" % rate_vs_threshold_data.background_live_time
-	print >>sys.stderr, "total live time at zero lag = %.16g s" % rate_vs_threshold_data.zero_lag_live_time
-	print >>sys.stderr, "number of coincs in background = %d" % rate_vs_threshold_data.n_background_amplitudes
-	print >>sys.stderr, "average number of background coincs per zero lag live time = %.16g" % (rate_vs_threshold_data.n_background_amplitudes / rate_vs_threshold_data.background_live_time * rate_vs_threshold_data.zero_lag_live_time)
-	print >>sys.stderr, "number of coincs at zero lag = %d" % len(rate_vs_threshold_data.zero_lag_amplitudes)
-	print >>sys.stderr, "at threshold, \\mu_{0} = %.16g Hz +/- %.16g Hz" % (rate_vs_threshold_data.mu_0, rate_vs_threshold_data.dmu_0)
-	print >>sys.stderr, "at threshold, \\mu_{0}' = %.16g Hz / unit of amplitude" % rate_vs_threshold_data.mu_0primed
-	print >>sys.stderr
-	print >>sys.stderr, "100 Highest-Ranked Zero Lag Events"
-	print >>sys.stderr, "----------------------------------"
-	print >>sys.stderr
-	print >>sys.stderr, "Detection Statistic\tFilename\tID"
+	print(file=sys.stderr)
+	print("=== Threshold Summary ===", file=sys.stderr)
+	print(file=sys.stderr)
+	print("threshold definition:  ln likelihood > %.16g ln confidence + %.16g" % (confidence_contour_slope, rate_vs_threshold_data.amplitude_threshold), file=sys.stderr)
+	print("total live time in background = %.16g s" % rate_vs_threshold_data.background_live_time, file=sys.stderr)
+	print("total live time at zero lag = %.16g s" % rate_vs_threshold_data.zero_lag_live_time, file=sys.stderr)
+	print("number of coincs in background = %d" % rate_vs_threshold_data.n_background_amplitudes, file=sys.stderr)
+	print("average number of background coincs per zero lag live time = %.16g" % (rate_vs_threshold_data.n_background_amplitudes / rate_vs_threshold_data.background_live_time * rate_vs_threshold_data.zero_lag_live_time), file=sys.stderr)
+	print("number of coincs at zero lag = %d" % len(rate_vs_threshold_data.zero_lag_amplitudes), file=sys.stderr)
+	print("at threshold, \\mu_{0} = %.16g Hz +/- %.16g Hz" % (rate_vs_threshold_data.mu_0, rate_vs_threshold_data.dmu_0), file=sys.stderr)
+	print("at threshold, \\mu_{0}' = %.16g Hz / unit of amplitude" % rate_vs_threshold_data.mu_0primed, file=sys.stderr)
+	print(file=sys.stderr)
+	print("100 Highest-Ranked Zero Lag Events", file=sys.stderr)
+	print("----------------------------------", file=sys.stderr)
+	print(file=sys.stderr)
+	print("Detection Statistic\tFilename\tID", file=sys.stderr)
 	for amplitude, filename, id in rate_vs_threshold_data.zero_lag_amplitudes[:100]:
-		print >>sys.stderr, "%.16g\t%s\t%s" % (amplitude, filename, id)
-	print >>sys.stderr
+		print("%.16g\t%s\t%s" % (amplitude, filename, id), file=sys.stderr)
+	print(file=sys.stderr)
 
 
 #
@@ -677,8 +677,8 @@
 	# start the rate vs. threshold plot
 
 
-	print >>sys.stderr
-	print >>sys.stderr, "plotting event rate ..."
+	print(file=sys.stderr)
+	print("plotting event rate ...", file=sys.stderr)
 
 	fig, axes = SnglBurstUtils.make_burst_plot(r"Detection Statistic Threshold", r"Mean Event Rate (Hz)")
 	axes.semilogy()
@@ -728,7 +728,7 @@
 
 	#print >>sys.stderr, "writing lalapps_excesspowerfinal_rate_vs_threshold.pdf ..."
 	#fig.savefig("lalapps_excesspowerfinal_rate_vs_threshold.pdf")
-	print >>sys.stderr, "writing lalapps_excesspowerfinal_rate_vs_threshold.png ..."
+	print("writing lalapps_excesspowerfinal_rate_vs_threshold.png ...", file=sys.stderr)
 	fig.savefig("lalapps_excesspowerfinal_rate_vs_threshold.png")
 
 	# start rate vs. threshold residual plot
@@ -754,7 +754,7 @@
 
 	#print >>sys.stderr, "writing lalapps_excesspowerfinal_rate_vs_threshold_residual.pdf ..."
 	#fig.savefig("lalapps_excesspowerfinal_rate_vs_threshold_residual.pdf")
-	print >>sys.stderr, "writing lalapps_excesspowerfinal_rate_vs_threshold_residual.png ..."
+	print("writing lalapps_excesspowerfinal_rate_vs_threshold_residual.png ...", file=sys.stderr)
 	fig.savefig("lalapps_excesspowerfinal_rate_vs_threshold_residual.png")
 
 	# done
@@ -775,7 +775,7 @@
 
 
 def diagnostic_plot(z, bins, title, ylabel, filename):
-	print >>sys.stderr, "generating %s ..." % filename
+	print("generating %s ..." % filename, file=sys.stderr)
 	fig, axes = SnglBurstUtils.make_burst_plot("Centre Frequency (Hz)", ylabel)
 	axes.loglog()
 	xcoords, ycoords = bins.centres()
@@ -843,7 +843,7 @@
 				# because we are only looking for coincs
 				# that are "nearby" an injection which can
 				# mean several seconds.
-				print >>sys.stderr, "odd, injection %s was found but not injected ..." % sim.simulation_id
+				print("odd, injection %s was found but not injected ..." % sim.simulation_id, file=sys.stderr)
 
 
 	def finish(self, threshold):
@@ -985,7 +985,7 @@
 		#
 
 		if verbose:
-			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+			print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 		working_filename = dbtables.get_connection_filename(filename, tmp_path = tmp_path, verbose = verbose)
 		connection = sqlite3.connect(working_filename)
 		connection.create_function("coinc_detection_statistic", 2, coinc_detection_statistic)
@@ -1011,7 +1011,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "binning and smoothnig efficiency data ..."
+		print("binning and smoothnig efficiency data ...", file=sys.stderr)
 	efficiency.finish(threshold)
 
 	#
@@ -1027,7 +1027,7 @@
 
 
 def plot_efficiency_data(efficiency_data):
-	print >>sys.stderr, "plotting efficiency curves ..."
+	print("plotting efficiency curves ...", file=sys.stderr)
 
 	# use the stock plotting routing in SimBurstUtils for the
 	# efficiency contour plot
@@ -1039,7 +1039,7 @@
 
 	#print >>sys.stderr, "writing lalapps_excesspowerfinal_efficiency.pdf ..."
 	#fig.savefig("lalapps_excesspowerfinal_efficiency.pdf")
-	print >>sys.stderr, "writing lalapps_excesspowerfinal_efficiency.png ..."
+	print("writing lalapps_excesspowerfinal_efficiency.png ...", file=sys.stderr)
 	fig.savefig("lalapps_excesspowerfinal_efficiency.png")
 
 
@@ -1089,7 +1089,7 @@
 
 
 def rate_upper_limit(efficiency_data, mu_0primed, zero_lag_live_time, p):
-	print >>sys.stderr, "computing rate upper limit ..."
+	print("computing rate upper limit ...", file=sys.stderr)
 
 	# initialize the rate upper limit array, giving it the same binning
 	# as the efficiency array.
@@ -1151,7 +1151,7 @@
 
 
 def plot_rate_upper_limit(rate_data):
-	print >>sys.stderr, "plotting rate upper limit ..."
+	print("plotting rate upper limit ...", file=sys.stderr)
 
 	#
 	# contour plot in frequency-energy plane
@@ -1172,7 +1172,7 @@
 
 	#print >>sys.stderr, "writing lalapps_excesspowerfinal_upper_limit_1.pdf ..."
 	#fig.savefig("lalapps_excesspowerfinal_upper_limit_1.pdf")
-	print >>sys.stderr, "writing lalapps_excesspowerfinal_upper_limit_1.png ..."
+	print("writing lalapps_excesspowerfinal_upper_limit_1.png ...", file=sys.stderr)
 	fig.savefig("lalapps_excesspowerfinal_upper_limit_1.png")
 
 	#
@@ -1195,7 +1195,7 @@
 
 	#print >>sys.stderr, "writing lalapps_excesspowerfinal_upper_limit_2.pdf ..."
 	#fig.savefig("lalapps_excesspowerfinal_upper_limit_2.pdf")
-	print >>sys.stderr, "writing lalapps_excesspowerfinal_upper_limit_2.png ..."
+	print("writing lalapps_excesspowerfinal_upper_limit_2.png ...", file=sys.stderr)
 	fig.savefig("lalapps_excesspowerfinal_upper_limit_2.png")
 
 
@@ -1266,20 +1266,20 @@
 
 
 if options.dump_scatter_data:
-	print >>sys.stderr, "=== Confidence-Likelihood Scatter Dump ==="
-	print >>sys.stderr
+	print("=== Confidence-Likelihood Scatter Dump ===", file=sys.stderr)
+	print(file=sys.stderr)
 	dump_confidence_likelihood_scatter_data(options.background_glob + options.injections_glob, live_time_program = options.live_time_program, tmp_path = options.tmp_space, verbose = options.verbose)
-	print >>sys.stderr
-	print >>sys.stderr, "=== Done ==="
+	print(file=sys.stderr)
+	print("=== Done ===", file=sys.stderr)
 	sys.exit(0)
 
 
 if options.plot_scatter_data:
-	print >>sys.stderr, "=== Confidence-Likelihood Scatter Plot ==="
-	print >>sys.stderr
+	print("=== Confidence-Likelihood Scatter Plot ===", file=sys.stderr)
+	print(file=sys.stderr)
 	plot_confidence_likelihood_scatter_data(options.confidence_contour_slope, verbose = options.verbose)
-	print >>sys.stderr
-	print >>sys.stderr, "=== Done ==="
+	print(file=sys.stderr)
+	print("=== Done ===", file=sys.stderr)
 	sys.exit(0)
 
 
@@ -1290,13 +1290,13 @@
 #
 
 
-print >>sys.stderr, "=== Threshold ==="
-print >>sys.stderr
-print >>sys.stderr, "\t\tBOX IS %s" % (options.open_box and "OPEN!!" or "CLOSED!!")
-print >>sys.stderr
+print("=== Threshold ===", file=sys.stderr)
+print(file=sys.stderr)
+print("\t\tBOX IS %s" % (options.open_box and "OPEN!!" or "CLOSED!!"), file=sys.stderr)
+print(file=sys.stderr)
 
 if options.verbose:
-	print >>sys.stderr, "building file list ..."
+	print("building file list ...", file=sys.stderr)
 filenames = sorted(filename for g in options.background_glob for filename in glob.glob(g))
 if not filenames:
 	raise ValueError("no background/zero lag files found")
@@ -1312,8 +1312,8 @@
 	rate_vs_threshold_data.amplitude_threshold = 49.3975937091782
 	rate_vs_threshold_data.mu_0primed = -2.000347702238217e-07
 
-print >>sys.stderr, "done."
-print >>sys.stderr
+print("done.", file=sys.stderr)
+print(file=sys.stderr)
 
 
 #
@@ -1321,26 +1321,26 @@
 #
 
 
-print >>sys.stderr
-print >>sys.stderr, "=== Efficiency =="
-print >>sys.stderr
+print(file=sys.stderr)
+print("=== Efficiency ==", file=sys.stderr)
+print(file=sys.stderr)
 
 if options.verbose:
-	print >>sys.stderr, "building file list ..."
+	print("building file list ...", file=sys.stderr)
 filenames = sorted(filename for g in options.injections_glob for filename in glob.glob(g))
 if not filenames:
 	raise ValueError("no injection files found")
 
 efficiency_data = measure_efficiency(filenames, rate_vs_threshold_data.amplitude_threshold, live_time_program = options.live_time_program, upper_limit_scale = options.upper_limit_scale, tmp_path = options.tmp_space, verbose = options.verbose)
 
-print >>sys.stderr
-print >>sys.stderr, "=== Efficiency Summary ==="
-print >>sys.stderr
+print(file=sys.stderr)
+print("=== Efficiency Summary ===", file=sys.stderr)
+print(file=sys.stderr)
 
 plot_efficiency_data(efficiency_data)
 
-print >>sys.stderr, "done."
-print >>sys.stderr
+print("done.", file=sys.stderr)
+print(file=sys.stderr)
 
 
 #
@@ -1348,15 +1348,15 @@
 #
 
 
-print >>sys.stderr
-print >>sys.stderr, "=== Rate Upper Limit ==="
-print >>sys.stderr
+print(file=sys.stderr)
+print("=== Rate Upper Limit ===", file=sys.stderr)
+print(file=sys.stderr)
 
 rate_data = rate_upper_limit(efficiency_data, rate_vs_threshold_data.mu_0primed, rate_vs_threshold_data.zero_lag_live_time, options.upper_limit_confidence)
 plot_rate_upper_limit(rate_data)
 
-print >>sys.stderr, "done."
-print >>sys.stderr
+print("done.", file=sys.stderr)
+print(file=sys.stderr)
 
 
 #
@@ -1364,6 +1364,6 @@
 #
 
 
-print >>sys.stderr
-print >>sys.stderr, "=== Done ==="
-print >>sys.stderr
+print(file=sys.stderr)
+print("=== Done ===", file=sys.stderr)
+print(file=sys.stderr)
--- ./src/power/lalapps_power_likelihood_pipe.py	(original)
+++ ./src/power/lalapps_power_likelihood_pipe.py	(refactored)
@@ -74,7 +74,7 @@
 	options, filenames = parser.parse_args()
 
 	if options.distribution_segments is None:
-		raise ValueError, "missing required argument --distribution-segments"
+		raise ValueError("missing required argument --distribution-segments")
 	options.distribution_segments = segmentsUtils.fromsegwizard(file(options.distribution_segments), coltype = lal.LIGOTimeGPS)
 
 	options.input_cache = set([CacheEntry(line) for filename in options.input_cache for line in file(filename)])
@@ -94,7 +94,7 @@
 
 def parse_config_file(options):
 	if options.verbose:
-		print >>sys.stderr, "reading %s ..." % options.config_file
+		print("reading %s ..." % options.config_file, file=sys.stderr)
 	config = ConfigParser.SafeConfigParser()
 	config.read(options.config_file)
 
@@ -176,7 +176,7 @@
 round_robin_cache_nodes = [set() for cache in options.round_robin_cache]
 for seg in options.distribution_segments:
 	if options.verbose:
-		print >>sys.stderr, "generating distribution measurement jobs for %s ..." % str(seg)
+		print("generating distribution measurement jobs for %s ..." % str(seg), file=sys.stderr)
 	input_cache_nodes |= power.make_burca_tailor_fragment(dag, set([entry for entry in options.input_cache if entry.segmentlistdict.intersects_segment(seg)]), seg, "LIKELIHOOD_MAIN")
 	for i, (nodes, cache) in enumerate(zip(round_robin_cache_nodes, options.round_robin_cache)):
 		nodes |= power.make_burca_tailor_fragment(dag, set([entry for entry in cache if entry.segmentlistdict.intersects_segment(seg)]), seg, "LIKELIHOOD_RR%02d" % i)
@@ -188,7 +188,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "generating likelihood assignment jobs for main group ..."
+	print("generating likelihood assignment jobs for main group ...", file=sys.stderr)
 parents = reduce(lambda a, b: a | b, round_robin_cache_nodes, input_cache_nodes)
 nodes = power.make_burca2_fragment(dag, options.input_cache, parents, "LIKELIHOOD_MAIN")
 
@@ -201,7 +201,7 @@
 
 for i, (parents, apply_to_cache) in enumerate(round_robin(round_robin_cache_nodes, options.round_robin_cache)):
 	if options.verbose:
-		print >>sys.stderr, "generating likelihood assignment jobs for round-robin group %d ..." % i
+		print("generating likelihood assignment jobs for round-robin group %d ..." % i, file=sys.stderr)
 	nodes |= power.make_burca2_fragment(dag, apply_to_cache, parents | input_cache_nodes, "LIKELIHOOD_RR%02d" % i)
 
 
@@ -211,6 +211,6 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "writing dag ..."
+	print("writing dag ...", file=sys.stderr)
 dag.write_sub_files()
 dag.write_dag()
--- ./src/power/lalapps_power_pipe.py	(original)
+++ ./src/power/lalapps_power_pipe.py	(refactored)
@@ -78,14 +78,14 @@
 	options, filenames = parser.parse_args()
 
 	if options.variant not in ("injections", "noninjections", "both"):
-		raise ValueError, "unrecognized --variant %s" % options.variant
+		raise ValueError("unrecognized --variant %s" % options.variant)
 	options.do_injections = options.variant in ("injections", "both")
 	options.do_noninjections = options.variant in ("noninjections", "both")
 
 	if options.do_injections and not options.injection_time_slides:
-		raise ValueError, "missing required --injection-time-slides argument"
+		raise ValueError("missing required --injection-time-slides argument")
 	if options.do_noninjections and not options.background_time_slides:
-		raise ValueError, "missing required --background-time-slides argument"
+		raise ValueError("missing required --background-time-slides argument")
 
 	# simplifies life later by allowing the background and injection
 	# branches of the dag to be constructed with nearly identical code
@@ -105,7 +105,7 @@
 
 def parse_config_file(options):
 	if options.verbose:
-		print >>sys.stderr, "reading %s ..." % options.config_file
+		print("reading %s ..." % options.config_file, file=sys.stderr)
 	config = ConfigParser.SafeConfigParser()
 	config.read(options.config_file)
 
@@ -141,7 +141,7 @@
 
 def compute_segment_lists(seglistdict, time_slides, minimum_gap, timing_params, full_segments = True, verbose = False):
 	if verbose:
-		print >>sys.stderr, "constructing segment list ..."
+		print("constructing segment list ...", file=sys.stderr)
 
 	seglistdict = seglistdict.copy()
 
@@ -253,7 +253,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "Computing segments for which lalapps_power jobs are required ..."
+	print("Computing segments for which lalapps_power jobs are required ...", file=sys.stderr)
 
 background_time_slides = {}
 background_seglistdict = segments.segmentlistdict()
@@ -318,7 +318,7 @@
 	if do_injections:
 		assert len(time_slides) == 1
 		if verbose:
-			print >>sys.stderr, "Building lalapps_binj jobs ..."
+			print("Building lalapps_binj jobs ...", file=sys.stderr)
 		binjnodes = power.make_binj_fragment(dag, seglistdict.extent_all(), time_slides.keys()[0], tag, 0.0, float(power.powerjob.get_opts()["low-freq-cutoff"]), float(power.powerjob.get_opts()["low-freq-cutoff"]) + float(power.powerjob.get_opts()["bandwidth"]))
 		# add binj nodes as parents of the datafinds to force the binj's to
 		# be run first.  this ensures that once a datafind has run the
@@ -337,7 +337,7 @@
 	trigger_nodes = power.make_single_instrument_stage(dag, datafinds, seglistdict, tag, timing_params, psds_per_power, binjnodes = binjnodes, verbose = verbose)
 	if enable_clustering:
 		if verbose:
-			print >>sys.stderr, "building pre-lladd bucluster jobs ..."
+			print("building pre-lladd bucluster jobs ...", file=sys.stderr)
 		trigger_nodes = power.make_bucluster_fragment(dag, trigger_nodes, "PRELLADD_%s" % tag, verbose = verbose)
 
 
@@ -351,7 +351,7 @@
 	assert len(binj_cache) < 2
 	for n, (time_slides_cache_entry, these_time_slides) in enumerate(time_slides.items()):
 		if verbose:
-			print >>sys.stderr, "%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path)
+			print("%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path), file=sys.stderr)
 		tisi_cache = set([time_slides_cache_entry])
 		if do_injections:
 			# lalapps_binj has already copied the time slide
@@ -366,13 +366,13 @@
 			nodes |= power.make_lladd_fragment(dag, parents | binjnodes, "%s_%d" % (tag, n), segment = seg, input_cache = cache | binj_cache, extra_input_cache = extra_input_cache, remove_input = do_injections, preserve_cache = binj_cache | tisi_cache)
 		if enable_clustering:
 			if verbose:
-				print >>sys.stderr, "building post-lladd bucluster jobs ..."
+				print("building post-lladd bucluster jobs ...", file=sys.stderr)
 			nodes = power.make_bucluster_fragment(dag, nodes, "POSTLLADD_%s_%d" % (tag, n), verbose = verbose)
 		if verbose:
-			print >>sys.stderr, "building burca jobs ..."
+			print("building burca jobs ...", file=sys.stderr)
 		coinc_nodes |= power.make_burca_fragment(dag, nodes, "%s_%d" % (tag, n), verbose = verbose)
 		if verbose:
-			print >>sys.stderr, "done %s %d/%d" % (tag, n + 1, len(time_slides))
+			print("done %s %d/%d" % (tag, n + 1, len(time_slides)), file=sys.stderr)
 
 
 	# injection identification
@@ -380,7 +380,7 @@
 
 	if do_injections:
 		if verbose:
-			print >>sys.stderr, "building binjfind jobs ..."
+			print("building binjfind jobs ...", file=sys.stderr)
 		coinc_nodes = power.make_binjfind_fragment(dag, coinc_nodes, tag, verbose = verbose)
 
 
@@ -388,7 +388,7 @@
 
 
 	if verbose:
-		print >>sys.stderr, "building sqlite jobs ..."
+		print("building sqlite jobs ...", file=sys.stderr)
 	coinc_nodes = power.make_sqlite_fragment(dag, coinc_nodes, tag, verbose = verbose)
 
 
@@ -409,6 +409,6 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "writing dag ..."
+	print("writing dag ...", file=sys.stderr)
 dag.write_sub_files()
 dag.write_dag()
--- ./src/power/lalapps_power_plot_binj.py	(original)
+++ ./src/power/lalapps_power_plot_binj.py	(refactored)
@@ -100,7 +100,7 @@
 	filenames = filenames or []
 	for cache in options.input_cache:
 		if options.verbose:
-			print >>sys.stderr, "reading '%s' ..." % cache
+			print("reading '%s' ..." % cache, file=sys.stderr)
 		filenames += [CacheEntry(line).path for line in file(cache)]
 
 	return options, filenames
@@ -722,7 +722,7 @@
 
 options, filenames = parse_command_line()
 if not options.plot and not options.coinc_plot or not filenames:
-	print >>sys.stderr, "Nothing to do!"
+	print("Nothing to do!", file=sys.stderr)
 	sys.exit(0)
 
 
@@ -736,7 +736,7 @@
 
 for n, filename in enumerate(utils.sort_files_by_size(filenames, options.verbose, reverse = True)):
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	database = SnglBurstUtils.CoincDatabase(sqlite3.connect(working_filename), options.live_time_program)
 	if options.verbose:
@@ -764,7 +764,7 @@
 				plots[instrument] = new_plots(instrument, options.amplitude_func, options.amplitude_lbl, options.plot)
 			for n, plot in zip(options.plot, plots[instrument]):
 				if options.verbose:
-					print >>sys.stderr, "adding to %s plot %d ..." % (instrument, n)
+					print("adding to %s plot %d ..." % (instrument, n), file=sys.stderr)
 				plot.add_contents(database)
 	if options.coinc_plot:
 		database.connection.cursor().execute("""
@@ -785,7 +785,7 @@
 		""")
 		for n, plot in enumerate(coincplots):
 			if options.verbose:
-				print >>sys.stderr, "adding to coinc plot %d ..." % options.coinc_plot[n]
+				print("adding to coinc plot %d ..." % options.coinc_plot[n], file=sys.stderr)
 			plot.add_contents(database)
 	database.connection.close()
 	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)
@@ -823,7 +823,7 @@
 		plot = plots[instrument].pop(0)
 		filename = format % (options.base, options.plot[n], options.format)
 		if options.verbose:
-			print >>sys.stderr, "finishing %s plot %d ..." % (instrument, options.plot[n])
+			print("finishing %s plot %d ..." % (instrument, options.plot[n]), file=sys.stderr)
 		try:
 			if isinstance(plot, SimBurstUtils.Efficiency_hrss_vs_freq):
 				plot.finish(binning = binning)
@@ -833,10 +833,10 @@
 				plot.finish()
 				fig = plot.fig
 		except ValueError as e:
-			print >>sys.stderr, "can't finish %s plot %d: %s" % (instrument, options.plot[n], str(e))
+			print("can't finish %s plot %d: %s" % (instrument, options.plot[n], str(e)), file=sys.stderr)
 		else:
 			if options.verbose:
-				print >>sys.stderr, "writing %s ..." % filename
+				print("writing %s ..." % filename, file=sys.stderr)
 			fig.savefig(filename)
 		n += 1
 
@@ -850,7 +850,7 @@
 	format = "%%s%s_%%0%dd.%%s" % ("coinc", int(math.log10(max(options.coinc_plot) or 1)) + 1)
 	filename = format % (options.base, options.coinc_plot[n], options.format)
 	if options.verbose:
-		print >>sys.stderr, "finishing coinc plot %d ..." % options.coinc_plot[n]
+		print("finishing coinc plot %d ..." % options.coinc_plot[n], file=sys.stderr)
 	try:
 		if isinstance(plot, SimBurstUtils.Efficiency_hrss_vs_freq):
 			plot.finish(binning = binning)
@@ -859,10 +859,10 @@
 			plot.finish()
 			fig = plot.fig
 	except ValueError as e:
-		print >>sys.stderr, "can't finish coinc plot %d: %s" % (options.coinc_plot[n], str(e))
+		print("can't finish coinc plot %d: %s" % (options.coinc_plot[n], str(e)), file=sys.stderr)
 	else:
 		if options.verbose:
-			print >>sys.stderr, "writing %s ..." % filename
+			print("writing %s ..." % filename, file=sys.stderr)
 		fig.savefig(filename)
 
 
@@ -925,9 +925,9 @@
 
 if efficiencies:
 	if options.verbose:
-		print >>sys.stderr, "computing theoretical coincident detection efficiency ..."
+		print("computing theoretical coincident detection efficiency ...", file=sys.stderr)
 	fig = plot_multi_Efficiency_hrss_vs_freq(efficiencies)
 	filename = "%scoincidence.png" % options.base
 	if options.verbose:
-		print >>sys.stderr, "writing %s ..." % filename
+		print("writing %s ..." % filename, file=sys.stderr)
 	fig.savefig(filename)
--- ./src/power/lalapps_power_plot_binjtf.py	(original)
+++ ./src/power/lalapps_power_plot_binjtf.py	(refactored)
@@ -290,7 +290,7 @@
 
 for n, filename in enumerate(filenames):
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 	database = SnglBurstUtils.CoincDatabase(sqlite3.connect(filename), "lalapps_power")
 	if options.verbose:
 		SnglBurstUtils.summarize_coinc_database(database)
@@ -298,6 +298,6 @@
 		for sim in found_injections(database, instrument):
 			plotname = "%s%d_%s.%s" % (options.base, sim.time_at_instrument(instrument).seconds, instrument, options.format)
 			if options.verbose:
-				print >>sys.stderr, "--> %s" % plotname
+				print("--> %s" % plotname, file=sys.stderr)
 			time_freq_plot(database, instrument, sim).savefig(plotname)
 	database.connection.close()
--- ./src/power/lalapps_power_plot_burca.py	(original)
+++ ./src/power/lalapps_power_plot_burca.py	(refactored)
@@ -587,7 +587,7 @@
 
 for n, filename in enumerate(ligolw_utils.sort_files_by_size(filenames, options.verbose, reverse = True)[options.skip:]):
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames) - options.skip, filename)
+		print("%d/%d: %s" % (n + 1, len(filenames) - options.skip, filename), file=sys.stderr)
 
 	database = SnglBurstUtils.CoincDatabase(sqlite3.connect(filename), options.live_time_program)
 	if options.verbose:
@@ -595,7 +595,7 @@
 
 	for n, plot in zip(options.plot, plots):
 		if options.verbose:
-			print >>sys.stderr, "adding to burca plot %d ..." % n
+			print("adding to burca plot %d ..." % n, file=sys.stderr)
 		plot.add_contents(database)
 
 	database.connection.close()
@@ -607,13 +607,13 @@
 while len(plots):
 	filename = format % (options.base, options.plot[n], options.format)
 	if options.verbose:
-		print >>sys.stderr, "finishing plot %d ..." % options.plot[n]
+		print("finishing plot %d ..." % options.plot[n], file=sys.stderr)
 	plots[0].finish()
 	if options.verbose:
-		print >>sys.stderr, "writing %s ..." % filename
+		print("writing %s ..." % filename, file=sys.stderr)
 	plots[0].fig.savefig(filename)
 	del plots[0]
 	n += 1
 
 if options.verbose:
-	print >>sys.stderr, "done."
+	print("done.", file=sys.stderr)
--- ./src/power/lalapps_power_plot_burca2.py	(original)
+++ ./src/power/lalapps_power_plot_burca2.py	(refactored)
@@ -261,19 +261,19 @@
 
 distributions, seglists = burca_tailor.EPGalacticCoreCoincParamsDistributions.from_filenames(filenames, u"lalapps_burca_tailor", verbose = options.verbose)
 if options.verbose:
-	print >>sys.stderr, "applying filters ..."
+	print("applying filters ...", file=sys.stderr)
 distributions.finish()
 
 
 for gmst in numpy.arange(0.0, 2 * math.pi, 2 * math.pi / 10):
 	filename = "%s-%%s-%d-%d-%s.%s" % (options.base, int(seglists.extent_all()[0]), int(abs(seglists.extent_all())), ("%.2f" % gmst).replace(".", "_"), options.format)
 	if options.verbose:
-		print >>sys.stderr, "writing %s ..." % (filename % "P")
+		print("writing %s ..." % (filename % "P"), file=sys.stderr)
 	plot_coinc_params(distributions, gmst, plottype = "P", with_zero_lag = options.with_zero_lag).savefig(filename % "P")
 	if options.verbose:
-		print >>sys.stderr, "writing %s ..." % (filename % "LR")
+		print("writing %s ..." % (filename % "LR"), file=sys.stderr)
 	plot_coinc_params(distributions, gmst, plottype = "LR").savefig(filename % "LR")
 
 
 if options.verbose:
-	print >>sys.stderr, "done."
+	print("done.", file=sys.stderr)
--- ./src/power/lalapps_power_plot_burst.py	(original)
+++ ./src/power/lalapps_power_plot_burst.py	(refactored)
@@ -473,7 +473,7 @@
 
 for n, filename in enumerate(ligolw_utils.sort_files_by_size(filenames, options.verbose, reverse = True)):
 	if options.verbose:
-		print >>sys.stderr, "%d/%d:" % (n + 1, len(filenames)),
+		print("%d/%d:" % (n + 1, len(filenames)), end=' ', file=sys.stderr)
 	xmldoc = ligolw_utils.load_filename(filename, verbose = options.verbose, contenthandler = ligolw.LIGOLWContentHandler)
 	seglists |= ligolw_search_summary.segmentlistdict_fromsearchsummary(xmldoc, options.livetime_program).coalesce()
 	xmldoc.unlink()
@@ -507,11 +507,11 @@
 	for plotnum, plot in zip(options.plot, new_plots(ifo, options.plot)):
 		filename = format % (options.base, plotnum, options.format)
 		if options.verbose:
-			print >>sys.stderr, "adding to %s plot %d ..." % (ifo, plotnum)
+			print("adding to %s plot %d ..." % (ifo, plotnum), file=sys.stderr)
 		plot.add_contents(summary, seglists)
 		if options.verbose:
-			print >>sys.stderr, "finishing %s plot %d ..." % (ifo, plotnum)
+			print("finishing %s plot %d ..." % (ifo, plotnum), file=sys.stderr)
 		plot.finish()
 		if options.verbose:
-			print >>sys.stderr, "writing %s ..." % filename
+			print("writing %s ..." % filename, file=sys.stderr)
 		plot.fig.savefig(filename)
--- ./src/power/lalapps_power_veto.py	(original)
+++ ./src/power/lalapps_power_veto.py	(refactored)
@@ -82,14 +82,14 @@
 
 def load_segments(filename, name, verbose = False):
 	if verbose:
-		print >>sys.stderr, "loading \"%s\" segments ... " % name,
+		print("loading \"%s\" segments ... " % name, end=' ', file=sys.stderr)
 	connection = sqlite3.connect(filename)
 	segs = ligolw_segments.segmenttable_get_by_name(dbtables.get_xml(connection), name).coalesce()
 	connection.close()
 	if verbose:
-		print >>sys.stderr, "done."
+		print("done.", file=sys.stderr)
 		for ifo in segs:
-			print >>sys.stderr, "loaded %d veto segment(s) for %s totalling %g s" % (len(segs[ifo]), ifo, float(abs(segs[ifo])))
+			print("loaded %d veto segment(s) for %s totalling %g s" % (len(segs[ifo]), ifo, float(abs(segs[ifo]))), file=sys.stderr)
 	return segs
 
 
@@ -117,8 +117,8 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "applying excess power event veto strategy:"
-		print >>sys.stderr, "\tremoving vetoed burst <--> burst coincs ..."
+		print("applying excess power event veto strategy:", file=sys.stderr)
+		print("\tremoving vetoed burst <--> burst coincs ...", file=sys.stderr)
 	cursor.execute("""
 DELETE FROM
 	coinc_event
@@ -144,7 +144,7 @@
 
 	if contents.sc_definer_id is not None:
 		if verbose:
-			print >>sys.stderr, "\tremoving vetoed sim <--> coinc coincs ..."
+			print("\tremoving vetoed sim <--> coinc coincs ...", file=sys.stderr)
 		cursor.execute("""
 DELETE FROM
 	coinc_event
@@ -172,7 +172,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "\tremoving vetoed bursts from coinc_def_map table ..."
+		print("\tremoving vetoed bursts from coinc_def_map table ...", file=sys.stderr)
 	cursor.execute("""
 DELETE FROM
 	coinc_event_map
@@ -196,7 +196,7 @@
 
 	if contents.sb_definer_id is not None:
 		if verbose:
-			print >>sys.stderr, "\tupdating sim <--> burst event counts ..."
+			print("\tupdating sim <--> burst event counts ...", file=sys.stderr)
 		cursor.execute("""
 UPDATE
 	coinc_event
@@ -206,7 +206,7 @@
 	coinc_event.coinc_def_id == ?
 		""", (contents.sb_definer_id,))
 		if verbose:
-			print >>sys.stderr, "\tremoving empty sim <--> burst coincs ..."
+			print("\tremoving empty sim <--> burst coincs ...", file=sys.stderr)
 		cursor.execute("""
 DELETE FROM
 	coinc_event
@@ -222,7 +222,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "\ttrimming coinc_event_map table ..."
+		print("\ttrimming coinc_event_map table ...", file=sys.stderr)
 	cursor.execute("""
 DELETE FROM
 	coinc_event_map
@@ -236,7 +236,7 @@
 	""")
 
 	if verbose:
-		print >>sys.stderr, "\ttrimming multi_burst table ..."
+		print("\ttrimming multi_burst table ...", file=sys.stderr)
 	cursor.execute("""
 DELETE FROM
 	multi_burst
@@ -255,7 +255,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "\tdone (excess power event vetos)."
+		print("\tdone (excess power event vetos).", file=sys.stderr)
 
 
 #
@@ -294,7 +294,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename),
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), end=' ', file=sys.stderr)
 
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
@@ -312,17 +312,17 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "committing ..."
+		print("committing ...", file=sys.stderr)
 	connection.commit()
 	if not options.no_vacuum:
 		if options.verbose:
-			print >>sys.stderr, "vacuuming ..."
+			print("vacuuming ...", file=sys.stderr)
 		connection.cursor().execute("VACUUM;")
 	connection.close()
 	del connection
 	dbtables.put_connection_filename(filename, working_filename, verbose = options.verbose)
 	if options.verbose:
-		print >>sys.stderr, "done (%s)." % filename
+		print("done (%s)." % filename, file=sys.stderr)
 
 
 #
@@ -331,4 +331,4 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "done."
+	print("done.", file=sys.stderr)
--- ./src/power/power.py	(original)
+++ ./src/power/power.py	(refactored)
@@ -87,7 +87,7 @@
 def make_dir_if_not_exists(dir):
 	try:
 		os.mkdir(dir)
-	except OSError, e:
+	except OSError as e:
 		if e.errno != errno.EEXIST:
 			# OK if directory exists, otherwise report error
 			raise e
@@ -217,7 +217,7 @@
 def write_output_cache(nodes, filename):
 	f = file(filename, "w")
 	for cache_entry, node in collect_output_caches(nodes):
-		print >>f, str(cache_entry)
+		print(str(cache_entry), file=f)
 
 
 #
@@ -308,7 +308,7 @@
 
 	def get_user_tag(self):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		return self.__usertag
 
 	def set_time_slide_file(self, filename):
@@ -319,7 +319,7 @@
 
 	def set_start(self, start):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		self.add_var_opt("gps-start-time", start)
 
 	def get_start(self):
@@ -327,7 +327,7 @@
 
 	def set_end(self, end):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		self.add_var_opt("gps-end-time", end)
 
 	def get_end(self):
@@ -350,7 +350,7 @@
 	def get_output(self):
 		if self._AnalysisNode__output is None:
 			if None in (self.get_start(), self.get_end(), self.__usertag):
-				raise ValueError, "start time, end time, ifo, or user tag has not been set"
+				raise ValueError("start time, end time, ifo, or user tag has not been set")
 			seg = segments.segment(lal.LIGOTimeGPS(self.get_start()), lal.LIGOTimeGPS(self.get_end()))
 			self.set_output(os.path.join(self.output_dir, "G1+H1+H2+L1+T1+V1-INJECTIONS_%s-%d-%d.xml.gz" % (self.__usertag, int(self.get_start()), int(self.get_end() - self.get_start()))))
 		return self._AnalysisNode__output
@@ -396,14 +396,14 @@
 		the config file.
 		"""
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		pipeline.AnalysisNode.set_ifo(self, instrument)
 		for optvalue in self.job()._AnalysisJob__cp.items("lalapps_power_%s" % instrument):
 			self.add_var_arg("--%s %s" % optvalue)
 
 	def set_user_tag(self, tag):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		self.__usertag = tag
 		self.add_var_opt("user-tag", self.__usertag)
 
@@ -426,7 +426,7 @@
 	def get_output(self):
 		if self._AnalysisNode__output is None:
 			if None in (self.get_start(), self.get_end(), self.get_ifo(), self.__usertag):
-				raise ValueError, "start time, end time, ifo, or user tag has not been set"
+				raise ValueError("start time, end time, ifo, or user tag has not been set")
 			seg = segments.segment(lal.LIGOTimeGPS(self.get_start()), lal.LIGOTimeGPS(self.get_end()))
 			self.set_output(os.path.join(self.output_dir, "%s-POWER_%s-%d-%d.xml.gz" % (self.get_ifo(), self.__usertag, int(self.get_start()), int(self.get_end()) - int(self.get_start()))))
 		return self._AnalysisNode__output
@@ -497,7 +497,7 @@
 	def write_input_files(self, *args):
 		f = file(self.cache_name, "w")
 		for c in self.input_cache:
-			print >>f, str(c)
+			print(str(c), file=f)
 		pipeline.LigolwAddNode.write_input_files(self, *args)
 
 	def get_output_files(self):
@@ -520,7 +520,7 @@
 
 		self.files_per_bucut = get_files_per_bucut(config_parser)
 		if self.files_per_bucut < 1:
-			raise ValueError, "files_per_bucut < 1"
+			raise ValueError("files_per_bucut < 1")
 
 
 class BucutNode(pipeline.CondorDAGNode):
@@ -568,7 +568,7 @@
 
 		self.files_per_bucluster = get_files_per_bucluster(config_parser)
 		if self.files_per_bucluster < 1:
-			raise ValueError, "files_per_bucluster < 1"
+			raise ValueError("files_per_bucluster < 1")
 
 
 class BuclusterNode(pipeline.CondorDAGNode):
@@ -593,7 +593,7 @@
 	def write_input_files(self, *args):
 		f = file(self.cache_name, "w")
 		for c in self.input_cache:
-			print >>f, str(c)
+			print(str(c), file=f)
 		pipeline.CondorDAGNode.write_input_files(self, *args)
 
 	def get_input_cache(self):
@@ -621,7 +621,7 @@
 
 		self.files_per_binjfind = get_files_per_binjfind(config_parser)
 		if self.files_per_binjfind < 1:
-			raise ValueError, "files_per_binjfind < 1"
+			raise ValueError("files_per_binjfind < 1")
 
 
 class BinjfindNode(pipeline.CondorDAGNode):
@@ -667,7 +667,7 @@
 
 		self.files_per_burca = get_files_per_burca(config_parser)
 		if self.files_per_burca < 1:
-			raise ValueError, "files_per_burca < 1"
+			raise ValueError("files_per_burca < 1")
 
 
 class Burca2Job(pipeline.CondorDAGJob):
@@ -746,7 +746,7 @@
 
 	def add_input_cache(self, cache):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		self.input_cache.extend(cache)
 		for c in cache:
 			filename = c.path
@@ -758,7 +758,7 @@
 
 	def set_output(self, filename):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		self.add_macro("macrodatabase", filename)
 
 	def get_input_cache(self):
@@ -805,7 +805,7 @@
 
 	def add_input_cache(self, cache):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		self.input_cache.extend(cache)
 		for c in cache:
 			filename = c.path
@@ -817,7 +817,7 @@
 
 	def set_output(self, description):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		cache_entry = make_cache_entry(self.input_cache, description, "")
 		filename = os.path.join(self.output_dir, "%s-%s-%d-%d.xml.gz" % (cache_entry.observatory, cache_entry.description, int(cache_entry.segment[0]), int(abs(cache_entry.segment))))
 		self.add_var_opt("output", filename)
@@ -831,7 +831,7 @@
 
 	def get_output_cache(self):
 		if not self.output_cache:
-			raise AttributeError, "must call set_output(description) first"
+			raise AttributeError("must call set_output(description) first")
 		return self.output_cache
 
 	def write_input_files(self, *args):
@@ -840,7 +840,7 @@
 			if "--add-from-cache" in arg:
 				f = file(self.cache_name, "w")
 				for c in self.input_cache:
-					print >>f, str(c)
+					print(str(c), file=f)
 				pipeline.CondorDAGNode.write_input_files(self, *args)
 				break
 
@@ -949,7 +949,7 @@
 	seconds.  In general, the return value is a non-integer.
 	"""
 	if t < 0:
-		raise ValueError, t
+		raise ValueError(t)
 	# convert to samples, and remove filter corruption
 	t = t * timing_params.resample_rate - 2 * timing_params.filter_corruption
 	if t < timing_params.psd_length:
@@ -963,7 +963,7 @@
 	of the job in seconds.
 	"""
 	if psds < 1:
-		raise ValueError, psds
+		raise ValueError(psds)
 	# number of samples
 	result = (psds - 1) * timing_params.psd_shift + timing_params.psd_length
 	# add filter corruption
@@ -1254,7 +1254,7 @@
 	likelihood_data_cache_filename = os.path.join(burca2job.cache_dir, "burca2_%s.cache" % tag)
 	likelihood_data_cache_file = file(likelihood_data_cache_filename, "w")
 	for cache_entry in [cache_entry for node in likelihood_parents for cache_entry in node.get_output_cache()]:
-		print >>likelihood_data_cache_file, str(cache_entry)
+		print(str(cache_entry), file=likelihood_data_cache_file)
 
 	nodes = set()
 	max_cost_per_job = 10	# 10000 s -equivalent files
@@ -1288,7 +1288,7 @@
 
 def make_datafind_stage(dag, seglists, verbose = False):
 	if verbose:
-		print >>sys.stderr, "building ligo_data_find jobs ..."
+		print("building ligo_data_find jobs ...", file=sys.stderr)
 
 	#
 	# Fill gaps smaller than the padding added to each datafind job.
@@ -1310,7 +1310,7 @@
 	nodes = set()
 	for seg, instrument in segs:
 		if verbose:
-			print >>sys.stderr, "making datafind job for %s spanning %s" % (instrument, seg)
+			print("making datafind job for %s spanning %s" % (instrument, seg), file=sys.stderr)
 		new_nodes = make_datafind_fragment(dag, instrument, seg)
 		nodes |= new_nodes
 
@@ -1352,7 +1352,7 @@
 		injargs = {}
 	seglist = split_segment(timing_params, segment, psds_per_job)
 	if verbose:
-		print >>sys.stderr, "Segment split: " + str(seglist)
+		print("Segment split: " + str(seglist), file=sys.stderr)
 	nodes = set()
 	for seg in seglist:
 		nodes |= make_power_fragment(dag, datafindnodes | binjnodes, instrument, seg, tag, framecache, injargs = injargs)
@@ -1369,12 +1369,12 @@
 	for instrument, seglist in seglistdict.iteritems():
 		for seg in seglist:
 			if verbose:
-				print >>sys.stderr, "generating %s fragment %s" % (instrument, str(seg))
+				print("generating %s fragment %s" % (instrument, str(seg)), file=sys.stderr)
 
 			# find the datafind job this job is going to need
 			dfnodes = set([node for node in datafinds if (node.get_ifo() == instrument) and (seg in segments.segment(node.get_start(), node.get_end()))])
 			if len(dfnodes) != 1:
-				raise ValueError, "error, not exactly 1 datafind is suitable for trigger generator job at %s in %s" % (str(seg), instrument)
+				raise ValueError("error, not exactly 1 datafind is suitable for trigger generator job at %s in %s" % (str(seg), instrument))
 
 			# trigger generator jobs
 			nodes += make_power_segment_fragment(dag, dfnodes, instrument, seg, tag, timing_params, psds_per_job, binjnodes = binjnodes, verbose = verbose)
@@ -1398,7 +1398,7 @@
 		return []
 
 	if verbose:
-		print >>sys.stderr, "Grouping jobs for coincidence analysis:"
+		print("Grouping jobs for coincidence analysis:", file=sys.stderr)
 
 	#
 	# use ligolw_cafe to group each output file according to how they
@@ -1444,13 +1444,13 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "Matching jobs to caches ..."
+		print("Matching jobs to caches ...", file=sys.stderr)
 	parent_groups, unused = match_nodes_to_caches(parents, caches)
 	if verbose and unused:
 		# there were parents that didn't match any caches.  this
 		# happens if ligolw_cafe decides their outputs aren't
 		# needed
-		print >>sys.stderr, "Notice:  %d jobs (of %d) produce output that will not be used by a coincidence job" % (unused, len(parents))
+		print("Notice:  %d jobs (of %d) produce output that will not be used by a coincidence job" % (unused, len(parents)), file=sys.stderr)
 
 	#
 	# done
--- ./src/pulsar/CrossCorr/combine_crosscorr_toplists.py	(original)
+++ ./src/pulsar/CrossCorr/combine_crosscorr_toplists.py	(refactored)
@@ -88,5 +88,5 @@
 for line in data:
         outfile.write("%.10f %.10f %.10g %.10f %.10g %.5f %.10g %.10g %.10g\n"
                       % tuple(line))
-print "Highest discarded candidate SNR was %f" % dropped_rho
+print("Highest discarded candidate SNR was %f" % dropped_rho)
 outfile.close()
--- ./src/pulsar/HeterodyneSearch/pulsarhtmlutils.py	(original)
+++ ./src/pulsar/HeterodyneSearch/pulsarhtmlutils.py	(refactored)
@@ -548,7 +548,7 @@
     Delete a row
     """
     if rowidx > self._nrows-1:
-      print("Warning... cannot delete row '%d'. Only %d row in table." % (rowdix, self._nrows))
+      print(("Warning... cannot delete row '%d'. Only %d row in table." % (rowdix, self._nrows)))
     else:
       self._rows.pop(rowidx) # remove row
       self._nrows -= 1
@@ -724,7 +724,7 @@
 # convert a floating point number into a string in X.X x 10^Z format
 def exp_str(f, p=1, otype='html'):
   if p > 16:
-    print >> sys.stderr, "Precision must be less than 16 d.p."
+    print("Precision must be less than 16 d.p.", file=sys.stderr)
     p = 16
 
   s = '%.16e' % f
--- ./src/pulsar/MakeSFTs/lalapps_MakeSFTDAG.py	(original)
+++ ./src/pulsar/MakeSFTs/lalapps_MakeSFTDAG.py	(refactored)
@@ -95,7 +95,7 @@
   -H, --use-hot              (optional) input data is from h(t) calibrated frames (h of t = hot!) (0 or 1).
   -u  --frame-struct-type    (optional) string specifying the input frame structure and data type. Must begin with ADC_ or PROC_ followed by REAL4, REAL8, INT2, INT4, or INT8; default: ADC_REAL4; -H is the same as PROC_REAL8.
 """
-  print >> sys.stdout, msg
+  print(msg, file=sys.stdout)
 
 #
 # FUNCTION THAT WRITE ONE JOB TO DAG FILE
@@ -315,103 +315,103 @@
   elif o in ("-Z", "--make-tmp-file"):
     makeTmpFile = True
   else:
-    print >> sys.stderr, "Unknown option:", o
+    print("Unknown option:", o, file=sys.stderr)
     usage()
     sys.exit(1)
 
 if not dagFileName:
-  print >> sys.stderr, "No dag filename specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No dag filename specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not tagString:
-  print >> sys.stderr, "No tag string specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No tag string specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not inputDataType:
-  print >> sys.stderr, "No input data type specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No input data type specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if extraDatafindTime < 0L:
-  print >> sys.stderr, "Invalid extra datafind time specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid extra datafind time specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if synchronizeStart<0 or synchronizeStart>1:
-  print >> sys.stderr, "Invalid use of synchronize-start."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid use of synchronize-start.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
   
 if filterKneeFreq < 0:
-  print >> sys.stderr, "No filter knee frequency specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No filter knee frequency specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not timeBaseline:
-  print >> sys.stderr, "No time baseline specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No time baseline specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not outputSFTPath:
-  print >> sys.stderr, "No output SFT path specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No output SFT path specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
   
 if not cachePath:
-  print >> sys.stderr, "No cache path specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No cache path specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not logPath:
-  print >> sys.stderr, "No log path specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No log path specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not subLogPath:
-  print >> sys.stderr, "No sub log path specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No sub log path specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not channelName:
-  print >> sys.stderr, "No channel name specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No channel name specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if (windowType != 0) and (windowType != 1) and (windowType != 2) and (windowType != 3):
-  print >> sys.stderr, "Invalid window type specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid window type specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if (overlapFraction < 0.0) or (overlapFraction >= 1.0):
-  print >> sys.stderr, "Invalid make overlap fraction specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid make overlap fraction specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if (sftVersion != 1) and (sftVersion != 2):
-  print >> sys.stderr, "Invalid SFT version specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid SFT version specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if (startFreq < 0.0):
-  print >> sys.stderr, "Invalid start freq specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid start freq specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if (freqBand < 0.0):
-  print >> sys.stderr, "Invalid band specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid band specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if (makeGPSDirs < 0) or (makeGPSDirs > 10):
-  print >> sys.stderr, "Invalid make gps dirs specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("Invalid make gps dirs specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 if not maxNumPerNode:
-  print >> sys.stderr, "No maximum number of SFTs per node specified."
-  print >> sys.stderr, "Use --help for usage details."
+  print("No maximum number of SFTs per node specified.", file=sys.stderr)
+  print("Use --help for usage details.", file=sys.stderr)
   sys.exit(1)
 
 # try and make a directory to store the cache files and job logs
@@ -425,11 +425,11 @@
 if (nodeListFile != None):
 
     if not nodePath:
-       print >> sys.stderr, "Node file list given, but no node path specified."
+       print("Node file list given, but no node path specified.", file=sys.stderr)
        sys.exit(1)
     
     if (outputJobsPerNode < 1L):
-       print >> sys.stderr, "Node file list given, but invalid output jobs per node specified."
+       print("Node file list given, but invalid output jobs per node specified.", file=sys.stderr)
        sys.exit(1)
 
     try:
@@ -439,10 +439,10 @@
              nodeList.append(splitLine[0])
          # End for line in open(nodeListFile)
          if (len(nodeList) < 1):
-             print >> sys.stderr, "No nodes found in node list file: %s." % nodeListFile
+             print("No nodes found in node list file: %s." % nodeListFile, file=sys.stderr)
              sys.exit(1)
     except:
-         print >> sys.stderr, "Error reading or parsing node list file: %s." % nodeListFile
+         print("Error reading or parsing node list file: %s." % nodeListFile, file=sys.stderr)
          sys.exit(1)
 
     # Set flag to use list of nodes in constructing output files
@@ -456,8 +456,8 @@
 if (segmentFile != None):
 
     if minSegLength < 0L:
-      print >> sys.stderr, "Invalid minimum segment length specified."
-      print >> sys.stderr, "Use --help for usage details."
+      print("Invalid minimum segment length specified.", file=sys.stderr)
+      print("Use --help for usage details.", file=sys.stderr)
       sys.exit(1)
 
     # the next flag causes extra time that cannot be processes to be trimmed from the start and end of a segment
@@ -480,25 +480,25 @@
                  pass
          # End for line in open(segmentFile)
          if (len(segList) < 1):
-             print >> sys.stderr, "No segments found in segment file: %s." % segmentFile
+             print("No segments found in segment file: %s." % segmentFile, file=sys.stderr)
              sys.exit(1)
     except:
-         print >> sys.stderr, "Error reading or parsing segment file: %s." % segmentFile
+         print("Error reading or parsing segment file: %s." % segmentFile, file=sys.stderr)
          sys.exit(1)
 else:
     if not analysisStartTime:
-      print >> sys.stderr, "No GPS analysis start time specified."
-      print >> sys.stderr, "Use --help for usage details."
+      print("No GPS analysis start time specified.", file=sys.stderr)
+      print("Use --help for usage details.", file=sys.stderr)
       sys.exit(1)
 
     if not analysisEndTime:
-      print >> sys.stderr, "No GPS analysis end time specified."
-      print >> sys.stderr, "Use --help for usage details."
+      print("No GPS analysis end time specified.", file=sys.stderr)
+      print("Use --help for usage details.", file=sys.stderr)
       sys.exit(1)
 
     if not maxLengthAllJobs:
-      print >> sys.stderr, "No maximum length of all jobs specified."
-      print >> sys.stderr, "Use --help for usage details."
+      print("No maximum length of all jobs specified.", file=sys.stderr)
+      print("Use --help for usage details.", file=sys.stderr)
       sys.exit(1)
 
     # Make sure not to exceed maximum allow analysis
@@ -511,7 +511,7 @@
         oneSeg.append(analysisEndTime);
         segList.append(oneSeg);
     except:
-        print >> sys.stderr, "There was a problem setting up the segment to run on: [%s, %s)." % (analysisStartTime,analysisEndTime)
+        print("There was a problem setting up the segment to run on: [%s, %s)." % (analysisStartTime,analysisEndTime), file=sys.stderr)
         sys.exit(1)
     
 # END if (segmentFile != None)
@@ -670,13 +670,13 @@
 endTimeAllNodes = endTimeThisNode
 
 if not startTimeAllNodes:
-  print >> sys.stderr, "The startTimeAllNodes == none; the DAG file contains no jobs!"
+  print("The startTimeAllNodes == none; the DAG file contains no jobs!", file=sys.stderr)
   sys.exit(1)
 
 if (endTimeAllNodes <= startTimeAllNodes):
-  print >> sys.stderr, "The endTimeAllNodes <= startTimeAllNodes; the DAG file contains no jobs!"
-  sys.exit(1)
-
-print >> sys.stdout, startTimeAllNodes, endTimeAllNodes
+  print("The endTimeAllNodes <= startTimeAllNodes; the DAG file contains no jobs!", file=sys.stderr)
+  sys.exit(1)
+
+print(startTimeAllNodes, endTimeAllNodes, file=sys.stdout)
 
 sys.exit(0)
--- ./src/ring/lalapps_ring_pipe.py	(original)
+++ ./src/ring/lalapps_ring_pipe.py	(refactored)
@@ -38,7 +38,7 @@
   -f, --config-file FILE   use configuration file FILE
   -l, --log-path PATH      directory to write condor log file
 """
-  print >> sys.stderr, msg
+  print(msg, file=sys.stderr)
 
 # pasrse the command line options to figure out what we should do
 shortop = "hvdrj:u:P:f:l:"
@@ -73,7 +73,7 @@
 
 for o, a in opts:
   if o in ("-v", "--version"):
-    print "$Id$"
+    print("$Id$")
     sys.exit(0)
   elif o in ("-h", "--help"):
     usage()
@@ -95,18 +95,18 @@
   elif o in ("-l", "--log-path"):
     log_path = a
   else:
-    print >> sys.stderr, "Unknown option:", o
+    print("Unknown option:", o, file=sys.stderr)
     usage()
     sys.exit(1)
 
 if not config_file:
-  print >> sys.stderr, "No configuration file specified."
-  print >> sys.stderr, "Use --config-file FILE to specify location."
+  print("No configuration file specified.", file=sys.stderr)
+  print("Use --config-file FILE to specify location.", file=sys.stderr)
   sys.exit(1)
 
 if not log_path:
-  print >> sys.stderr, "No log file path specified."
-  print >> sys.stderr, "Use --log-path PATH to specify a location."
+  print("No log file path specified.", file=sys.stderr)
+  print("Use --log-path PATH to specify a location.", file=sys.stderr)
   sys.exit(1)
 
 # try and make a directory to store the cache files and job logs
@@ -226,9 +226,9 @@
 dag.write_dag()
 
 # write a message telling the user that the DAG has been written
-print "\nCreated a DAG file which can be submitted by executing"
-print "\n   condor_submit_dag", dag.get_dag_file()
-print """\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
+print("\nCreated a DAG file which can be submitted by executing")
+print("\n   condor_submit_dag", dag.get_dag_file())
+print("""\nfrom a condor submit machine (e.g. hydra.phys.uwm.edu)\n
 If you are running LSCdataFind jobs, do not forget to initialize your grid 
 proxy certificate on the condor submit machine by running the commands
 
@@ -251,7 +251,7 @@
 
 Contact the administrator of your cluster to find the hostname and port of the
 LSCdataFind server.
-"""
+""")
 
 # write out a log file for this script
 if usertag:
@@ -271,14 +271,14 @@
 for seg in data:
   for chunk in seg:
     total_data += len(chunk)
-print >> log_fh, "total data =", total_data
-
-print >> log_fh, "\n===========================================\n"
-print >> log_fh, data
+print("total data =", total_data, file=log_fh)
+
+print("\n===========================================\n", file=log_fh)
+print(data, file=log_fh)
 for seg in data:
-  print >> log_fh, seg
+  print(seg, file=log_fh)
   for chunk in seg:
-    print >> log_fh, chunk
+    print(chunk, file=log_fh)
 
 sys.exit(0)
 
--- ./src/ring/lalapps_ringcorse.py	(original)
+++ ./src/ring/lalapps_ringcorse.py	(refactored)
@@ -88,9 +88,9 @@
 	#
 
 	if options.categories not in ("frequency-ifos-oninstruments", "oninstruments"):
-		raise ValueError, "missing or unrecognized --categories option"
+		raise ValueError("missing or unrecognized --categories option")
 	if options.rank_by not in ("snr", "uncombined-ifar", "likelihood"):
-		raise ValueError, "missing or unrecognized --rank-by option"
+		raise ValueError("missing or unrecognized --rank-by option")
 
 
 	options.populate_column = "false_alarm_rate"
@@ -101,10 +101,10 @@
 
 	if options.categories in ("frequency-ifos-oninstruments",):
 		if options.frequency_bins is None:
-			raise ValueError, "--frequency-bins required with category algorithm \"%s\"" % options.categories
+			raise ValueError("--frequency-bins required with category algorithm \"%s\"" % options.categories)
 		options.frequency_bins = sorted(map(float, options.frequency_bins.split(",")))
 		if len(options.frequency_bins) < 2:
-			raise ValueError, "must set at least two frequency bin boundaries (i.e., define at least one frequency bin)"
+			raise ValueError("must set at least two frequency bin boundaries (i.e., define at least one frequency bin)")
 		options.frequency_bins = rate.IrregularBins(options.frequency_bins)
 
 	#
@@ -112,7 +112,7 @@
 	#
 
 	if options.live_time_program is None:
-		raise ValueError, "missing required option -p or --live-time-program"
+		raise ValueError("missing required option -p or --live-time-program")
 
 	#
 	# done
@@ -140,9 +140,9 @@
 	#
 
 	if len(reputations) < N and len(reputations) >= 2:
-		print >>sys.stderr,"Warning: found a category with less than N=%i background triggers, using the number available: %i"%(N,len(reputations))
+		print("Warning: found a category with less than N=%i background triggers, using the number available: %i"%(N,len(reputations)), file=sys.stderr)
 	elif len(reputations) < 2:
-		print >>sys.stderr,"Warning: found a category with less than 2 background triggers, cannot perform extrapolation for this category"
+		print("Warning: found a category with less than 2 background triggers, cannot perform extrapolation for this category", file=sys.stderr)
 		return (0, (0, 0))
 
 	x = reputations[-N:]
@@ -194,7 +194,7 @@
 		elif category_algorithm == "oninstruments":
 			self.category_func = lambda self, on_instruments, participating_instruments, frequency: on_instruments
 		else:
-			raise ValueError, category_algorithm
+			raise ValueError(category_algorithm)
 		self.category_func = instancemethod(self.category_func, self, self.__class__)
 
 		if rank_by == "snr":
@@ -206,7 +206,7 @@
 		elif rank_by == "likelihood":
 			self.reputation_func = lambda self, snr, uncombined_ifar, likelihood: likelihood
 		else:
-			raise ValueError, rank_by
+			raise ValueError(rank_by)
 		self.rank_by = rank_by
 		self.reputation_func = instancemethod(self.reputation_func, self, self.__class__)
 
@@ -219,14 +219,14 @@
 
 	def add_livetime(self, connection, live_time_program, veto_segments_name = None, verbose = False):
 		if verbose:
-			print >>sys.stderr, "\tcomputing livetimes:",
+			print("\tcomputing livetimes:", end=' ', file=sys.stderr)
 
 		xmldoc = dbtables.get_xml(connection)
 		veto_segments = farutils.get_veto_segments(connection, live_time_program, xmldoc, veto_segments_name=veto_segments_name)
 		segs = farutils.get_segments(connection, xmldoc, "lalapps_ring").coalesce()
 		bglivetime=farutils.add_background_livetime(connection, live_time_program, segs, veto_segments, coinc_segments=None, verbose=verbose)
 
-		if verbose: print >>sys.stderr, "\n\tbackground livetime %s\n" % (str(bglivetime))
+		if verbose: print("\n\tbackground livetime %s\n" % (str(bglivetime)), file=sys.stderr)
 		for key in bglivetime.keys():
 			try:
 				self.cached_livetime[key] += bglivetime[key]
@@ -285,8 +285,8 @@
 		try:
 			return n / t
 
-		except ZeroDivisionError, e:
-			print >>sys.stderr, "found an event in category %s that has a livetime of 0 s" % repr(self.category_func(on_instruments, participating_instruments, frequency))
+		except ZeroDivisionError as e:
+			print("found an event in category %s that has a livetime of 0 s" % repr(self.category_func(on_instruments, participating_instruments, frequency)), file=sys.stderr)
 			raise e
 
 
@@ -321,7 +321,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "collecting background statistics ..."
+	print("collecting background statistics ...", file=sys.stderr)
 
 
 for n, filename in enumerate(filenames):
@@ -330,7 +330,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
 
@@ -343,7 +343,7 @@
 
 	if "sim_ringdown" in dbtables.get_table_names(connection):
 		if options.verbose:
-			print >>sys.stderr, "\tdatabase contains sim_ringdown table, skipping ..."
+			print("\tdatabase contains sim_ringdown table, skipping ...", file=sys.stderr)
 
 		#
 		# close the database
@@ -364,7 +364,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "\tcollecting background statistics ..."
+		print("\tcollecting background statistics ...", file=sys.stderr)
 	for on_instruments, participating_instruments, frequency, snr, uncombined_ifar, likelihood in connection.cursor().execute("""
 SELECT
 	coinc_event.instruments,
@@ -405,7 +405,7 @@
 background.index()
 if options.extrapolation_num:
 	if options.verbose:
-		print >>sys.stderr, "\tcalculating FAR extrapolation coefficients ..."
+		print("\tcalculating FAR extrapolation coefficients ...", file=sys.stderr)
 	background.rate_extrapolation(options.extrapolation_num)
 
 #
@@ -419,7 +419,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
 
@@ -434,7 +434,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "\tcalculating and recording false alarm rates ..."
+		print("\tcalculating and recording false alarm rates ...", file=sys.stderr)
 	connection.cursor().execute("""
 UPDATE
 	coinc_ringdown
--- ./src/ring/ring.py	(original)
+++ ./src/ring/ring.py	(refactored)
@@ -60,7 +60,7 @@
     synchronized with the name of the output file in ring.c.
     """
     if not self.get_start() or not self.get_end() or not self.get_ifo():
-      raise RingError, "Start time, end time or ifo has not been set"
+      raise RingError("Start time, end time or ifo has not been set")
 
     basename = self.get_ifo() + '-RING'
 
--- ./src/string/cosmicstring.py	(original)
+++ ./src/string/cosmicstring.py	(refactored)
@@ -75,7 +75,7 @@
 		self.output_dir = "."
 		self.files_per_meas_likelihood = get_files_per_meas_likelihood(config_parser)
 		if self.files_per_meas_likelihood < 1:
-			raise ValueError, "files_per_meas_likelihood < 1"
+			raise ValueError("files_per_meas_likelihood < 1")
 
 
 class MeasLikelihoodNode(pipeline.CondorDAGNode):
@@ -95,7 +95,7 @@
 
 	def add_input_cache(self, cache):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		self.input_cache.extend(cache)
 
 	def add_file_arg(self, filename):
@@ -103,7 +103,7 @@
 
 	def set_output(self, description):
 		if self.output_cache:
-			raise AttributeError, "cannot change attributes after computing output cache"
+			raise AttributeError("cannot change attributes after computing output cache")
 		cache_entry = power.make_cache_entry(self.input_cache, description, "")
 		filename = os.path.join(self.output_dir, "%s-STRING_LIKELIHOOD_%s-%d-%d.xml.gz" % (cache_entry.observatory, cache_entry.description, int(cache_entry.segment[0]), int(abs(cache_entry.segment))))
 		cache_entry.url = "file://localhost" + os.path.abspath(filename)
@@ -117,13 +117,13 @@
 
 	def get_output_cache(self):
 		if not self.output_cache:
-			raise AttributeError, "must call set_output(description) first"
+			raise AttributeError("must call set_output(description) first")
 		return self.output_cache
 
 	def write_input_files(self, *args):
 		f = file(self.cache_name, "w")
 		for c in self.input_cache:
-			print >>f, str(c)
+			print(str(c), file=f)
 		pipeline.CondorDAGNode.write_input_files(self, *args)
 
 	def get_output_files(self):
@@ -145,7 +145,7 @@
 		self.cache_dir = power.get_cache_dir(config_parser)
 		self.files_per_calc_likelihood = get_files_per_calc_likelihood(config_parser)
 		if self.files_per_calc_likelihood < 1:
-			raise ValueError, "files_per_calc_likelihood < 1"
+			raise ValueError("files_per_calc_likelihood < 1")
 
 
 class CalcLikelihoodNode(pipeline.CondorDAGNode):
@@ -187,10 +187,10 @@
 	def write_input_files(self, *args):
 		f = file(self.cache_name, "w")
 		for c in self.input_cache:
-			print >>f, str(c)
+			print(str(c), file=f)
 		f = file(self.likelihood_cache_name, "w")
 		for c in self.likelihood_cache:
-			print >>f, str(c)
+			print(str(c), file=f)
 		pipeline.CondorDAGNode.write_input_files(self, *args)
 
 	def get_output_files(self):
@@ -246,14 +246,14 @@
     the config file.
     """
     if self.output_cache:
-      raise AttributeError, "cannot change attributes after computing output cache"
+      raise AttributeError("cannot change attributes after computing output cache")
     pipeline.AnalysisNode.set_ifo(self, instrument)
     for optvalue in self.job()._AnalysisJob__cp.items("lalapps_StringSearch_%s" % instrument):
       self.add_var_arg("--%s %s" % optvalue)
 
   def set_user_tag(self, tag):
     if self.output_cache:
-      raise AttributeError, "cannot change attributes after computing output cache"
+      raise AttributeError("cannot change attributes after computing output cache")
     self.__usertag = tag
     self.add_var_opt("user-tag", self.__usertag)
 
@@ -280,7 +280,7 @@
     """
     if self._AnalysisNode__output is None:
       if None in (self.get_start(), self.get_end(), self.get_ifo(), self.__usertag):
-        raise ValueError, "start time, end time, ifo, or user tag has not been set"
+        raise ValueError("start time, end time, ifo, or user tag has not been set")
       seg = segments.segment(LIGOTimeGPS(self.get_start()), LIGOTimeGPS(self.get_end()))
       self.set_output(os.path.join(self.output_dir, "%s-STRINGSEARCH_%s-%d-%d.xml.gz" % (self.get_ifo(), self.__usertag, int(self.get_start()), int(self.get_end()) - int(self.get_start()))))
 
@@ -316,7 +316,7 @@
                 self.set_sub_file("lalapps_run_sqlite.sub")
 		self.files_per_run_sqlite = get_files_per_run_sqlite(config_parser)
 		if self.files_per_run_sqlite < 1:
-			raise ValueError, "files_per_run_sqlite < 1"
+			raise ValueError("files_per_run_sqlite < 1")
 
 
 class RunSqliteNode(pipeline.CondorDAGNode):
@@ -513,7 +513,7 @@
 def split_segment(seg, min_segment_length, pad, overlap, short_segment_duration, max_job_length):
 	# avoid infinite loop
 	if min_segment_length + 2 * pad <= overlap:
-		raise ValueError, "infinite loop: min_segment_length + 2 * pad must be > overlap"
+		raise ValueError("infinite loop: min_segment_length + 2 * pad must be > overlap")
 
 	# clip max_job_length down to an allowed size
 	max_job_length = clip_segment_length(max_job_length, pad, short_segment_duration)
@@ -528,11 +528,11 @@
 		assert abs(seglist[-1]) != 0	# safety-check for no-op
 		# bounds must be integers
 		if abs((int(seglist[-1][0]) - seglist[-1][0]) / seglist[-1][0]) > 1e-14 or abs((int(seglist[-1][1]) - seglist[-1][1]) / seglist[-1][1]) > 1e-14:
-			raise ValueError, "segment %s does not have integer boundaries" % str(seglist[-1])
+			raise ValueError("segment %s does not have integer boundaries" % str(seglist[-1]))
 		# advance segment
 		seg = segments.segment(seglist[-1][1] - overlap, seg[1])
 	if not seglist:
-		raise ValueError, "unable to use segment %s" % str(seg)
+		raise ValueError("unable to use segment %s" % str(seg))
 	return seglist
 
 
@@ -555,7 +555,7 @@
 		injargs = {}
 	seglist = split_segment(seg, min_segment_length, pad, overlap, short_segment_duration, max_job_length)
 	if verbose:
-		print >>sys.stderr, "Segment split: " + str(seglist)
+		print("Segment split: " + str(seglist), file=sys.stderr)
 	nodes = set()
 	for seg in seglist:
 		nodes |= make_string_fragment(dag, datafindnodes | binjnodes, instrument, seg, tag, framecache, injargs = injargs)
@@ -572,12 +572,12 @@
 	for instrument, seglist in seglistdict.items():
 		for seg in seglist:
 			if verbose:
-				print >>sys.stderr, "generating %s fragment %s" % (instrument, str(seg))
+				print("generating %s fragment %s" % (instrument, str(seg)), file=sys.stderr)
 
 			# find the datafind job this job is going to need
 			dfnodes = set([node for node in datafinds if (node.get_ifo() == instrument) and (seg in segments.segment(node.get_start(), node.get_end()))])
 			if len(dfnodes) != 1:
-				raise ValueError, "error, not exactly 1 datafind is suitable for trigger generator job at %s in %s" % (str(seg), instrument)
+				raise ValueError("error, not exactly 1 datafind is suitable for trigger generator job at %s in %s" % (str(seg), instrument))
 
 			# trigger generator jobs
 			nodes |= make_string_segment_fragment(dag, dfnodes, instrument, seg, tag, min_segment_length, pad, overlap, short_segment_duration, max_job_length, binjnodes = binjnodes, verbose = verbose)
@@ -605,7 +605,7 @@
 
 VACUUM;"""
 
-	print >>file(filename, "w"), code
+	print(code, file=file(filename, "w"))
 
 	return filename
 
--- ./src/string/lalapps_cosmicstring_pipe.py	(original)
+++ ./src/string/lalapps_cosmicstring_pipe.py	(refactored)
@@ -72,7 +72,7 @@
 	required_options = ["log_path", "config_file", "background_time_slides", "injection_time_slides", "segments_file"]
 	missing_options = [option for option in required_options if getattr(options, option) is None]
 	if missing_options:
-		raise ValueError, "missing required options %s" % ", ".join(sorted("--%s" % option.replace("_", "-") for option in missing_options))
+		raise ValueError("missing required options %s" % ", ".join(sorted("--%s" % option.replace("_", "-") for option in missing_options)))
 
 	if options.vetoes_file is not None:
 		options.vetoes_cache = set([CacheEntry(None, "VETO", None, "file://localhost" + os.path.abspath(options.vetoes_file))])
@@ -103,10 +103,10 @@
 log_fh = open(basename + '.pipeline.log', 'w')
 # FIXME: the following code uses obsolete CVS ID tags.
 # It should be modified to use git version information.
-print >>log_fh, "$Id$\nInvoked with arguments:"
+print("$Id$\nInvoked with arguments:", file=log_fh)
 for name_value in options.__dict__.items():
-	print >>log_fh, "%s %s" % name_value
-print >>log_fh
+	print("%s %s" % name_value, file=log_fh)
+print(file=log_fh)
 
 #
 # create the config parser object and read in the ini file
@@ -179,11 +179,11 @@
 # remove extra instruments
 for instrument in set(seglists) - instruments:
 	if options.verbose:
-		print >>sys.stderr, "warning: ignoring segments for '%s' found in '%s'" % (instrument, options.segments_file)
+		print("warning: ignoring segments for '%s' found in '%s'" % (instrument, options.segments_file), file=sys.stderr)
 	del seglists[instrument]
 # check for missing instruments
 if not instruments.issubset(set(seglists)):
-	raise ValueError, "segment lists retrieved from '%s' missing segments for instruments %s" % (options.segments_file, ", ".join(instruments - set(seglists)))
+	raise ValueError("segment lists retrieved from '%s' missing segments for instruments %s" % (options.segments_file, ", ".join(instruments - set(seglists))))
 # now rely on seglists' keys to provide the instruments
 del instruments
 
@@ -193,7 +193,7 @@
 #
 
 if options.verbose:
-	print >>sys.stderr, "Computing segments for which lalapps_StringSearch jobs are required ..."
+	print("Computing segments for which lalapps_StringSearch jobs are required ...", file=sys.stderr)
 
 background_time_slides = {}
 background_seglists = segments.segmentlistdict()
@@ -230,7 +230,7 @@
 	for background_cache_entry, background_offsetvector in [(cache_entry, offsetvector) for cache_entry, offsetvectors in background_time_slides.items() for offsetvector in offsetvectors]:
 		for injection_cache_entry, injection_offsetvector in [(cache_entry, offsetvector) for cache_entry, offsetvectors in injection_time_slides.items() for offsetvector in offsetvectors]:
 			if background_offsetvector.deltas == injection_offsetvector.deltas:
-				raise ValueError, "injections offset vector %s from %s is the same as non-injections offset vector %s from %s.  to avoid a self-selection bias, injections must not be performed at the same relative time shifts as a non-injection run" % (str(injection_offsetvector), injection_cache_entry.url, str(background_offsetvector), background_cache_entry.url)
+				raise ValueError("injections offset vector %s from %s is the same as non-injections offset vector %s from %s.  to avoid a self-selection bias, injections must not be performed at the same relative time shifts as a non-injection run" % (str(injection_offsetvector), injection_cache_entry.url, str(background_offsetvector), background_cache_entry.url))
 
 check_for_reused_offsetvectors(background_time_slides, injection_time_slides)
 
@@ -317,7 +317,7 @@
 	coinc_nodes = []
 	for n, (time_slides_cache_entry, these_time_slides) in enumerate(time_slides.items()):
 		if verbose:
-			print >>sys.stderr, "%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path)
+			print("%s %d/%d (%s):" % (tag, n + 1, len(time_slides), time_slides_cache_entry.path), file=sys.stderr)
 		coinc_nodes.append(set())
 
 		#
@@ -369,10 +369,10 @@
 		#
 
 		if verbose:
-			print >>sys.stderr, "building burca jobs ..."
+			print("building burca jobs ...", file=sys.stderr)
 		coinc_nodes[-1] |= power.make_burca_fragment(dag, lladd_nodes, "%s_%d" % (tag, n), verbose = verbose)
 		if verbose:
-			print >>sys.stderr, "done %s %d/%d" % (tag, n + 1, len(time_slides))
+			print("done %s %d/%d" % (tag, n + 1, len(time_slides)), file=sys.stderr)
 
 	#
 	# lalapps_binjfind
@@ -380,7 +380,7 @@
 
 	if do_injections:
 		if verbose:
-			print >>sys.stderr, "building binjfind jobs ..."
+			print("building binjfind jobs ...", file=sys.stderr)
 		coinc_nodes = [power.make_binjfind_fragment(dag, these_coinc_nodes, "%s_%d" % (tag, n), verbose = verbose) for n, these_coinc_nodes in enumerate(coinc_nodes)]
 
 	#
@@ -388,7 +388,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "building sqlite jobs ..."
+		print("building sqlite jobs ...", file=sys.stderr)
 	coinc_nodes = [power.make_sqlite_fragment(dag, these_coinc_nodes, "%s_%d" % (tag, n), verbose = verbose) for n, these_coinc_nodes in enumerate(coinc_nodes)]
 	coinc_nodes = [cosmicstring.make_run_sqlite_fragment(dag, these_coinc_nodes, "%s_%d" % (tag, n), clipsegments_sql_filename) for n, these_coinc_nodes in enumerate(coinc_nodes)]
 
@@ -397,7 +397,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "building lalapps_string_meas_likelihood jobs ..."
+		print("building lalapps_string_meas_likelihood jobs ...", file=sys.stderr)
 	likelihood_nodes = [cosmicstring.make_meas_likelihood_fragment(dag, these_coinc_nodes, "%s_%d" % (tag, n)) for n, these_coinc_nodes in enumerate(coinc_nodes)]
 
 	#
@@ -405,7 +405,7 @@
 	#
 
 	if verbose:
-		print >>sys.stderr, "writing output cache ..."
+		print("writing output cache ...", file=sys.stderr)
 	for n, (these_coinc_nodes, these_likelihood_nodes) in enumerate(zip(coinc_nodes, likelihood_nodes)):
 		power.write_output_cache(these_coinc_nodes | these_likelihood_nodes, "%s_%s_output.cache" % (os.path.splitext(dag.get_dag_file())[0], "%s_%d" % (tag, n)))
 
@@ -455,7 +455,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "building lalapps_string_calc_likelihood jobs ..."
+	print("building lalapps_string_calc_likelihood jobs ...", file=sys.stderr)
 
 def round_robin_and_flatten(injection_coinc_node_groups, injection_likelihood_node_groups):
 	# round-robin the node lists
@@ -499,7 +499,7 @@
 #
 
 if options.verbose:
-	print >>sys.stderr, "writing dag ..."
+	print("writing dag ...", file=sys.stderr)
 dag.write_sub_files()
 dag.write_dag()
 cosmicstring.write_clip_segment_sql_file(clipsegments_sql_filename)
@@ -508,7 +508,7 @@
 # write a message telling the user that the DAG has been written
 #
 
-print """Created a DAG file which can be submitted by executing
+print("""Created a DAG file which can be submitted by executing
 
 $ condor_submit_dag %s
 
@@ -520,4 +520,4 @@
 $ unset X509_USER_PROXY
 $ grid-proxy-init -hours $((24*7)):00
 
-""" % dag.get_dag_file()
+""" % dag.get_dag_file())
--- ./src/string/lalapps_string_apply_vetoes.py	(original)
+++ ./src/string/lalapps_string_apply_vetoes.py	(refactored)
@@ -121,7 +121,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
--- ./src/string/lalapps_string_calc_likelihood.py	(original)
+++ ./src/string/lalapps_string_calc_likelihood.py	(refactored)
@@ -104,7 +104,7 @@
 
 coincparamsdistributions, likelihood_seglists = stringutils.load_likelihood_data(options.likelihood_filenames, verbose = options.verbose)
 if options.verbose:
-	print >>sys.stderr, "computing event densities ..."
+	print("computing event densities ...", file=sys.stderr)
 coincparamsdistributions.finish()
 
 
@@ -119,7 +119,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
@@ -134,7 +134,7 @@
 	if options.verbose:
 		SnglBurstUtils.summarize_coinc_database(contents)
 	if not contents.seglists and options.verbose:
-		print >>sys.stderr, "\twarning:  no segments found"
+		print("\twarning:  no segments found", file=sys.stderr)
 
 	#
 	# Build triangulators.  The timing uncertainty of +/- 8e-5 s was
--- ./src/string/lalapps_string_cs_gamma.py	(original)
+++ ./src/string/lalapps_string_cs_gamma.py	(refactored)
@@ -90,7 +90,7 @@
     epsilon = math.exp(math.log(ops.epsilonstart) + i * (math.log(ops.epsilonend) - math.log(ops.epsilonstart)) / (ops.nepsilon - 1))
     for j in range(ops.nGmu):
         Gmu = math.exp(math.log(ops.Gmustart) + j * (math.log(ops.Gmuend) - math.log(ops.Gmustart)) / (ops.nGmu - 1))
-        print >>sys.stderr, "%.1f%%: Gmu=%10.4g, epsilon=%10.4g, p=%4.2g\r" % (100.0 * (i * ops.nGmu + j) / (ops.nepsilon * ops.nGmu), Gmu, epsilon, P),
+        print("%.1f%%: Gmu=%10.4g, epsilon=%10.4g, p=%4.2g\r" % (100.0 * (i * ops.nGmu + j) / (ops.nepsilon * ops.nGmu), Gmu, epsilon, P), end=' ', file=sys.stderr)
 
         alpha = epsilon * (LOOPS_RAD_POWER * Gmu)**ops.index
 
@@ -104,4 +104,4 @@
         gammaMax = scipy.integrate.simps(numpy.clip(eff[:-1] + Deff[:-1], 0.0, 1.0) * zofA[:-1] * dRdz[:-1] * -Dlnz) * CUSPS_PER_LOOP / P
 
         outfile.write("%.17g  %.17g  %.17g  %.17g  %.17g  %.17g  %.17g\n" % (P, ops.index, epsilon, Gmu, gammaAverage, gammaMin, gammaMax))
-print >>sys.stderr, "100.0%%: Gmu=%10.4g, epsilon=%10.4g, p=%4.2g" % (Gmu, epsilon, P)
+print("100.0%%: Gmu=%10.4g, epsilon=%10.4g, p=%4.2g" % (Gmu, epsilon, P), file=sys.stderr)
--- ./src/string/lalapps_string_final.py	(original)
+++ ./src/string/lalapps_string_final.py	(refactored)
@@ -301,28 +301,28 @@
 		self.candidates.sort()
 
 		f = file("string_most_significant_background.txt", "w")
-		print >>f, "Highest-Ranked Background Events"
-		print >>f, "================================"
+		print("Highest-Ranked Background Events", file=f)
+		print("================================", file=f)
 		for ln_likelihood_ratio, filename, coinc_event_id, instruments, peak_time in self.most_significant_background:
-			print >>f
-			print >>f, "%s in %s:" % (str(coinc_event_id), filename)
-			print >>f, "Recovered in: %s" % ", ".join(sorted(instruments or []))
-			print >>f, "Mean peak time:  %.16g s GPS" % peak_time
-			print >>f, "\\log \\Lambda:  %.16g" % ln_likelihood_ratio
+			print(file=f)
+			print("%s in %s:" % (str(coinc_event_id), filename), file=f)
+			print("Recovered in: %s" % ", ".join(sorted(instruments or [])), file=f)
+			print("Mean peak time:  %.16g s GPS" % peak_time, file=f)
+			print("\\log \\Lambda:  %.16g" % ln_likelihood_ratio, file=f)
 
 		f = file("string_candidates.txt", "w")
-		print >>f, "Highest-Ranked Zero-Lag Events"
-		print >>f, "=============================="
+		print("Highest-Ranked Zero-Lag Events", file=f)
+		print("==============================", file=f)
 		if self.open_box:
 			for ln_likelihood_ratio, filename, coinc_event_id, instruments, peak_time in self.candidates:
-				print >>f
-				print >>f, "%s in %s:" % (str(coinc_event_id), filename)
-				print >>f, "Recovered in: %s" % ", ".join(sorted(instruments or []))
-				print >>f, "Mean peak time:  %.16g s GPS" % peak_time
-				print >>f, "\\log \\Lambda:  %.16g" % ln_likelihood_ratio
+				print(file=f)
+				print("%s in %s:" % (str(coinc_event_id), filename), file=f)
+				print("Recovered in: %s" % ", ".join(sorted(instruments or [])), file=f)
+				print("Mean peak time:  %.16g s GPS" % peak_time, file=f)
+				print("\\log \\Lambda:  %.16g" % ln_likelihood_ratio, file=f)
 		else:
-			print >>f
-			print >>f, "List suppressed:  box is closed"
+			print(file=f)
+			print("List suppressed:  box is closed", file=f)
 
 		#
 		# sort the ranking statistics and convert to arrays.
@@ -476,9 +476,9 @@
 
 
 def write_efficiency(fileobj, bins, eff, yerr):
-	print >>fileobj, "# A	e	D[e]"
+	print("# A	e	D[e]", file=fileobj)
 	for A, e, De in zip(bins.centres()[0], eff, yerr):
-		print >>fileobj, "%.16g	%.16g	%.16g" % (A, e, De)
+		print("%.16g	%.16g	%.16g" % (A, e, De), file=fileobj)
 
 
 def render_data_from_bins(dump_file, axes, efficiency_num, efficiency_den, cal_uncertainty, filter_width, colour = "k", erroralpha = 0.3, linestyle = "-"):
@@ -525,8 +525,8 @@
 	# print some analysis FIXME:  this calculation needs attention
 	num_injections = efficiency_den.array.sum()
 	num_samples = len(efficiency_den.array)
-	print >>sys.stderr, "Bins were %g samples wide, ideal would have been %g" % (filter_width, (num_samples / num_injections / interpolate.interp1d(x, dydx)(A50)**2.0)**(1.0/3.0))
-	print >>sys.stderr, "Average number of injections in each bin = %g" % efficiency_den.array.mean()
+	print("Bins were %g samples wide, ideal would have been %g" % (filter_width, (num_samples / num_injections / interpolate.interp1d(x, dydx)(A50)**2.0)**(1.0/3.0)), file=sys.stderr)
+	print("Average number of injections in each bin = %g" % efficiency_den.array.mean(), file=sys.stderr)
 
 	return line, A50, A50_err
 
@@ -612,7 +612,7 @@
 						heapq.heappushpop(self.loudest_missed, record)
 			elif found:
 				# no, but it was found anyway
-				print >>sys.stderr, "%s: odd, injection %s was found but not injected ..." % (contents.filename, sim.simulation_id)
+				print("%s: odd, injection %s was found but not injected ..." % (contents.filename, sim.simulation_id), file=sys.stderr)
 
 	def finish(self):
 		fig, axes = SnglBurstUtils.make_burst_plot(r"Injection Amplitude (\(\mathrm{s}^{-\frac{1}{3}}\))", "Detection Efficiency", width = 108.0)
@@ -655,7 +655,7 @@
 		efficiency_den.array[(efficiency_num.array == 0) & (efficiency_den.array == 0)] = 1
 
 		line1, A50, A50_err = render_data_from_bins(file("string_efficiency.dat", "w"), axes, efficiency_num, efficiency_den, self.cal_uncertainty, self.filter_width, colour = "k", linestyle = "-", erroralpha = 0.2)
-		print >>sys.stderr, "Pipeline's 50%% efficiency point for all detections = %g +/- %g%%\n" % (A50, A50_err * 100)
+		print("Pipeline's 50%% efficiency point for all detections = %g +/- %g%%\n" % (A50, A50_err * 100), file=sys.stderr)
 
 		# add a legend to the axes
 		axes.legend((line1,), (r"\noindent Injections recovered with $\log \Lambda > %.2f$" % self.detection_threshold,), loc = "lower right")
@@ -673,44 +673,44 @@
 		self.quietest_found.sort(reverse = True)
 
 		f = file("string_loud_missed_injections.txt", "w")
-		print >>f, "Highest Amplitude Missed Injections"
-		print >>f, "==================================="
+		print("Highest Amplitude Missed Injections", file=f)
+		print("===================================", file=f)
 		for amplitude, sim, offsetvector, filename, ln_likelihood_ratio in self.loudest_missed:
-			print >>f
-			print >>f, "%s in %s:" % (str(sim.simulation_id), filename)
+			print(file=f)
+			print("%s in %s:" % (str(sim.simulation_id), filename), file=f)
 			if ln_likelihood_ratio is None:
-				print >>f, "Not recovered"
+				print("Not recovered", file=f)
 			else:
-				print >>f, "Recovered with \\log \\Lambda = %.16g, detection threshold was %.16g" % (ln_likelihood_ratio, self.detection_threshold)
+				print("Recovered with \\log \\Lambda = %.16g, detection threshold was %.16g" % (ln_likelihood_ratio, self.detection_threshold), file=f)
 			for instrument in self.seglists:
-				print >>f, "In %s:" % instrument
-				print >>f, "\tInjected amplitude:\t%.16g" % SimBurstUtils.string_amplitude_in_instrument(sim, instrument, offsetvector)
-				print >>f, "\tTime of injection:\t%s s" % sim.time_at_instrument(instrument, offsetvector)
-			print >>f, "Amplitude in waveframe:\t%.16g" % sim.amplitude
+				print("In %s:" % instrument, file=f)
+				print("\tInjected amplitude:\t%.16g" % SimBurstUtils.string_amplitude_in_instrument(sim, instrument, offsetvector), file=f)
+				print("\tTime of injection:\t%s s" % sim.time_at_instrument(instrument, offsetvector), file=f)
+			print("Amplitude in waveframe:\t%.16g" % sim.amplitude, file=f)
 			t = sim.get_time_geocent()
-			print >>f, "Time at geocentre:\t%s s" % t
-			print >>f, "Segments within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.seglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.seglists))
-			print >>f, "Vetoes within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.vetoseglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.vetoseglists))
+			print("Time at geocentre:\t%s s" % t, file=f)
+			print("Segments within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.seglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.seglists)), file=f)
+			print("Vetoes within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.vetoseglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.vetoseglists)), file=f)
 
 		f = file("string_quiet_found_injections.txt", "w")
-		print >>f, "Lowest Amplitude Found Injections"
-		print >>f, "================================="
+		print("Lowest Amplitude Found Injections", file=f)
+		print("=================================", file=f)
 		for inv_amplitude, sim, offsetvector, filename, ln_likelihood_ratio in self.quietest_found:
-			print >>f
-			print >>f, "%s in %s:" % (str(sim.simulation_id), filename)
+			print(file=f)
+			print("%s in %s:" % (str(sim.simulation_id), filename), file=f)
 			if ln_likelihood_ratio is None:
-				print >>f, "Not recovered"
+				print("Not recovered", file=f)
 			else:
-				print >>f, "Recovered with \\log \\Lambda = %.16g, detection threshold was %.16g" % (ln_likelihood_ratio, self.detection_threshold)
+				print("Recovered with \\log \\Lambda = %.16g, detection threshold was %.16g" % (ln_likelihood_ratio, self.detection_threshold), file=f)
 			for instrument in self.seglists:
-				print >>f, "In %s:" % instrument
-				print >>f, "\tInjected amplitude:\t%.16g" % SimBurstUtils.string_amplitude_in_instrument(sim, instrument, offsetvector)
-				print >>f, "\tTime of injection:\t%s s" % sim.time_at_instrument(instrument, offsetvector)
-			print >>f, "Amplitude in waveframe:\t%.16g" % sim.amplitude
+				print("In %s:" % instrument, file=f)
+				print("\tInjected amplitude:\t%.16g" % SimBurstUtils.string_amplitude_in_instrument(sim, instrument, offsetvector), file=f)
+				print("\tTime of injection:\t%s s" % sim.time_at_instrument(instrument, offsetvector), file=f)
+			print("Amplitude in waveframe:\t%.16g" % sim.amplitude, file=f)
 			t = sim.get_time_geocent()
-			print >>f, "Time at geocentre:\t%s s" % t
-			print >>f, "Segments within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.seglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.seglists))
-			print >>f, "Vetoes within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.vetoseglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.vetoseglists))
+			print("Time at geocentre:\t%s s" % t, file=f)
+			print("Segments within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.seglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.seglists)), file=f)
+			print("Vetoes within 60 seconds:\t%s" % segmentsUtils.segmentlistdict_to_short_string(self.vetoseglists & segments.segmentlistdict((instrument, segments.segmentlist([segments.segment(t-offsetvector[instrument]-60, t-offsetvector[instrument]+60)])) for instrument in self.vetoseglists)), file=f)
 
 		#
 		# done
@@ -734,15 +734,15 @@
 	noninjection_filenames = []
 	for n, filename in enumerate(filenames):
 		if verbose:
-			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+			print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 		connection = sqlite3.connect(filename)
 		if "sim_burst" in dbtables.get_table_names(connection):
 			if verbose:
-				print >>sys.stderr, "\t--> injections"
+				print("\t--> injections", file=sys.stderr)
 			injection_filenames.append(filename)
 		else:
 			if verbose:
-				print >>sys.stderr, "\t--> non-injections"
+				print("\t--> non-injections", file=sys.stderr)
 			noninjection_filenames.append(filename)
 		connection.close()
 	return injection_filenames, noninjection_filenames
@@ -759,18 +759,18 @@
 	efficiency = None
 	for filename in filenames:
 		if verbose:
-			print >>sys.stderr, "\t%s ..." % filename,
+			print("\t%s ..." % filename, end=' ', file=sys.stderr)
 		dump = pickle.load(open(filename))
 		if type(dump) is RateVsThreshold:
 			if verbose:
-				print >>sys.stderr, "found rate vs. threshold data"
+				print("found rate vs. threshold data", file=sys.stderr)
 			if rate_vs_threshold is None:
 				rate_vs_threshold = dump
 			else:
 				rate_vs_threshold += dump
 		elif type(dump) is Efficiency:
 			if verbose:
-				print >>sys.stderr, "found efficiency data"
+				print("found efficiency data", file=sys.stderr)
 			if efficiency is None:
 				efficiency = dump
 			else:
@@ -818,7 +818,7 @@
 
 	for n, product in enumerate(products):
 		if verbose:
-			print >>sys.stderr, "%s: adding to product %d ..." % (working_filename, n)
+			print("%s: adding to product %d ..." % (working_filename, n), file=sys.stderr)
 		product.add_contents(contents, verbose = verbose)
 
 	#
@@ -832,7 +832,7 @@
 def process_files(filenames, products, live_time_program, tmp_path = None, veto_segments_name = None, verbose = False):
 	for n, filename in enumerate(filenames):
 		if verbose:
-			print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+			print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 		process_file(filename, products, live_time_program, tmp_path = tmp_path, veto_segments_name = veto_segments_name, verbose = verbose)
 
 
@@ -841,42 +841,42 @@
 	n = 1
 	while products:
 		if verbose:
-			print >>sys.stderr, "finishing product %d ..." % (n - 1)
+			print("finishing product %d ..." % (n - 1), file=sys.stderr)
 		product = products.pop(0)
 		# write dump of raw data
 		filename = "%sdump_%d.pickle" % (prefix, n)
 		if verbose:
-			print >>sys.stderr, "\twriting %s ..." % filename,
+			print("\twriting %s ..." % filename, end=' ', file=sys.stderr)
 		pickle.dump(product, open(filename, "w"))
 		if verbose:
-			print >>sys.stderr, "done"
+			print("done", file=sys.stderr)
 		# write plots
 		for m, fig in enumerate(product.finish()):
 			for ext in image_formats:
 				filename = format % (prefix, n, chr(m + 97), ext)
 				if verbose:
-					print >>sys.stderr, "\twriting %s ..." % filename,
+					print("\twriting %s ..." % filename, end=' ', file=sys.stderr)
 				fig.savefig(filename)
 				if verbose:
-					print >>sys.stderr, "done"
+					print("done", file=sys.stderr)
 		n += 1
 
 
 options, filenames = parse_command_line()
 
 
-print >>sys.stderr, """Command line:
+print("""Command line:
 
 $ %s
-""" % " ".join(sys.argv)
+""" % " ".join(sys.argv), file=sys.stderr)
 if options.open_box:
-	print >>sys.stderr, """
+	print("""
 
 ---=== !! BOX IS OPEN !! ===---
 
 PRESS CTRL-C SOON IF YOU DIDN'T MEAN TO OPEN THE BOX
 
-"""
+""", file=sys.stderr)
 
 
 #
@@ -885,7 +885,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "Identifying injection and non-injection databases ..."
+	print("Identifying injection and non-injection databases ...", file=sys.stderr)
 injection_filenames, noninjection_filenames = group_files(filenames, verbose = options.verbose)
 
 
@@ -896,7 +896,7 @@
 
 if options.import_dump:
 	if options.verbose:
-		print >>sys.stderr, "Loading dump files ..."
+		print("Loading dump files ...", file=sys.stderr)
 	rate_vs_threshold, efficiency = import_dumps(options.import_dump, verbose = options.verbose)
 	# override box openness in rate-vs-threshold data
 	if rate_vs_threshold is not None:
@@ -914,7 +914,7 @@
 
 if options.detection_threshold is None:
 	if options.verbose:
-		print >>sys.stderr, "Collecting background and zero-lag statistics ..."
+		print("Collecting background and zero-lag statistics ...", file=sys.stderr)
 
 	children = {}
 	rate_vs_thresholds = []
@@ -958,7 +958,7 @@
 				else:
 					rate_vs_threshold += process_output
 			else:
-				print >>sys.stderr, process_output
+				print(process_output, file=sys.stderr)
 			del process_output
 			if exit_status:
 				sys.exit(exit_status)
@@ -971,17 +971,17 @@
 	# print summary information, and set detection threshold
 	if options.open_box:
 		try:
-			print >>sys.stderr, "Zero-lag events: %d" % len(rate_vs_threshold.zero_lag)
+			print("Zero-lag events: %d" % len(rate_vs_threshold.zero_lag), file=sys.stderr)
 		except OverflowError:
 			# python < 2.5 can't handle list-like things with more than 2**31 elements
 			# FIXME:  remove when we can rely on python >= 2.5
-			print >>sys.stderr, "Zero-lag events: %d" % rate_vs_threshold.zero_lag.n
-	print >>sys.stderr, "Total time in zero-lag segments: %s s" % str(rate_vs_threshold.zero_lag_time)
-	print >>sys.stderr, "Time-slide events: %d" % rate_vs_threshold.n_background
-	print >>sys.stderr, "Total time in time-slide segments: %s s" % str(rate_vs_threshold.background_time)
+			print("Zero-lag events: %d" % rate_vs_threshold.zero_lag.n, file=sys.stderr)
+	print("Total time in zero-lag segments: %s s" % str(rate_vs_threshold.zero_lag_time), file=sys.stderr)
+	print("Time-slide events: %d" % rate_vs_threshold.n_background, file=sys.stderr)
+	print("Total time in time-slide segments: %s s" % str(rate_vs_threshold.background_time), file=sys.stderr)
 	if options.open_box:
 		detection_threshold = rate_vs_threshold.zero_lag[-1]
-		print >>sys.stderr, "Likelihood ratio for highest-ranked zero-lag survivor: %.9g" % detection_threshold
+		print("Likelihood ratio for highest-ranked zero-lag survivor: %.9g" % detection_threshold, file=sys.stderr)
 	else:
 		# if the background and zero-lag live times are identical,
 		# then the loudest zero-lag event is simulated by the
@@ -991,10 +991,10 @@
 		# simulated by the next-to-loudest background event so we
 		# want entry -2 in the sorted list.
 		detection_threshold = sorted(rate_vs_threshold.background)[-int(round(rate_vs_threshold.background_time / rate_vs_threshold.zero_lag_time))]
-		print >>sys.stderr, "Simulated \\log \\Lambda for highest-ranked zero-lag survivor: %.9g" % detection_threshold
+		print("Simulated \\log \\Lambda for highest-ranked zero-lag survivor: %.9g" % detection_threshold, file=sys.stderr)
 else:
 	detection_threshold = options.detection_threshold
-	print >>sys.stderr, "Likelihood ratio for highest-ranked zero-lag survivor from command line: %.9g" % detection_threshold
+	print("Likelihood ratio for highest-ranked zero-lag survivor from command line: %.9g" % detection_threshold, file=sys.stderr)
 
 
 #
@@ -1004,7 +1004,7 @@
 
 
 if options.verbose:
-	print >>sys.stderr, "Collecting efficiency statistics ..."
+	print("Collecting efficiency statistics ...", file=sys.stderr)
 children = {}
 # group files into bins of about the same total number of bytes.  don't try
 # to create more groups than there are files
@@ -1046,7 +1046,7 @@
 			else:
 				efficiency += process_output
 		else:
-			print >>sys.stderr, process_output
+			print(process_output, file=sys.stderr)
 		del process_output
 		if exit_status:
 			sys.exit(exit_status)
@@ -1055,7 +1055,7 @@
 	write_products([efficiency], "string_efficiency_", options.image_formats, verbose = options.verbose)
 else:
 	if options.verbose:
-		print >>sys.stderr, "no injection data available, not writing efficiency data products."
+		print("no injection data available, not writing efficiency data products.", file=sys.stderr)
 
 if options.verbose:
-	print >>sys.stderr, "done."
+	print("done.", file=sys.stderr)
--- ./src/string/lalapps_string_meas_likelihood.py	(original)
+++ ./src/string/lalapps_string_meas_likelihood.py	(refactored)
@@ -189,7 +189,7 @@
 	#
 
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	connection = sqlite3.connect(working_filename)
@@ -204,7 +204,7 @@
 	if options.verbose:
 		SnglBurstUtils.summarize_coinc_database(contents)
 	if not contents.seglists and options.verbose:
-		print >>sys.stderr, "\twarning:  no segments found"
+		print("\twarning:  no segments found", file=sys.stderr)
 	segs |= contents.seglists
 
 	#
--- ./src/string/lalapps_string_plot_binj.py	(original)
+++ ./src/string/lalapps_string_plot_binj.py	(refactored)
@@ -90,7 +90,7 @@
 	filenames = filenames or []
 	for cache in options.input_cache:
 		if options.verbose:
-			print >>sys.stderr, "reading '%s' ..." % cache
+			print("reading '%s' ..." % cache, file=sys.stderr)
 		filenames += [CacheEntry(line).path for line in file(cache)]
 	if not filenames:
 		raise ValueError("Nothing to do!")
@@ -479,7 +479,7 @@
 
 for n, filename in enumerate(ligolw_utils.sort_files_by_size(filenames, options.verbose, reverse = True)):
 	if options.verbose:
-		print >>sys.stderr, "%d/%d: %s" % (n + 1, len(filenames), filename)
+		print("%d/%d: %s" % (n + 1, len(filenames), filename), file=sys.stderr)
 	working_filename = dbtables.get_connection_filename(filename, tmp_path = options.tmp_space, verbose = options.verbose)
 	database = SnglBurstUtils.CoincDatabase(sqlite3.connect(working_filename), options.live_time_program, search = "StringCusp")
 	if options.verbose:
@@ -511,7 +511,7 @@
 		if requires_injection_db and not is_injection_db:
 			continue
 		if options.verbose:
-			print >>sys.stderr, "adding to plot group %d ..." % n
+			print("adding to plot group %d ..." % n, file=sys.stderr)
 		plot.add_contents(database)
 	database.connection.close()
 	dbtables.discard_connection_filename(filename, working_filename, verbose = options.verbose)
@@ -526,15 +526,15 @@
 while len(plots):
 	n, requires_injection_db, plot = plots.pop(0)
 	if options.verbose:
-		print >>sys.stderr, "finishing plot group %d ..." % n
+		print("finishing plot group %d ..." % n, file=sys.stderr)
 	try:
 		figs = plot.finish()
 	except ValueError as e:
-		print >>sys.stderr, "can't finish plot group %d: %s" % (n, str(e))
+		print("can't finish plot group %d: %s" % (n, str(e)), file=sys.stderr)
 	else:
 		for f, fig in enumerate(figs):
 			for extension in options.format:
 				filename = format % (options.base, n, chr(97 + f), extension)
 				if options.verbose:
-					print >>sys.stderr, "writing %s ..." % filename
+					print("writing %s ..." % filename, file=sys.stderr)
 				fig.savefig(filename)
--- ./src/string/lalapps_string_plot_likelihood.py	(original)
+++ ./src/string/lalapps_string_plot_likelihood.py	(refactored)
@@ -294,7 +294,7 @@
 coincparamsdistributions, seglists = stringutils.load_likelihood_data(filenames, verbose = options.verbose)
 
 if options.verbose:
-	print >>sys.stderr, "computing event densities ..."
+	print("computing event densities ...", file=sys.stderr)
 if not options.no_filter:
 	coincparamsdistributions.finish(verbose = options.verbose)
 
@@ -304,18 +304,18 @@
 	instruments = set(name.split("_")) & set(stringutils.StringCoincParamsDistributions.instrument_categories)
 	if name.endswith("_snr2_chi2"):
 		if options.verbose:
-			print >>sys.stderr, "generating plots for %s ..." % name
+			print("generating plots for %s ..." % name, file=sys.stderr)
 		denominator_xcoords, denominator_ycoords, denominator_data = clip_binned_array_2d(denominator_pdf, [10, 1e6], [.01, 1e4])
 		numerator_xcoords, numerator_ycoords, numerator_data = clip_binned_array_2d(numerator_pdf, [10, 1e6], [.01, 1e4])
 		fig = snr2_chi2_plot("%s" % name.replace("_", "-"), denominator_xcoords, denominator_ycoords, denominator_data, numerator_xcoords, numerator_ycoords, numerator_data)
 		for extension in options.format:
 			outname = "%s.%s" % (name, extension)
 			if options.verbose:
-				print >>sys.stderr, "\twriting %s ..." % outname
+				print("\twriting %s ..." % outname, file=sys.stderr)
 			fig.savefig(outname)
 	elif name.endswith("_dt"):
 		if options.verbose:
-			print >>sys.stderr, "generating plots for %s ..." % name
+			print("generating plots for %s ..." % name, file=sys.stderr)
 		dt = .010 + snglcoinc.light_travel_time(*instruments)
 		denominator_coords, denominator_data = clip_binned_array_1d(denominator_pdf, (-dt, +dt))
 		numerator_coords, numerator_data = clip_binned_array_1d(numerator_pdf, (-dt, +dt))
@@ -323,56 +323,56 @@
 		for extension in options.format:
 			outname = "%s.%s" % (name, extension)
 			if options.verbose:
-				print >>sys.stderr, "\twriting %s ..." % outname
+				print("\twriting %s ..." % outname, file=sys.stderr)
 			fig.savefig(outname)
 	elif name.endswith("_dA"):
 		if options.verbose:
-			print >>sys.stderr, "generating plots for %s ..." % name
+			print("generating plots for %s ..." % name, file=sys.stderr)
 		denominator_coords, denominator_data = clip_binned_array_1d(denominator_pdf, (-2, +2))
 		numerator_coords, numerator_data = clip_binned_array_1d(numerator_pdf, (-2, +2))
 		fig = dA_plot("%s" % name.replace("_", "-"), denominator_coords, denominator_data, numerator_coords, numerator_data)
 		for extension in options.format:
 			outname = "%s.%s" % (name, extension)
 			if options.verbose:
-				print >>sys.stderr, "\twriting %s ..." % outname
+				print("\twriting %s ..." % outname, file=sys.stderr)
 			fig.savefig(outname)
 	elif name.endswith("_df"):
 		if options.verbose:
-			print >>sys.stderr, "generating plots for %s ..." % name
+			print("generating plots for %s ..." % name, file=sys.stderr)
 		denominator_coords, denominator_data = clip_binned_array_1d(denominator_pdf, (-0.6, +0.6))
 		numerator_coords, numerator_data = clip_binned_array_1d(numerator_pdf, (-0.6, +0.6))
 		fig = df_plot("%s" % name.replace("_", "-"), denominator_coords, denominator_data, numerator_coords, numerator_data)
 		for extension in options.format:
 			outname = "%s.%s" % (name, extension)
 			if options.verbose:
-				print >>sys.stderr, "\twriting %s ..." % outname
+				print("\twriting %s ..." % outname, file=sys.stderr)
 			fig.savefig(outname)
 	elif name == "nevents":
 		if options.verbose:
-			print >>sys.stderr, "generating plots for %s ..." % name
+			print("generating plots for %s ..." % name, file=sys.stderr)
 		fig = nevents_plot("%s" % name.replace("_", "-"), denominator_pdf.centres()[0], denominator_pdf.array, numerator_pdf.array)
 		for extension in options.format:
 			outname = "%s.%s" % (name, extension)
 			if options.verbose:
-				print >>sys.stderr, "\twriting %s ..." % outname
+				print("\twriting %s ..." % outname, file=sys.stderr)
 			fig.savefig(outname)
 	elif name == "instrumentgroup":
 		if options.verbose:
-			print >>sys.stderr, "generating plots for %s ..." % name
+			print("generating plots for %s ..." % name, file=sys.stderr)
 		fig = instrumentgroup_plot("%s" % name.replace("_", "-"), denominator_pdf.centres()[0], denominator_pdf.array, numerator_pdf.array)
 		for extension in options.format:
 			outname = "%s.%s" % (name, extension)
 			if options.verbose:
-				print >>sys.stderr, "\twriting %s ..." % outname
+				print("\twriting %s ..." % outname, file=sys.stderr)
 			fig.savefig(outname)
 	elif name == "instrumentgroup,rss_timing_residual":
 		if options.verbose:
-			print >>sys.stderr, "generating plots for %s ..." % name
+			print("generating plots for %s ..." % name, file=sys.stderr)
 		denominator_xcoords, denominator_ycoords, denominator_data = clip_binned_array_2d(denominator_pdf, [1, denominator_pdf.array.shape[0] + 0.5], [0, .02])
 		numerator_xcoords, numerator_ycoords, numerator_data = clip_binned_array_2d(numerator_pdf, [1, denominator_pdf.array.shape[0] + 0.5], [0, .02])
 		fig = instrumentgroup_timingresidual_plot("%s" % name.replace("_", "-").replace(",", "--"), denominator_xcoords, denominator_ycoords, denominator_data, numerator_xcoords, numerator_ycoords, numerator_data)
 		for extension in options.format:
 			outname = "%s.%s" % (name.replace(",", "_"), extension)
 			if options.verbose:
-				print >>sys.stderr, "\twriting %s ..." % outname
-			fig.savefig(outname)
+				print("\twriting %s ..." % outname, file=sys.stderr)
+			fig.savefig(outname)
