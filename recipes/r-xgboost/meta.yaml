{% set version = '1.6.0.1' %}
{% set posix = 'm2-' if win else '' %}
{% set native = 'm2w64-' if win else '' %}

package:
  name: r-xgboost
  version: {{ version|replace("-", "_") }}

source:
  url:
    - {{ cran_mirror }}/src/contrib/xgboost_{{ version }}.tar.gz
    - {{ cran_mirror }}/src/contrib/Archive/xgboost/xgboost_{{ version }}.tar.gz
  sha256: 9ae99a20997e1b02ffd21cabada2a55e53f5754746238ee900de5eb6cd964ebd

build:
  merge_build_host: True  # [win]
  number: 0
  rpaths:
    - lib/R/lib/
    - lib/

requirements:
  build:
    - {{ compiler('c') }}              # [not win]
    - {{ compiler('m2w64_c') }}        # [win]
    - {{ compiler('cxx') }}            # [not win]
    - {{ compiler('m2w64_cxx') }}      # [win]
    - {{ posix }}filesystem        # [win]
    - {{ posix }}sed               # [win]
    - {{ posix }}grep              # [win]
    - {{ posix }}autoconf
    - {{ posix }}automake          # [not win]
    - {{ posix }}automake-wrapper  # [win]
    - {{ posix }}pkg-config
    - {{ posix }}make
    - {{ posix }}coreutils         # [win]
    - {{ posix }}zip               # [win]
    - cross-r-base {{ r_base }}    # [build_platform != target_platform]
  host:
    - _openmp_mutex                # [linux]
    - llvm-openmp                  # [osx]
    - r-base
    - r-matrix >=1.1_0
    - r-data.table >=1.9.6
    - r-jsonlite >=1.0
  run:
    - _openmp_mutex                # [linux]
    - llvm-openmp                  # [osx]
    - r-base
    - {{ native }}gcc-libs         # [win]
    - r-matrix >=1.1_0
    - r-data.table >=1.9.6
    - r-jsonlite >=1.0

test:
  commands:
    - $R -e "library('xgboost')"           # [not win]
    - "\"%R%\" -e \"library('xgboost')\""  # [win]

about:
  home: https://github.com/dmlc/xgboost
  license: Apache-2.0
  summary: Extreme Gradient Boosting, which is an efficient implementation of the gradient boosting
    framework from Chen & Guestrin (2016) <doi:10.1145/2939672.2939785>. This package
    is its R interface. The package includes efficient linear model solver and tree
    learning algorithms. The package can automatically do parallel computation on a
    single machine which could be more than 10 times faster than existing gradient boosting
    packages. It supports various objective functions, including regression, classification
    and ranking. The package is made to be extensible, so that users are also allowed
    to define their own objectives easily.
  license_family: APACHE
  license_file:
    - LICENSE

extra:
  recipe-maintainers:
    - conda-forge/r
