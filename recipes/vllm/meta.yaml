{% set name = "vllm" %}
{% set version = "0.2.3" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/vllm-{{ version }}.tar.gz
  sha256: d93434824e90da203d28a44f48cc2163026bf973a5bdf7377c8cb4246ceaeee1

build:
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0

requirements:
  host:
    - python >=3.8
    - ninja
    - packaging
    - setuptools >=49.4.0
    - pytorch >=2.1.0
    - wheel
    - pip
  run:
    - python >=3.8
    - ninja
    - psutil
    - ray >=2.5.1
    - pandas
    - pyarrow
    - sentencepiece
    - numpy
    - einops
    - pytorch >=2.1.0
    - transformers >=4.34.0
    - xformers >=0.0.22.post7
    - fastapi
    - uvicorn
    - pydantic ==1.10.13
    - aioprometheus

test:
  imports:
    - vllm
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/vllm-project/vllm
  summary: A high-throughput and memory-efficient inference and serving engine for LLMs
  license: Apache-2.0
  license_file: LICENSE

extra:
  recipe-maintainers:
    - mediocretech
