{% set name = "vllm" %}
{% set version = "0.4.2" %}
{% set build_num = 0 %}

{% if cuda_compiler_version is not defined or cuda_compiler_version == "None" %}
{% set build_num = build_num + 1000 %}
{% endif %}

{% if cuda_compiler_version in (None, "None", True, False) %}
{% set cuda_major = 0 %}
{% else %}
{% set cuda_major = environ.get("cuda_compiler_version", "11.8").split(".")[0] | int %}
{% endif %}


package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/vllm-{{ version }}.tar.gz
  sha256: 0b857f3084b507cbdd3bfcbaae19d171c55df9955eb3ac41c9c711768e852772

build:
  number: {{ build_num }}
  string: cuda{{ cuda_compiler_version | replace('.', '') }}py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ build_num }}  # [cuda_compiler_version != "None"]
  string: cpu_py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ build_num }}                                                # [cuda_compiler_version == "None"]
#  skip: True  # [cuda_compiler_version == "None"]
  skip: True  # [osx or win]
  #missing_dso_whitelist:
    #- '*/libtorch_python.so'
    #- '*/libcuda.so.1'
  rpaths:
    - lib/
    # conda-forge::pytorch provides these libs
    - {{ SP_DIR }}/torch/lib/
  # weigh down gpu implementation and give nocuda preference
  track_features:
    - cuda_i7khcYiPpQptWags  # [cuda_compiler_version != "None"]
  {% if cuda_major >= 12 %}
  ignore_run_exports_from:
    # not identical to list of host deps; we do need cuda-cudart,
    # cuda-driver-dev & cuda-nvml-dev have no run-exports
    - cuda-nvrtc-dev
    - cuda-nvtx-dev
    - libcublas-dev
    - libcufft-dev
    - libcurand-dev
    - libcusolver-dev
    - libcusparse-dev
  {% endif %}

requirements:
  build:
    - cmake
    - make
    - pytorch =2.3.0*
    - pytorch =*=cuda*  # [cuda_compiler_version != "None"]
    - pytorch =*=cpu*   # [cuda_compiler_version == "None"]
    - {{ stdlib("c") }}
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}  # [cuda_compiler_version != "None"]
    - cuda-version =={{ cuda_compiler_version }}  # [cuda_compiler_version not in (undefined, 'None')]
    {% if cuda_major >= 12 %}
    - cuda-driver-dev                        # [build_platform != target_platform]
    - cuda-cudart-dev                        # [build_platform != target_platform]
    - cuda-nvrtc-dev                         # [build_platform != target_platform]
    - cuda-nvtx-dev                          # [build_platform != target_platform]
    - cuda-nvml-dev                          # [build_platform != target_platform]
    - libcublas-dev                          # [build_platform != target_platform]
    - libcufft-dev                           # [build_platform != target_platform]
    - libcurand-dev                          # [build_platform != target_platform]
    - libcusolver-dev                        # [build_platform != target_platform]
    - libcusparse-dev                        # [build_platform != target_platform]
    {% endif %}
  host:
    - python
    - pytorch =2.3.0*
    - pytorch =*=cuda*  # [cuda_compiler_version != "None"]
    - pytorch =*=cpu*   # [cuda_compiler_version == "None"]
    - cmake >=3.21
    - ninja
    - packaging
    - setuptools >=49.4.0
    - wheel
    - cuda-version =={{ cuda_compiler_version }}  # [cuda_compiler_version not in (undefined, 'None')]
    {% if cuda_major >= 12 %}
    - cuda-driver-dev
    - cuda-cudart-dev
    - cuda-nvrtc-dev
    - cuda-nvtx-dev
    - cuda-nvml-dev
    - libcublas-dev
    - libcufft-dev
    - libcurand-dev
    - libcusolver-dev
    - libcusparse-dev
    {% endif %}
    - pip
  run:
    - python
    - numpy
    - pytorch =2.3.0*
    - pytorch =*=cuda*  # [cuda_compiler_version != "None"]
    - pytorch =*=cpu*   # [cuda_compiler_version == "None"]
    - cmake >=3.21
    - ninja
    - psutil
    - sentencepiece
    - requests
    - py-cpuinfo
    - transformers >=4.40.0
    - tokenizers >=0.19.1
    - fastapi
    - openai
    - uvicorn-standard
    - pydantic >=2.0
    - prometheus_client >=0.18.0
    - prometheus-fastapi-instrumentator >=7.0.0
    - tiktoken ==0.6.0
    - lm-format-enforcer ==0.9.8
    - outlines ==0.0.34
    - typing-extensions
    - filelock >=3.10.4
    - ray-core >=2.9   # [cuda_compiler_version == "None"]
    - nvidia-ml-py   # [cuda_compiler_version == "None"]
    - vllm-nccl-cu12 >=2.18,<2.19   # [cuda_compiler_version == "None"]
    - xformers ==0.0.26.post1   # [cuda_compiler_version == "None"]
    - triton >=2.0.0   # [cuda_compiler_version == "None"]

test:
  imports:
    - vllm
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/vllm-project/vllm
  summary: A high-throughput and memory-efficient inference and serving engine for LLMs
  license: Apache-2.0
  license_file:
    - LICENSE
    - csrc/punica/LICENSE
    - csrc/quantization/marlin/LICENSE

extra:
  recipe-maintainers:
    - rxm7706
