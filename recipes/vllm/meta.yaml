{% set name = "vllm" %}
{% set version = "0.4.2" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/vllm-{{ version }}.tar.gz
  sha256: 0b857f3084b507cbdd3bfcbaae19d171c55df9955eb3ac41c9c711768e852772

build:
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0

requirements:
  host:
    - python >=3.8
    - cmake >=3.21
    - ninja
    - packaging
    - setuptools >=49.4.0
    - pytorch ==2.3.0
    - wheel
    - pip
  run:
    - python >=3.8
    - cmake >=3.21
    - ninja
    - psutil
    - sentencepiece
    - numpy
    - requests
    - py-cpuinfo
    - transformers >=4.40.0
    - tokenizers >=0.19.1
    - fastapi
    - openai
    - uvicorn
    - pydantic >=2.0
    - prometheus_client >=0.18.0
    - prometheus-fastapi-instrumentator >=7.0.0
    - tiktoken ==0.6.0
    - lm-format-enforcer ==0.9.8
    - outlines ==0.0.34
    - typing-extensions
    - filelock >=3.10.4
    - ray-core >=2.9
    - nvidia-ml-py
    - vllm-nccl-cu12 >=2.18,<2.19
    - pytorch ==2.3.0
    - xformers ==0.0.26.post1

test:
  imports:
    - vllm
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/vllm-project/vllm
  summary: A high-throughput and memory-efficient inference and serving engine for LLMs
  license: Apache-2.0
  license_file:
    - LICENSE
    - csrc/punica/LICENSE
    - csrc/quantization/marlin/LICENSE

extra:
  recipe-maintainers:
    - rxm7706
