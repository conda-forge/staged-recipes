{% set name = "vllm" %}
{% set version = "0.2.4" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/vllm-{{ version }}.tar.gz
  # sha256: 41a7266be66b2887be1afd879a77c3e4062c5f30fd3159eace2e8e271fe21271
  sha256: 1b32df7f00ebdd2a828032d1579a1b7865f853ca0149dbee60c2a40c040b21f3

build:
  # noarch: python

  script_env:
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX      # [cuda_compiler_version == "11.2"]
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9+PTX  # [cuda_compiler_version == "11.8"]
    - TORCH_CUDA_ARCH_LIST=5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX  # [(cuda_compiler_version or "").startswith("12")]
    - MAX_JOBS=1
    
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation

  number: 0
  skip: True  # [cuda_compiler_version == "None"]
  skip: True  # [osx or win]
  missing_dso_whitelist:
    - '*/libtorch_python.so'
    
requirements:
  build:
    - cmake
    - make
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
  host:
    - python
    - ninja
    - packaging
    - setuptools >=49.4.0
    - pytorch >=2.1.0
    - wheel
    - pip
  run:
    - python
    - ninja
    - psutil
    - ray-core >=2.5.1
    - pandas
    - pyarrow
    - sentencepiece
    - numpy
    - einops
    - pytorch >=2.1.0
    - transformers >=4.34.0
    - xformers
    - fastapi
    - uvicorn
    - pydantic ==1.10.13
    - aioprometheus
    - libtorch
    - cudatoolkit
    - libstdcxx-ng
    - libgcc-ng

test:
  imports:
    - vllm
  requires:
    - pip

about:
  home: https://github.com/vllm-project/vllm
  summary: A high-throughput and memory-efficient inference and serving engine for LLMs
  license: Apache-2.0
  license_file: LICENSE

extra:
  recipe-maintainers:
    - mediocretech
