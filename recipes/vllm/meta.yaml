{% set name = "vllm" %}
{% set version = "0.2.7" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/vllm-{{ version }}.tar.gz
  sha256: be96122b62c74f6be9974ba403dbe320d4d70f30185cdcfe2a49f9e4c9ac07d4

build:
  noarch: python
  script_env:
    - DISABLE_QIGEN=1 # coming soon
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX      # [cuda_compiler_version == "11.2"]
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9+PTX  # [cuda_compiler_version == "11.8"]
    - TORCH_CUDA_ARCH_LIST=5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX  # [(cuda_compiler_version or "").startswith("12")]
    - MAX_JOBS=1
  script: {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0
  skip: True  # [cuda_compiler_version == "None"]
  skip: True  # [osx or win]
  missing_dso_whitelist:
    - '*/libtorch_python.so'
    
requirements:
  build:
    - cmake
    - make
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
  host:
    - python >=3.8
    - ninja
    - packaging
    - setuptools >=49.4.0
    - pytorch >=2.1.0
    - wheel
    - pip
  run:
    - python >=3.8
    - ninja
    - psutil
    - ray-core >=2.5.1
    - pandas
    - pyarrow
    - sentencepiece
    - numpy
    - einops
    - pytorch >=2.1.0
    - transformers >=4.34.0
    - xformers
    - fastapi
    - uvicorn
    - pydantic ==1.10.13
    - aioprometheus
    - libtorch
    - cudatoolkit
    - libstdcxx-ng
    - libgcc-ng

test:
  imports:
    - vllm
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/vllm-project/vllm
  summary: A high-throughput and memory-efficient inference and serving engine for LLMs
  license: Apache-2.0
  license_file: LICENSE

extra:
  recipe-maintainers:
    - mediocretech
