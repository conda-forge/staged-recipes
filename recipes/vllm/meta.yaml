{% set name = "vllm" %}
{% set version = "0.4.1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/vllm-{{ version }}.tar.gz
  sha256: 2a0ac627b4cc9ff260b50bd63a43f71bf9b72b432bff334fd6cfe86cc9ac7437

build:
  # noarch: python
  
  rpaths:
    - lib/

  script_env:
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX      # [cuda_compiler_version == "11.2"]
    - TORCH_CUDA_ARCH_LIST=3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9+PTX  # [cuda_compiler_version == "11.8"]
    - TORCH_CUDA_ARCH_LIST=5.0;6.0;6.1;7.0;7.5;8.0;8.6;8.9;9.0+PTX  # [(cuda_compiler_version or "").startswith("12")]

  number: 0
  skip: True  # [cuda_compiler_version == "None"]
  skip: True  # [osx or win]
  missing_dso_whitelist:
    - '*/libtorch_python.so'
    - '*/libcuda.so.1'

requirements:
  build:
    - cmake
    - make
    - {{ stdlib("c") }}
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}
    
  host:
    - cuda-version {{ cuda_compiler_version }}
    - python
    - ninja
    - packaging
    - setuptools >=49.4.0
    - pytorch
    - torchvision
    - wheel
    - pip
    - libxcrypt
    
  run:
    - python
    - ninja
    - psutil
    - ray-core >=2.9.0
    - pandas
    - pyarrow
    - sentencepiece
    - numpy
    - einops
    - pytorch ~=2.1.0
    - transformers >=4.34.0
    - xformers
    - fastapi
    - uvicorn
    - pydantic
    - aioprometheus
    - prometheus_client
    - triton
    # - libtorch
    # - cuda-version {{ cuda_compiler_version }}  # [cuda_compiler_version != "None"]
    # - cuda-cupti-dev    # [(cuda_compiler_version or "").startswith("12")]
    # - cuda-cudart-dev   # [(cuda_compiler_version or "").startswith("12")]
    # - cuda-nvml-dev     # [(cuda_compiler_version or "").startswith("12")]
    # - cuda-nvtx-dev     # [(cuda_compiler_version or "").startswith("12")]
    # - libstdcxx-ng
    # - libgcc-ng
    # - libxcrypt

test:
  imports:
    - vllm
  # commands:
  #  - pip check
  requires:
    - pip

about:
  home: https://github.com/vllm-project/vllm
  summary: A high-throughput and memory-efficient inference and serving engine for LLMs
  license: Apache-2.0
  license_file: LICENSE

extra:
  recipe-maintainers:
    - mediocretech
    - iamthebot
